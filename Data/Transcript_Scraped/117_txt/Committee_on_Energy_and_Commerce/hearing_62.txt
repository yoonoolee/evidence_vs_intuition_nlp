- HOLDING BIG TECH ACCOUNTABLE: LEGISLATION TO PROTECT ONLINE USERS

[House Hearing, 117 Congress]
[From the U.S. Government Publishing Office]

HOLDING BIG TECH ACCOUNTABLE: LEGISLATION TO PROTECT ONLINE USERS

=======================================================================

HYBRID HEARING

BEFORE THE

SUBCOMMITTEE ON CONSUMER PROTECTION AND COMMERCE

OF THE

COMMITTEE ON ENERGY AND COMMERCE
HOUSE OF REPRESENTATIVES

ONE HUNDRED SEVENTEENTH CONGRESS

SECOND SESSION

__________

MARCH 1, 2022

__________

Serial No. 117-70

Published for the use of the Committee on Energy and Commerce

govinfo.gov/committee/house-energy
energycommerce.house.gov

______

U.S. GOVERNMENT PUBLISHING OFFICE
59-693 PDF           WASHINGTON : 2025

COMMITTEE ON ENERGY AND COMMERCE

FRANK PALLONE, Jr.,  New Jersey
Chairman
BOBBY L. RUSH, Illinois              CATHY Mc  MORRIS RODGERS,
ANNA G. ESHOO, California                Washington
DIANA De  GETTE, Colorado              Ranking Member
MIKE DOYLE, Pennsylvania             FRED UPTON, Michigan
JAN SCHAKOWSKY, Illinois             MICHAEL C. BURGESS, Texas
G. K. BUTTERFIELD, North Carolina    STEVE SCALISE, Louisiana
DORIS O. MATSUI, California          ROBERT E. LATTA, Ohio
KATHY CASTOR, Florida                BRETT GUTHRIE, Kentucky
JOHN P. SARBANES, Maryland           DAVID B. Mc  KINLEY, West Virginia
JERRY Mc  NERNEY, California         ADAM KINZINGER, Illinois
PETER WELCH, Vermont                 H. MORGAN GRIFFITH, Virginia
PAUL TONKO, New York                 GUS M. BILIRAKIS, Florida
YVETTE D. CLARKE, New York           BILL JOHNSON, Ohio
KURT SCHRADER, Oregon                BILLY LONG, Missouri
TONY CARDENAS, California            LARRY BUCSHON, Indiana
RAUL RUIZ, California                MARKWAYNE MULLIN, Oklahoma
SCOTT H. PETERS, California          RICHARD HUDSON, North Carolina
DEBBIE DINGELL, Michigan             TIM WALBERG, Michigan
MARC A. VEASEY, Texas                EARL L. ``BUDDY'' CARTER, Georgia
ANN M. KUSTER, New Hampshire         JEFF DUNCAN, South Carolina
ROBIN L. KELLY, Illinois, Vice       GARY J. PALMER, Alabama
Chair                            NEAL P. DUNN, Florida
NANETTE DIAZ BARRAGAN, California    JOHN R. CURTIS, Utah
A. DONALD Mc  EACHIN, Virginia       DEBBIE LESKO, Arizona
LISA BLUNT ROCHESTER, Delaware       GREG PENCE, Indiana
DARREN SOTO, Florida                 DAN CRENSHAW, Texas
TOM O'HALLERAN, Arizona              JOHN JOYCE, Pennsylvania
KATHLEEN M. RICE, New York           KELLY ARMSTRONG, North Dakota
ANGIE CRAIG, Minnesota
KIM SCHRIER, Washington
LORI TRAHAN, Massachusetts
LIZZIE FLETCHER, Texas

Professional Staff

TIFFANY GUARASCIO, Staff Director
WAVERLY GORDON, Deputy Staff Director
NATE HODSON, Minority Staff Director
Subcommittee on Consumer Protection and Commerce

JAN SCHAKOWSKY, Illinois
Chair
BOBBY L. RUSH, Illinois              GUS M. BILIRAKIS, Florida
KATHY CASTOR, Florida                  Ranking Member
LORI TRAHAN, Massachusetts           FRED UPTON, Michigan
JERRY Mc  NERNEY, California         ROBERT E. LATTA, Ohio
YVETTE D. CLARKE, New York           BRETT GUTHRIE, Kentucky
TONY CARDENAS, California, Vice      LARRY BUCSHON, Indiana
Chair                            NEAL P. DUNN, Florida
DEBBIE DINGELL, Michigan             GREG PENCE, Indiana
ROBIN L. KELLY, Illinois             DEBBIE LESKO, Arizona
DARREN SOTO, Florida                 KELLY ARMSTRONG, North Dakota
KATHLEEN M. RICE, New York           CATHY Mc  MORRIS RODGERS,
ANGIE CRAIG, Minnesota                   Washington (ex officio)
LIZZIE FLETCHER, Texas
FRANK PALLONE, Jr.,  New Jersey (ex
officio)
C O N T E N T S

----------
Page
Hon. Jan Schakowsky, a Representative in Congress from the State
of Illinois, opening statement.................................     2
Prepared statement...........................................     4
Hon. Gus Bilirakis, a Representative in Congress from the State
of Florida, opening statement..................................     6
Prepared statement...........................................     8
Hon. Frank Pallone, a Representative in Congress from the State
of New Jersey, opening statement...............................    11
Prepared statement...........................................    13
Hon. Cathy McMorris Rodgers, a Representative in Congress from
the State of Washington, opening statement.....................    15
Prepared statement...........................................    17

Witnesses

Laurel Lehman, Policy Analyst, Consumer Reports..................    21
Prepared statement...........................................    23
Answers to submitted questions \3\...........................
Katie McInnis, Senior Public Policy Manager U.S., Duckduckgo,
Inc............................................................    43
Prepared statement...........................................    45
Answers to submitted questions \3\...........................
Mike Duffey, Special Agent Supervisor, Florida Department of Law
Enforcement....................................................    50
Prepared statement...........................................    52
Answers to submitted questions...............................   126
Mutale Nkonde, Chief Executive Officer, AI for the People U.S....    56
Prepared statement...........................................    58
Answers to submitted questions...............................   145

Submitted Material

H.R. 6416 the Banning Surveillance Advertising Act of 2022 \1\
H.R. 6580 the Algorithmic Accountability Act of 2022 \1\
H.R. 6755 the Cooperation Among Police, Tech, and Users to Resist
Exploitation Act \1\
H.R. 6786 the Increasing Consumers' Education on Law Enforcement
Resources \1\
H.R. 6796 the Digital Services Oversight and Safety Act of 2022
\1\
Letter of February 14, 2022, from Christopher Oswald, Executive
Vice President, Government Relations the Association of
National Advertisers, Driving Growth, to Ms. Eshoo, et al.,
submitted by Mr. Latta.........................................    99
Letter of March 1, 2022, from Interactive Advertising Bureau,
submitted by Ms. Lesko \2\

----------
\1\ Legislation has been retained in committee files and also is
available at https://docs.house.gov/Committee/Calendar/
ByEvent.aspx?EventID=114439.
\2\ The information has been retained in committee files and also
is available at https://docs.house.gov/meetings/IF/IF17/
20220301/114439/HHRG-117-IF17-20220301-SD004.pdf.
\3\ Ms. Lehman and Ms. McInnis, did not answer submitted
questions for the record by the time of publications.
Report ``Enhancing the FTC's Consumer Protection Authority to
Regulate Social Media Companies'' by Paul M. Barrett and Lily
Warnke, NYU/STERN, submitted by Ms. Trahan.....................   102
Letter of February 28, 2022, from Paul Lekas, Senior Vice
President, Global Public Policy, Software and Information
Industry Association, to Ms. Schakowsky and Mr. Bilirakis,
submitted by Ms. Schakowsky....................................   115
Letter of February 28, 2022, from Ms. Rodgers, et al., to Ms.
Khan, submitted by Mr. Bilirakis...............................   121
Letter of March 1, 2022, Neil L. Bradley, Executive Vice
President, Chief Policy Officer, Head of Strategic Advocacy,
U.S. Chamber of Commerce, to Ms. Schakowsky and Mr. Bilirakis,
submitted by Mr. Guthrie.......................................   124

HOLDING BIG TECH ACCOUNTABLE: LEGISLATION TO PROTECT ONLINE USERS

----------

TUESDAY, MARCH 1, 2022

House of Representatives,
Subcommittee on Consumer Protection and Commerce,
Committee on Energy and Commerce,
Washington, DC.

The subcommittee met, pursuant to notice, at 10:31 a.m., in
the John D. Dingell Room 2123, Rayburn House Office Building,
and remotely via Cisco Webex online video conferencing, Hon.
Jan Schakowsky, (chair of the subcommittee) presiding.
Members present: Representatives Schakowsky, Rush, Castor,
Trahan, McNerney, Clarke, Cardenas, Dingell, Kelly, Soto,
Craig, Pallone (ex officio); Bilirakis (subcommittee ranking
member), Upton, Latta, Guthrie, Bucshon, Dunn, Lesko, Pence,
and Rodgers (ex officio).
Also present: Representatives Eshoo and Walberg.
Staff present: Katherine Durkin, Policy Coordinator;
Waverly Gordon, Deputy Staff Director and General Counsel;
Jessica Grandberry, Staff Assistant; Tiffany Guarascio, Staff
Director; Perry Hamilton, Clerk; Ed Kaczmarski, Policy Analyst;
Zach Kahan, Deputy Director Outreach and Member Service;
Mackenzie Kuhl, Press Assistant; Jerry Leverich, Chief Counsel,
Communications and Technology; David Miller, Counsel; Kaitlyn
Peel, Digital Director; Caroline Rinker, Press Assistant; Chloe
Rodriguez, Clerk; Andrew Souvall, Director of Communications,
Outreach, and Member Services; Michele Viterise, Counsel;
Caroline Wood, Staff Assistant; C.J. Young, Deputy
Communications Director; Michael Cameron, Minority Policy
Analyst, Consumer Protection and Commerce, Energy, Environment;
Peter Kielty, Minority General Counsel; Emily King, Minority
Member Services Director; Tim Kurth, Minority Chief Counsel,
Consumer Protection and Commerce; Kate O'Connor, Minority Chief
Counsel, Communication and Technology; Brannon Rains, Minority
Professional Staff Member, Consumer Protection and Commerce;
and Michael Taggart, Minority Policy Director.
Ms. Schakowsky. The Subcommittee on Consumer Protection and
Commerce will now come to order.
Today we will hold a legislative hearing entitled,
``Holding Big Tech Accountable: Legislation to Protect Online
Users.''
Due to COVID-19, members can now participate in today's
hearing, either in person, remotely, or remotely online via
conference, and in accord with the updated guidelines issued by
the attending physician.
Members, staff, and members of the press can participate in
the hearing room, and are not required to wear masks.
For members participating remotely, your microphones will
be set on mute for the purpose of eliminating inadvertent
background noise. Members participating remotely will need to
unmute your microphones each time that you wish to speak.
Please note--please notice that, once you have unmuted your
microphone and anything that is--but anything that is said will
be--can be heard over the loudspeakers. And so you don't want
to do that.
Since members are participating from different locations at
today's hearing, all recognition of members shall--such as--
such for--as such for questions will be in the order of
subcommittee seniority.
Documents for the record can be sent to Ed Kaczmarski at
the email address that was provided to staff, and all documents
shall be entered into the record at the end of--the conclusion
of the hearing.
The Chair now recognizes herself for 5 minutes for an
opening statement.

OPENING STATEMENT OF HON. JAN SCHAKOWSKY, A REPRESENTATIVE IN
CONGRESS FROM THE STATE OF ILLINOIS

So today we begin the third legislative hearing of the
Energy and Commerce Committee to consider legislation to rein
in Big Tech.
This subcommittee has worked long and hard to respond to
the challenge, the challenges presented by Big Tech. But
despite our scrutiny and repeated calls for change, the tech
industry has not responded. Now is the time for change and for
accountability.
We know social media platforms can be used for good, and we
have seen in the last week the heroic stories of Ukrainian
people, and the amazing fight-back that they have shown, and
courage that they have shown. But misinformation and
disinformation are seemingly still amplified, and that is
having dramatic costs for people around the world.
For example, Russian state-owned media is targeting Spanish
speakers around the globe with disinformation to--excuse me,
when it comes to the--what is going on in the invasion of
Ukraine. Previously, social media abetted a genocide in Myanmar
and a deadly insurrection on January 6th.
We are done with apologies and denials from Big Tech
companies. We are done turning a blind eye when billionaires
build economic empires by feeding Americans an ever-increasing
diet of disinformation. It is time to regulate. So we will be
considering five bills today.
Ms. Eshoo's bill, the Banning Surveillance--excuse me. What
does that say?
Voice. Advertising.
Ms. Schakowsky. What?
Voice. Advertising.
Ms. Schakowsky. OK, the Banning Surveillance Advertising
Act, which I am a proud cosponsor of, to ban targeting
advertising that can track individual users across the
internet.
Ms. Clarke's bill, the Algorithms Accountability Act, will
require technology companies using the art of--using artificial
intelligence to assess the impact of their algorithms on
consumers--disinformation to--discrimination, rather, to--of
protected classes no--has no place in our digital world.
Mrs. Trahan's bill, the Digital Service, Oversight, and
Safety Act, improves transparency for consumers and ensures
that research can be--can measure the impact of social media on
our society.
Mr. Bilirakis's bill, the CAPTURE Act--that is the short
title--studies whether law enforcement has the resources to
keep us safe online.
Mr. Mullin's bill, the Increasing Consumer Education on Law
Enforcement Act, empowers consumers to protect the--them--to
protect themselves.
So as we refine these proposals, I have no doubt that can
help create a fairer, safer internet, one that protects
consumers, and who--and whose business model isn't rooted in
disinformation.
So I look forward to the hearing, and welcome all the
comments of my colleagues on both sides of the aisle, so that
we can work together on this.
[The prepared statement of Ms. Schakowsky follows:]

Prepared Statement of Hon. Jan Schakowsky

Ms. Schakowsky. And with that, I would like to welcome the
ranking member, Mr. Bilirakis, for his 5 minutes.

OPENING STATEMENT OF HON. GUS BILIRAKIS, A REPRESENTATIVE IN
CONGRESS FROM THE STATE OF FLORIDA

Mr. Bilirakis. Thank you so very much, Madam Chair.
Good morning, and welcome to today's subcommittee
legislative hearing, our witnesses.
Today's hearing is the third in a series of what the
majority calls ``Holding Big Tech Accountable.'' I am--I
certainly hope the fourth will be on a national privacy and
data security framework.
For the December hearing, Republicans invited TikTok to
testify as our witness, since they are not at the center--they
are at the center of many of our shared concerns. But TikTok
declined.
For this hearing, I appreciate that the majority backed our
efforts to invite the company, this particular company, TikTok,
this time on a bipartisan basis. But sadly, TikTok declined yet
again. I know this isn't what you want, Madam Chair. I
certainly don't want it, either. So again, very disappointing,
and we are not going to give up.
Madam Chair, if we put our heads together, I am sure we can
figure out a way to get this company, this particular company,
TikTok, before our subcommittee, like so many others who have
appeared as part of our country's democratic process. How very
arrogant of them. But again, we are not going to give up. We
need to hear from TikTok.
The legislation today covers a broad range of issues, you
know, not just hear from TikTok. We need to ask questions of
TikTok.
While I share many of the same concerns as my colleagues
regarding Big Tech's abuse of power, I worry about proposals
that miss the target and hurt other parts of our economy.
Without more careful vetting, these bills will lead to a worse
consumer experience, adversely impact American innovators and
small businesses, and increase market dominance for large
companies, specifically Google in this case.
So far this Congress, this committee has heard from
academics, industry experts, and consumer advocates, which is
great. I am pleased that today we finally get to hear from law
enforcement's perspective. They are on the front lines--and I
know you agree, Madam Chair--for protecting our kids online,
which is a primary focus in our fight against Big Tech. And
thank you again for holding this hearing.
I am proud to welcome Mike Duffey, a law enforcement
officer based in my home State of Florida, who has dedicated
more than 20 years of his career to protecting our Floridians
and our Nation's children from predators and other dangerous
threats online.
Law enforcement's insights are critically important, as we
consider multiple avenues to rein in the abuses perpetrated by
Big Tech. I believe two bills under consideration today will
help.
First we have got the H.R. 6786, the Increasing Consumers
Education on Law Enforcement Resources Act, introduced by my
good friend, Representative Mullin. It will require FTC and
attorneys general to develop a public education program to
inform our constituents about law enforcement resources
regarding online safety.
The second is my bill--and thank you for mentioning it,
Madam Chair, and agendaing the bill--H.R. 6755, the Cooperation
Among Police, Tech, and Users to Resist Exploitation Act, or
the CAPTURE Act, which will require the Government
Accountability Office to research and provide recommendations
on how to improve coordination and consultation between social
media companies and law enforcement without creating
constitutional issues.
Some here may recognize the single point of contact
proposal, as it currently exists in the FirstNet framework.
This is a model I am looking to apply to tech platforms so
Federal, State, and local law enforcement have clarity on who
is reached reaching out to these companies in order to tackle
harms online.
Given the connection to today's Democratic bills have on
privacy and data security, I know my friend, the Chair, will
appreciate me once again emphasizing the importance of passing
a national privacy and data security law. Even the House
Administration Committee just held a hearing on a national
privacy standard. We are all well overdue--we are overdue to
hold a committee hearing because we have the jurisdiction in
this committee, and I know the Chair has promised that we
would.
Madam Chair, I know these issues are extremely important to
you, and the best way to protect our constituents online is
through a national privacy and data security framework. It is
time our subcommittee moves beyond these one-off bills that
beat around the bush of privacy and data security concerns.
Leader Rodgers--and she will--I am sure she will confirm this--
and I remain fully committed to working across the aisle with
anyone to enact a truly comprehensive privacy and data security
law, and I urge my colleagues to join me in these efforts.
Thank you again for allowing me to participate today, and I
look forward to the discussion, especially how these proposals
will help protect our kids online.
[The prepared statement of Mr. Bilirakis follows:]

Prepared Statement of Hon. Gus Bilirakis

Mr. Bilirakis. Thank you, and I yield back.
Ms. Schakowsky. Thank you, Mr. Bilirakis. And I do look
forward to very soon dealing with a comprehensive privacy bill.
I think we are on our way.
And let me now welcome the chair of the full committee, Mr.
Pallone, for 5 minutes.

OPENING STATEMENT OF HON. FRANK PALLONE, Jr., A REPRESENTATIVE
IN CONGRESS FROM THE STATE OF NEW JERSEY

Mr. Pallone. Thank you, Chair Schakowsky. Today's hearing
is the third in a series of hearings this committee has held on
legislative reforms to hold social media companies accountable.
The two previous legislative hearings covered reforms to
section 230 of the Communications Decency Act and consumer
protection-focused legislation to help build a safer internet.
Today we will examine legislation to enhance transparency and
further promote safety online.
These hearings come after years of repeated bipartisan
calls for social media companies to change their ways. Since
2018 the committee has held 8 hearings on the subject. We have
examined these issues from all sides, and now it is time for us
to come together and to act. We are committed to working with
our Republican colleagues on legislation to increase
transparency, limit online manipulation, and improve online
safety.
And we all know how important social media is to our daily
lives. It allows us to connect with family and friends to
organize and to stay safe. And we are seeing that firsthand
right now in Ukraine, as images posted on social media are
exposing the world to just how depraved and misguided Vladimir
Putin's actions are.
Ukrainians armed with their smartphones are documenting the
bravery of their fellow citizens standing up to the Russian
military and the brutality of war. Social media is allowing
Ukrainians to spread the word without any filters of the true
impacts of this war. The images they are capturing on their
phones are being shown worldwide, showing the world the
atrocities being inflicted on the Ukrainian people.
But at the very same time, we have seen weeks of Russian
disinformation campaigns used to lay the groundwork for the
invasion of Ukraine. These campaigns use propaganda to build
support for the Kremlin and, unfortunately, they spread like
wildfire online.
And there is no question that fast-moving current events
can be difficult for social media companies to respond to
quickly. But that is their responsibility, and they must be
held accountable. We must ensure they are transparent, and
their incentives align with the good social media can do for
people, and not the bad.
Today we will discuss five bills that target different
parts of the social media ecosystem to make platforms safer for
users.
One of the best ways to make these companies more
accountable is to make them more transparent. We will discuss
legislation that establishes an Office of Independent Research
Facilitation at the FTC. This new office would help facilitate
academic research on social media platforms to help us get the
data we need on how these companies are targeting users.
Another bill requires companies that use automated
decisionmaking to conduct impact assessments on their systems,
and regularly report the results to the FTC. Now, these
assessments will help ensure that machine learning is being
employed in a fair and nondiscriminatory manner.
We will also consider a bill to ban the practice of
targeted advertising, which includes a provision prohibiting
advertisers from using information that identifies a consumer
as a member of a protected class for advertising purposes.
And finally, we will consider bills that will help social
media companies work better with Federal, State, and local law
enforcement to protect users who feel their safety has been
violated online.
Now, these proposals, along with the proposals we
considered in the two previous legislative hearings, are
collectively major steps in addressing the real harms caused by
Big Tech.
Another part of tech accountability is protecting people's
privacy. I know that both the ranking member of the
subcommittee and the Chair mentioned this, and have been
involved with this, and we are particularly concerned about our
children's privacy, as more and more apps are used by and
targeted to our kids.
I think every member of this committee agrees that more
must be done on privacy, and that is why we have been working
since last Congress on a bipartisan staff draft. Our work on
that legislation continues, and I hope the Republicans will
work together with us on that, as well.
The bills before us today collectively address tech
accountability. The time to act is now, and these bills can
help us make the internet a safer place. And I look forward to
our discussion today and, obviously, hearing from our excellent
panel.
[The prepared statement of Mr. Pallone follows:]

Prepared Statement of Hon. Frank Pallone, Jr.

Mr. Pallone. I yield back, Madam Chair, and thank you for
all that you have done on these issues. You are--you really
have been working on them for a long time, and it is--it has
made a difference in terms of where we are today. Thank you.
Ms. Schakowsky. Thank you, Mr. Chairman. And now, wearing
her Ukrainian yellow----
Mrs. Rodgers. That is right.
Ms. Schakowsky. I want to recognize Mrs. Rodgers, our
ranking member, for 5 minutes for her opening statement.

OPENING STATEMENT OF HON. CATHY McMORRIS RODGERS, A
REPRESENTATIVE IN CONGRESS FROM THE STATE OF WASHINGTON

Mrs. Rodgers. Thank you, Madam Chair.
First I want to say, ``Glory to Ukraine.'' Several of us on
this committee had the opportunity to travel to Ukraine, and
were inspired at the time with their fight for self-
determination and independence. And today I continue to support
their fight for freedom, and I pray for their strength and
security, and their freedom to prevail.
While some tech companies have taken steps to prevent the
proliferation of Russian state-sponsored propaganda, I hope
that we too on the subcommittee will be vigilant in addressing
how others, like Twitter, maintaining an account for the
Kremlin; or TikTok, reinstating Russian, state-owned media, are
complicit in spreading it, as well as other disinformation
campaigns from Putin. This has real-world consequences for the
Ukrainians and our other European allies.
We all share the goal of holding Big Tech accountable,
especially for our children. Big Tech platforms are my biggest
fear as a parent. Nearly every conversation I have with parents
in schools these days comes back to the concerns about Big Tech
and our kids. It is echoed by pediatricians, school
administrators, and teachers. Big Tech are not advocates for
children. Our kids, the users, are their product, and they are
being manipulated and exploited for profit. They are being
exposed to cruelty, bullying, and induced to self-harm. These
harms have been worsened by the move to virtual life because of
government-imposed school closures and lockdowns.
We must--this committee can and must--do more to address
this crisis. If we are going to address issues with Big Tech
and the advertising industry, that discussion includes
improving transparency and accountability for how these
businesses are collecting personal information, especially from
our children. My Republican colleagues and I unveiled a privacy
and data security framework last year. Addressing these issues
in a hearing would help us make progress.
One bipartisan bill we asked to be considered is the
Walberg-Rush Child Online Privacy Protection Act update.
Despite many changes being considered on the collection and use
of data today, unfortunately, this one is still missing on the
schedule.
Also, if we are discussing emerging technologies like
artificial intelligence, we should be addressing how AI can
strengthen American leadership and reflect our values of
freedom, human rights, and human dignity. These are issues that
require time, education, and hearings to debate and identify
common ground.
A key component for many of these issues is the authority
Congress grants to the Federal Trade Commission. The bills
today proposed by the Democrats enact sweeping changes for the
FTC. I appreciate that the Commission wants to be more involved
in improving data security. We have been clear that we are
willing to negotiate on additional authority, but changes like
this must be given the appropriate time for discussion and
vetting.
We need to start by ensuring accountability and
transparency from the FTC on its current authorities before we
give them more power. The FTC should come before this committee
to help us better understand how they would use additional
authority, as well as provide us assurances it has its own
house in order.
For this reason I sent a letter to Chair Khan yesterday on
the FTC's recent enforcement announcements for the security
vulnerabilities found in the open source software--
specifically, Log4J. Recent testimony in the Senate suggests
government systems may have used Log4J, so we must learn
whether agencies under our jurisdiction have system
vulnerabilities that could be exploited specifically by foreign
actors like Russia or China.
Missing from today's witness panel is TikTok, who declined
an invitation to testify. And it is not the first time. TikTok
threatens the safety, mental health, and well-being of our
kids. A recent Wall Street Journal investigation found teen
girls are developing tics from content they watch on TikTok.
This is alarming, and their complete carelessness with people's
personal information is even more concerning.
TikTok's ties to China raise significant concerns regarding
the amount of access the CCP has to information of Americans.
We know TikTok's parent company, ByteDance, is an extension of
this authoritarian regime, and now we see reports that TikTok
is allowing Russian Government propaganda to proliferate.
TikTok operates with disregard for U.S. national security
concerns. Our witness today, Officer Duffey, can share more on
how they challenge the reach of local law enforcement, as well.
These concerns about privacy and Big Tech are issues we
have been leading on, and we look forward to working together
to address.
[The prepared statement of Mrs. Rodgers follows:]

Prepared Statement of Hon. Cathy McMorris Rodgers

Mrs. Rodgers. I yield back.
Ms. Schakowsky. The gentlelady yields back, and the Chair
would like to remind members that, pursuant to committee rules,
that all Members' written opening statements shall be included
in the record.
And now I would like to introduce our witnesses for today's
hearing. As you can see, two are here. Is that three in person?
Voice. Yes, just----
Ms. Schakowsky. Oh, three in person and one remotely. And I
want to welcome Laurel Lehman, who is Policy Analyst at
Consumer Reports; Katie McInnis, who is Senior Policy--Public
Policy Manager at Duck Duck Goose.
Voice. DuckDuckGo.
[Laughter.]
Ms. Schakowsky. I am sorry. Duck, Duck Goose, that is a
game. Duck Duck Go. OK, sorry about that.
OK, I want to also welcome Mike Duffey, who is Special
Agent Supervisor in charge at the Florida Department of Law
Enforcement.
And Mutale Nkonde, who is the Founder and Chief Executive
Officer at AI--excuse me, of AI at the--what is this, public?
Voice. AI for the People.
Ms. Schakowsky. Oh, AI for the People U.S. There we go.
So at this time the Chair will recognize each witness for 5
minutes to provide their opening statements.
But before we begin, I want to make sure that I call
attention for the witnesses that are testifying in person. In
front of you, you--some of you have testified before, but if
not, you will notice that there are lights in front of you, and
the light will initially be green. The green will turn yellow
when you have 1-minute remaining. And please begin to wrap up
your testimony at that point. The light will turn red when your
time has expired.
For the witness who is testifying remotely, there is a
timer on your screen that will count down your remaining time.
And with that, let me ask Ms. Lehman.
You are recognized now for your 5 minutes.
Ms. Lehman. Thank you, Chair Schakowsky.
Ms. Schakowsky. Microphone.
Voice. We can't hear you. Is it on?
Voice. Maybe closer----
Voice. Pull it closer.
Ms. Lehman. All right. There we go?
Voice. There we----
Ms. Schakowsky. There we go.
Ms. Lehman. All right.
Voice. Fine.
Ms. Lehman. All right. Thank you, Chair Schakowsky.

STATEMENT OF LAUREL LEHMAN, POLICY ANALYST, CONSUMER REPORTS;
KATIE MCINNIS, SENIOR PUBLIC POLICY MANAGER U.S., DUCKDUCKGO,
INC.; MIKE DUFFEY, SPECIAL AGENT SUPERVISOR, FLORIDA DEPARTMENT
OF LAW ENFORCEMENT; AND MUTALE NKONDE, CHIEF EXECUTIVE OFFICER,
AI FOR THE PEOPLE U.S.

STATEMENT OF LAUREL LEHMAN

Ms. Lehman. Chairman Pallone, Ranking Member Rodgers, Chair
Schakowsky, Ranking Member Bilirakis, and members of the
subcommittee, thank you for inviting Consumer Reports to
testify today on the crucial matter of protecting consumers
online.
My name is Laurel Lehman, and my work at CR focuses
specifically on platform accountability, so that is where I
will spend most of my time today.
For 86 years, consumers have turned to CR for answers to
questions like, ``Is this product safe? Is it worth my time or
money? What risks might it pose to my family?'' Consumers today
grapple with the same questions, but about their online
experiences. It is this platform safe? Is it worth my time? Are
the goods I am buying from this ad authentic? Why am I seeing
this ad again?
Today's hearing grapples with this century's version of
confronting the very same challenges that have driven CR's
fight for a fair, just, and transparent marketplace all along.
The bills before the subcommittee today seek these same aims,
and we are excited to work with you to refine, craft, and pass
legislation that can help us usher in the online ecosystem that
consumers deserve.
Consumers face a range of issues in the digital
marketplace. In a nationally representative survey, CR found
that, of the 94 percent of Americans who said they had ever had
at least 1 social media account, 84 percent had adjusted their
social media settings at some point to limit or filter content
in some way; 39 percent had elected to turn off targeted ads;
59 percent said that they had read news on a social media site
they initially believed to be true, but later learned was made
up; and 37 percent told us that they had, at some point, wanted
to change their privacy settings on a social media platform,
but didn't know how.
And their concerns are limited to social media. They expand
to e-commerce and the way that AI impacts their lives, as I
elaborate on in my written testimony.
Yet the digital marketplace is as varied in potential
solutions as it is in its problems. Just as one no recall
safety standard or warning label can unilaterally keep
consumers safe offline, the variety of challenges consumers
face online requires a range of evidence-based policy
solutions, from comprehensive transparency to expanded use of
section 5 authorities to approaches of consumer protection law
to cautious, narrow changes to section 230, and certainly
include a stronger, better-funded FTC.
Auto and product manufacturers are expected to conduct
safety testing, and CR can test cars and appliances for
performance under stress to see how manufacturer claims line up
with their advertising. Digital product manufacturers, however,
have no such obligations to research, mitigate, or disclose
risks or dangers in the ways their systems work. They are not
required to publish clear community guidelines or terms of
service, or report on how effective their enforcement of such
guidelines may be. And they make no guarantees they are
appropriately staffing and equipping the teams that--dedicated
to keeping consumers safe from harassment, spam, counterfeit
products, hate speech, and misinformation.
Transparency will be fundamental to all such evidence-based
policy. In particular, three different kinds. I talk about
pipelines, processes, and personnel.
When I talk about pipeline transparency, we are talking
about what factors influence what a consumer sees online.
Whether that is an algorithm, or whether that is an ad, what
components go into that?
When we talk about process transparency, consumers deserve
to know the rules of the road. What can they expect from their
online communities, and how can they expect those rules to be
enforced?
When we talk about personnel, we are talking about, when
platform fail consumers, how can we make sure that platform
whistleblowers are empowered to make sure the public knows? And
also, how we are making sure that platforms are appropriately
staffed to enforce their terms of service everywhere they
operate?
CR's test track can figure out how cars handle stress
because we have speedometers. Right now no one can crash test
ads and algorithmic recommendations, except the platforms that
profit from them.
But transparency is the floor, not the ceiling, when it
comes to building a more responsible online information
ecosystem. Platforms clearly require stronger incentives to
take responsibility for the harms they compound and accelerate.
Across consumer products, it is well understood that
companies should bear responsibility for design and process
choices related to foreseeable preventable harms. Meanwhile,
YouTube failed to ban vaccine misinformation until September
2021, when platforms like Pinterest had started as early as
February 2019. CR investigations in 2020 show that Facebook
failed--that Facebook had approved ads with COVID-19
misinformation.
Online platforms fail to take reasonable baseline steps to
ensure their products are designed and managed responsibly.
Consumers should be able to expect an online system where we
can trust that platforms have values past those which they
return to their shareholders.
At the end of the day, consumers deserve a safe, just, and
transparent digital marketplace that they can trust. Congress
can and must work to pass legislation that makes it possible,
and CR looks forward to working with you in pursuit of that
aim.
Thank you again for inviting us here today. I look forward
to your questions, and to today's discussion.
[The prepared statement of Ms. Lehman follows:]

Ms. Schakowsky. Well, thank you. And now Ms. McInnis from
DuckDuckGo is recognized for 5 minutes.

STATEMENT OF KATIE McINNIS

Ms. McInnis. Thank you, Chair. Chair Schakowsky, Ranking
Member Bilirakis, Chairman Pallone, Ranking Member McMorris
Rodgers, and members of the subcommittee, thank you for holding
this important hearing and inviting me to testify. I am here to
discuss DuckDuckGo's privacy-protected business model, and why
we need bills like the Banning Surveillance Advertising Act to
protect individuals online and to reform the general market.
DuckDuckGo is a privacy technology company. We believe that
privacy is a human right, and that being private online should
be simple and accessible to all. With one download of the
DuckDuckGo privacy browser for mobile or the DuckDuckGo privacy
essentials browser extension for desktop, we offer seamless
protection from surveillance ads by blocking trackers. This
includes our email protection service, our tracker blocker
technology, and our private search engine, which is the fourth
largest in the United States, and serves over three billion
searches a month, globally. Our company was founded in 2008 in
Valley Forge, Pennsylvania. And year after year, we have seen
record growth, proving users' interests in being private
online.
DuckDuckGo is robustly profitable, and has been since 2014,
thanks to our contextual ad model. Contextual advertisements
are based on the content being shown on the screen, and not--
and do not need to know anything about you. So if you go to
DuckDuckGo and perform a search--let's say you search for
mobile phones, we are going to serve you with ads for phones.
It is as simple as that.
Behavioral advertising, by contrast, is based on personal
profiles from data collected both on and offline about you.
Have you ever searched for something online, and then saw an ad
for that exact same thing pop up on another website, or an app?
Or maybe you thought that your phone is listening to you, based
on the creepy ads that you are seeing online? That is
surveillance advertising. And rather than finding these ads
useful, a majority of Americans say that this is a
misappropriate use of their data. And it is no surprise this is
a massive violation of their user privacy and data collection
expectations.
And it is not just that. Data collection also results in
additional harm, like discrimination, identity theft, fraud,
scams, and filter bubbles. This same personal information that
allows a company to target you with an ad also enables them to
discriminate against you and others in the presentation of
critical opportunities. And these harms are not just
theoretical.
For instance, the Department of Housing and Urban
Development brought a case against Facebook, charging them with
housing discrimination by allowing advertisers to restrict who
saw ads based on race, religion, and national origin. This is a
harm that was made possible, thanks to data collection for
surveillance ads.
Online surveillance also enabled advertisers to use
Google's ad tracking--ad targeting algorithm to present more
men than women with ads for higher-paying jobs. The practice of
data collection, therefore, is not just about a user's privacy
or data collection expectations, but about the presentation of
critical access to opportunities and employment.
Consumers should have an easy and effective way to avoid
this surveillance. And as our history makes clear, internet
companies can be successful and profitable with a contextual ad
model. Studies have additionally shown that publishers receive
very little significant increase in revenue by using behavioral
ads. And in fact, many online advertising companies started
with a contextual ad model, including Google. Until recently
even, most of the ads you are seeing online were contextual
ads. However, due to Google's acquisition of DoubleClick and
Facebook's ad--Facebook's control of the ad duopoly with
Google, there has been a focus on behavioral advertising, and
innovation in contextual advertising has been severely
suppressed.
If data collection for targeted ads were banned, access to
personal data would no longer determine whether companies would
succeed or fail in the advertising market, meaning that more
companies would be able to compete against Google and
Facebook's ad duopoly.
As surveillance ads result in measurable harm for users and
little to no increase in revenue for publishers, we must ask,
``What is all this surveillance for, if it is not just lining
the pockets of Facebook and Google''
As a House Antitrust Subcommittee report and reports from
governments around the world demonstrate, access to personal
data enables Big Tech to protect and entrench their dominant
status online, generally, and in the ad market, in particular.
A shift back to contextual advertising, therefore, would
significantly reduce Google and Facebook's inherent advantages
in advertising and beyond.
Our mission at DuckDuckGo is to raise the standard of trust
online. We therefore support bills like the Banning
Surveillance Advertising Act that would do exactly that.
Consumers should be protected from the harm surveillance
advertising and data collection cause, like discrimination,
identity theft, scams, and fraud.
We believe that getting privacy online should be as simple
as closing the blinds. Therefore, we urge Congress to pass
bills that allow individuals to easily protect themselves and
would also have the benefit of strengthening the online market.
I am pleased to answer your questions today, and make
myself available to members in the future for in-depth
discussions about this bill. Thank you.
[The prepared statement of Ms. McInnis follows:]

Ms. Schakowsky. Thank you. And now it is my pleasure to
invite Mr. Duffey for 5 minutes.

STATEMENT OF MIKE DUFFEY

Mr. Duffey. Thank you, Madam Chair Schakowsky and Ranking
Member Bilirakis, and members of the subcommittee. Thank you
for inviting me today. I serve as assistant special agent in
charge of Florida Department of Law Enforcement's--crimes unit.
I started my law enforcement career during the AOL dial-up
days. And in the 25 years since, I have seen massive changes in
the way crime is committed and investigated. In that time I
have learned that, whether crimes are committed in the virtual
world or in the physical world, technology companies possess a
large amount of essential evidence law enforcement needs to do
our job. But lack the regulatory framework that enables
efficient and lawful access to that evidence means that we are
ineffective at reducing criminal threats, preventing
victimization, or getting justice for the victims (sic).
My investigative unit at FDLE, like thousands of others
across local, State, and Federal law enforcement, is in a
never-ending race to improve our access to digital evidence. We
need money for training and tools to access evidence we can
obtain from devices that we seize. But we are also facing
increased challenges with the complete inability to access
digital evidence when platforms deploy end-to-end encryption.
Congress can help us address both of these issues.
I want to share today with you a few examples of the
challenges we are facing. Service providers are subject to
little or no data retention requirements. In contrast, the
banking industry is obligated to keep financial records for a
certain period of time.
In one example of how this can hurt our ability to respond
to a crisis, a young person was livestreaming how he was going
to commit suicide, potentially in front of a live audience. We
immediately contacted the service provider that we had received
this information, and to try to attempt to determine where the
user was located. We noted that the live stream appeared to
have ended, and the content provider was unable to find the
record of the streaming event. We had nothing to followup on,
if it had not been for the family's own posting that they had
intervened.
The lack of data retention requirements frustrates the most
traumatic cases we deal with every day, which is the child
sexual abuse cases. Service providers do--who do identify CSAM
on their platforms are required to report it to our partners at
the National Center for Missing and Exploited Children. An
investigation then begins with an agency serving legal process
to a company for internet protocol information in an attempt to
identify where the activities were occurring. If the provider
does not retain data, a user has deleted it, the child predator
becomes harder to identify and stop.
The lack of standard terminology regarding the exchange of
legal process between law enforcement and service providers
leads to great confusion. Unless the terms we use to determine
certain types of data matches within the own company's unique
corporate terms, law enforcement must engage in a lengthy back
and forth, costing valuable time in an investigation.
We have seen some service providers implement online
portals that are designed to facilitate the requests from law
enforcement. These portals have been helpful in terms of
improving timeliness and security of data that--exchanged
between both parties. The response time is anywhere from 1
month to 1 day. This lag can ultimately be dangerous to the
cases we investigate.
Most tech companies routinely provide law enforcement
information when they really need--in an emergency. But
determining the exigent is actually in the hands of tech
companies. As a matter of Federal law, we in law enforcement
have the most relevant facts and content to determine exigency,
but yet the companies are the ones who have the final say.
In one example, an individual made comments regarding being
excited about July the 9th, and wanting to do what Nikolas Cruz
did. Cruz was the murderer who took 17 innocent lives and
injured 17 others at Marjory Stoneman Douglas High School,
Parkland, Florida, in 2018. This individual's social media post
had indicated they idolized Nikolas Cruz, and appeared to have
visited the location where Cruz was arrested. Upon review of
this information law enforcement provided the social media
provider when asking relevant information about the user, the
company unilaterally made a determination that they did not
think the situation was an immediate threat at this time.
Big Tech platforms create--Big Tech platforms have
created--have transformed society in many ways for the better.
Others less so. Users of these platforms should expect that law
enforcement officers charged with protecting them have a clear
path to the evidence they need to punish the guilty and
exonerate the innocent. The establishment of regulatory
framework includes standardized legal processes, guidelines,
and address the issues that would benefit industry, law
enforcement and, most importantly, the citizens we serve (sic).
Thank you again for your invitation, and I look forward to
your questions.
[The prepared statement of Mr. Duffey follows:]

Ms. Schakowsky. Thank you.
And now, Mr. Nkonde, I welcome you for 5 minutes for your
testimony.

STATEMENT OF MUTALE NKONDE

Ms. Nkonde. Good morning, Chair Schakowsky, Ranking Member
Bilirakis----
Ms. Schakowsky. Oh, Ms. I am sorry.
Ms. Nkonde. No problem. And Ranking Member McMorris
Rodgers, my name is Mutale Nkonde, and I am the founder and
executive director of AI for the People, a national non-profit
that uses strategic communications to really insert
conversations around the impact that technology is having on
society into public life.
I am here today in support of the Algorithmic
Accountability Act that I have been working on, really, for the
last 5 years, as an advocate, and something that I am happy
that this committee is finally looking for.
As a storyteller, the best way for me to really express the
impact that machine learning protocols have on public life is
through an illustration. So this committee is very aware of the
dangers of social media and algorithmic decisionmaking, so I am
not going to go long, but I think that this will be
illustrative in terms of the need for this Act.
In February 2022, Twitter users noticed that they were
being shown ads from a company called OpenSea. OpenSea is a
provider of NFTs or non-fungible tokens, which are parts of
currency based in blockchain that can be traded. What these
users were being offered were the ability to buy what were
described as Meta Slaves. These were avatars of African
American people that were put up for sale in the same way that
people were as sold in times of enslavement during this
country.
Upon AI for the People contacting Twitter to say that this
had happened, we found that the account had been taken down
from the platform. But news reports found that, instead of
removing that lot, what OpenSea actually did was just add
Asians and other non-White Americans to their platform.
How did those slaves get onto those social media sites?
Through a process called machine learning that informs our
algorithmic decisionmakers and decides who and what people get
to see on their social media feeds. This is an example of the
way the combination of targeted advertising online, racial
animus toward non-White Americans, and machine learning
conflates to create unsafe online environments for Black and
other--Black people and other members of protected classes.
The Algorithmic Accountability Act is important, because
what that would do would be to introduce impact assessments so
that an office within the FTC could look at this potential use
case, and then decide. Is this in the national best interest? I
would argue that it is not, because we know, from the
investigation conducted by Special Counsel Robert Mueller, one
of the tactics of hostile nations such as Russia is to really
incite racial divides and therefore, I would argue, undermine
our national security as we divide against ourselves, instead
of looking outward to protecting this great nation.
I argue that the Algorithmic Accountability Act, and even
its what I would consider in many ways to be companion
legislation, the Surveillance Advertising Act, provide an
online environment in which people from protected classes can
be safe.
AI for the People, as an organization, really does seek to
highlight the numerous ways in which advanced technological
systems--and as we are looking to things like the Metaverse,
Web3, and NFTs, technologies that we are not really commonly
speaking about, we need to keep the protection of civil rights
in mind. These platforms are enriching the few, without
thinking about the ways in which they comply with laws that we
have already standing.
And as I finish my remarks, because of intellectual
property laws--excuse the background noise. I sit--I come to
you from Brooklyn, New York. Unfortunately, this is a noisy
part of the country, so it won't be long.
But the one thing I will say is, because of international--
intellectual property laws, researchers and advocates do not
have the ability to look at the source code to understand how
these decisions are being made. So the passage of the
Algorithmic Accountability Act will not only safeguard those of
us from protected classes, but also give us insight into how
these technologies make decisions that really have incredible
impacts on the lives of many Americans.
I look forward to the discussion. I thank you for the
invitation, and I yield my time.
[The prepared statement of Ms. Nkonde follows:]

Ms. Schakowsky. Thank you, Ms. Nkonde, and we have now
concluded our witnesses' opening statements. At this time we
will move to member questions.
Each member will have 5 minutes to ask questions of our
witnesses.
I want to thank our witnesses for being here today.
And since we have witnesses appearing virtually, I need to
ask my colleagues in the hearing room to mute themselves
whenever they are not directing--directly speaking during their
Q&A portion, so that we can clearly hear the witnesses'
remarks.
I will begin with my 5 minutes of questions.
So targeted advertising is at the heart of the online
consumer model--online business model. But it fundamentally
violates users' privacy in a way that would never be accepted
if consumers were actually given the choice. That is why I
joined Representative Eshoo in introducing the Banning
Surveillance Advertising Act, and this legislation prohibits
companies from targeting advertising--advertisement based on
personal information.
So, Ms. McInnis, your company already bans targeting--
targeted advertising. Can you tell us why you made this
decision?
Ms. McInnis. Thank you for the question, Chair. Our founder
and CEO started out DuckDuckGo by wanting to serve answers--
serve users with great instant answers at the top of the search
engine results page. And he was really looking for his company
to have a positive impact. As he was developing this company in
2008, he saw that there was a clear need for private search
engines. And so we developed privacy for the search engine,
have been expanding our privacy options ever since.
And thank you for noting that DuckDuckGo helps stem
surveillance ads. We are working very hard to ensure that our
users are protected as they do whatever they need to do online
by blocking trackers in the first instance, so that information
is not collected about them.
Ms. Schakowsky. And is your business still profitable,
without the use of targeted advertising?
Ms. McInnis. Yes, we are very profitable. Our revenue is
somewhere around 100 million per year. We have over 30 million
U.S. users in the United States, and they span the whole nation
and the political spectrum.
Ms. Schakowsky. Thank you. When you were asking whether
people get frustrated when they see that there is an ad that
pops up, you know, ``How did they know to target me?'' And
people feel uncomfortable online because of that.
In our last tech hearing we discussed the impact of
whistleblowers protection at the Federal Trade Commission. I am
so glad to see that provisions in Mrs. Trahan's bill to protect
whistleblowers, similar to those in the FTC--in the FTC
Whistleblower Act that she and I have introduced in November.
So I wanted to ask Ms. Lehman, would consumers be better
protected from online harm if the FTC whistleblowers were
protected by Federal law?
Ms. Lehman. Thank you, Chair Schakowsky. Absolutely, yes.
We are excited about a number of the transparency
provisions before us today. There is context that--as many
transparency reports as we have, as much nuance as we can get
into, as much research as we do, there is context that you
could only ever get when someone can tell you why a decision
was made. And so for that reason, whistleblowers are critical
to providing that context for consumers.
Ms. Schakowsky. I really don't have any more questions,
except to say that we are going to move along as fast as we can
to protect consumers from the harms that are out there, from
the information that they don't want collected, and we want to
do it in these--the bills that are introduced today that we are
discussing, but also to move quickly, and efficiently, and,
very hopefully, in a bipartisan way to do comprehensive privacy
legislation.
The time to regulate is now. The time to hold Big Tech
accountable is now. We have heard over and over again the
promises, the denials of wrongdoing that we have heard from Big
Tech and, really, enough is enough. Consumers want to be able
to safely and confidently use what they can online, and not be
exploited as they have been.
So I yield back, and now will--Mr. Bilirakis for his 5
minutes of questions.
Mr. Bilirakis. Thank you, Madam Chair. I appreciate it very
much.
Mr. Duffey, I thank you again for your testimony. Again, we
appreciate the valuable insight you bring to this discussion.
I don't know if he can hear me.
But Mr. Duffey, we were having a little difficulty hearing
you before, in your opening remarks. So hopefully we have--you
know, we have fixed that. I don't know, maybe you need to speak
into the microphone. You know, your information is so valuable,
so we want to catch everything.
So anyway, I have a question for you. I am gravely
concerned about the crimes perpetrated on Big Tech platforms,
such as the sale of deadly fentanyl on Snapchat. And I believe
companies are not doing enough to help solve these crimes. I
mean, enough is enough, as the chairman said. Not--again, we
can't allow this to happen anymore. Kids are dying, for crying
out loud.
And so in fact, in our subcommittee, the Health
Subcommittee, we had--again, we had somebody testify on the
fentanyl crisis in December. And Chairman Eshoo agreed with my
assessment that Big Tech companies aren't doing enough to
coordinate the DEA on drug sales, in particular.
OK, that is why I recently introduced the CAPTURE Act,
which will help us to develop legislation to assist social
media companies to better communicate with law enforcement,
specifically in relation to events of imminent injury or death
to individuals.
So from your experience, sir--and I appreciate you
testifying and, again, you are doing a great job in our state--
so from your experience, how has Big Tech failed at cooperating
with law enforcement to assist in solving these dangerous
crimes? And I would like for you to elaborate, sir.
Mr. Duffey. Thank you, Representative. Can you hear me
better now?
Mr. Bilirakis. Yes, we can.
Mr. Duffey. OK----
Mr. Bilirakis. Madam Chair, can we hear better? Yes, I
think so. Thank you. I can. Thank you.
Mr. Duffey. OK. So they have failed in the sense that I
don't think what we see is some of the--they don't understand
the trends that today's youth is communicating by on their
platforms. The data retention, when we do----
[Audio malfunction.]
Mr. Duffey  [continuing]. Investigation, is an issue, and--
is what they do and what they don't keep, and for how long they
keep it.
The content moderation, they build--what we have seen is a
lot of these companies build a platform for one type of use,
and then, as you explained, it gets used and abused for other
areas.
So I think the failure is the lack of communication in--
between Big Tech and law enforcement to discuss the trends, to
discuss the acronyms, and the use of emojis, and the different
codes by which today's youth communicate.
Mr. Bilirakis. Yes, sir, when you speak slower, we can hear
a lot better, so let's try that.
Ms. Schakowsky. And stay real close to the mike.
Mr. Bilirakis. Yes, and stay close to the mike. All right,
we really appreciate it.
What more can Big Tech do, in your opinion, do you
believe--again, this legislation that I filed, the CAPTURE
Act--can help accomplish this?
And then also, as I mentioned in my testimony, we invited
TikTok to testify at today's hearing to answer how they have
failed to protect our youth online. They again ignored our
requests. OK, we are not going to give up, we are going to have
them here, and they are going to have to answer our questions.
I know the Chair agrees with me. She is the leader in this
area. It is clear they are not investing the time or resources
needed to adequately curb dangerous content on their platform.
Are you familiar with the dangerous viral changes that
populate the site, and how schools are scrambling to stay ahead
of the harms to other students and teachers alike?
If so, can you elaborate on how this has stretched the
resources?
And again, the teachers, the police officers, we can't do--
they can't do it all. They need our help. So it has stretched
the resources for Florida police, I know. If you can elaborate
on that, I would appreciate it.
Mr. Duffey. Thank you, Representative. I agree with you 100
percent. The resources that we in law enforcement have for
identification of these viral challenges is, a lot of times,
brought to our attention by parents or school resources
officers who are within the school system, who are listening to
the kids, watching their activity, and hearing firsthand----
[Audio malfunction.]
Mr. Duffey. As you mentioned, we don't have----
[Audio malfunction.]
Mr. Duffey. We are left to prioritize the different types
of leads which we can investigate properly, and then identify
the ones that we can----
[Audio malfunction.]
Mr. Bilirakis. Thank you. I appreciate it. I yield back,
Madam Chair. Hopefully, we can get this cleared up and
straightened out, because I know a lot of members here, at
least on my side, want to talk to Mr. Duffey, and get valuable
information from him.
So thank you, I appreciate it.
Ms. Schakowsky. Yes. And now I recognize the chairman of
the full committee, Mr. Pallone, for 5 minutes for questions.
Mr. Pallone. Thank you, Chair. And we are here today, as we
know, to have an open, meaningful dialog about solutions to
address the harm social media companies have caused, and this
is more timely than ever as we see social media companies
expanding their reach.
So I want to use Facebook as an example, which now goes by
Meta. We have seen reports of sexual assault and harassment in
the virtual reality world that Facebook is trying to create
with the Metaverse. So let me start with Ms. Nkonde.
Can you expand on this issue? What are some of the real
harms that we will see as social media companies expand their
reach, if you will?
Ms. Nkonde. Thank you for the question, Mr. Pallone. The
issue--just to read everybody in to the issue with the
Metaverse as it stands, the head of research of the Metaverse
recently wrote a
[inaudible] article, in which she told us that when she
went in, she was sexually abused by male avatars.
The issue is, in order to get this immersive experience,
what the Meta are currently doing, what their team is doing are
investigating how to use light sensors to make it feel like we
are seeing, audio sensors to make it feel like we are hearing,
and tactile sensors to make it feel like we are being touched.
Therefore, if you go into an immersive environment where
you really feel that these things are happening to you, then,
despite the fact that it is in a headset, you are still going
to have that very real experience of abuse. And without
regulation of what is called Web2, what we know of Facebook,
these harms are going to be perpetrated further in the virtual
environments.
Mr. Pallone. And what about--well, let me ask about
marginalized communities. How might they be impacted by these
changes that we are seeing?
Ms. Nkonde. Marginalized communities are actually the most
vulnerable within this.
And so the head of Metaverse research is a woman. She is a
protected class. So what we were seeing in her attack by male
avatars were really the logics of sexism and hatred of women
being enforced in a place where you can reach your target.
Another group that we are seeing more reports of are
children in the Metaverse, which is a whole explosion of
issues, specifically because people under 13 are not even
supposed to be on that platform. And so, when we think about
regulation, we need to think about who are the most vulnerable.
Could disabled avatar got away in those situations. There is a
history of racial violence in this country. If we see attacks
on women, it is logical that we are going to see attacks on
negatively racialized groups.
And it is committees like this and the bills that are being
put forward that are really going to keep us American people
safe.
Mr. Pallone. [Inaudible] instances. I mean, I think
increased transparency would go a long way in making the
internet a safer place, so let me ask Ms. Lehman.
How would the legislation before us today bring greater
transparency, and with a greater accountability, to these Big
Tech platforms, if you will?
Ms. Lehman. Thank you, Chair Pallone. So the transparency
in the bill across the board today is a crucial prerequisite to
really understanding and forming the kinds of harms we are
seeing across the board, and being able to have that kind of
context, being able to speak to the specific harms.
And so the bills before us today, particularly
Representative Clarke and Representative Trahan's bills, are--
in requiring impact assessments, for example, and requiring--it
requires companies to actually take account for what kinds of
harms, what kinds of risks do we need to consider, do we need
to think about, rather than building first and asking questions
later, once the harms have happened.
And I will yield back.
Mr. Pallone. I only have a little time left, but let me
just say we have consistently seen some social media companies
withhold critical information about how their systems work from
the public, which makes it difficult for consumer protection
agencies like the FTC to address the issues users face online.
Could you tell me, Ms. Lehman, how would these bills help
the FTC better protect consumers from unfair or deceptive
practices? But you only got 20 seconds.
Ms. Lehman. Thank you, I will try. The short answer is it
helps inform how we do it.
Eight seconds? Boy.
It offers so much more perspective to the FTC to understand
the problems and, therefore, to keep consumers safe in sort of
the mission.
[Inaudible] seconds, OK.
Mr. Pallone. I made you go too fast. Thank you.
Ms. Lehman. Thank you.
Mr. Pallone. Thank you, Madam Chair.
Ms. Schakowsky. The gentleman yields back. Let me just
remind everyone in the room, all members, to please unmute
after you have spoken. Apparently, we are getting some feedback
here.
I am sorry?
Voice. They have to mute their mic after they have spoken.
Ms. Schakowsky. Yes, to mute your mic after you have
spoken.
And now I recognize the ranking member of the full
committee, Mrs. Rodgers, for 5 minutes.
Mrs. Rodgers. Thank you, Madam Chair. There certainly is a
lot before us in this hearing, and I appreciate the attention.
The Republicans also have--we have a couple of dozen bills
around transparency and accountability.
Clearly, there is action that needs to take place, and we
have been working with a variety of stakeholders getting input.
I know that, in committee, there is 35 bills that have received
a legislative hearing in the subcommittee. Three have been
marked up, and two have received the bipartisan support.
I really believe--and what I want to make the request of--
is that we work together, that this is a problem that is going
to require us seeking input and support from a lot of different
entities, Republicans and Democrats.
But beyond that, I believe that we need to be hearing from
the advertising industry, AI developers, small businesses, even
the FTC, for that matter, on how this is going to work, how
they are going to operate after this--what you might call an
extreme makeover, FTC addition.
And I fundamentally believe that we need to be addressing a
privacy law. We need a national privacy framework in order to
gird up what the whole space is right now. And so we have
drafted legislation, and are anxious to work with our
colleagues across the aisle on a privacy standard. Certainly,
there is an opportunity to find some common ground.
On some of the issues before us today, I wanted to start
with Ms. McInnis. Given recent reports on Big Tech needing to
do more to protect the people of Ukraine from the
misinformation campaigns generated by the Kremlin and its
still-active Twitter account, I have a couple of questions to
give you an opportunity to reflect on DuckDuckGo's part in this
debate.
Does your company still maintain a partnership with the
Russian search engine Yandex?
Ms. McInnis. In light of Russia's assault on democracy in
Ukraine, we have paused our relationship with Yandex.
Mrs. Rodgers. Thank you----
Ms. McInnis. Yandex was used to provide traditional links,
meaning non-news links on the search engine results page, in
Russia and Turkey.
Mrs. Rodgers. Thank you. Given you raise revenue via
contextual advertising and affiliated programs, are you able to
say whether such revenue has ever come from Russian State media
sources in this partnership?
Ms. McInnis. I do not know. Our advertisements are provided
through Bing's contextual ads program. And so Microsoft's
advertising would have the best information about that.
That said, we are--yes, I think that they would be the best
ones.
Mrs. Rodgers. OK, thank you. Do you--or how do you see we
best address concerns that we have over Chinese-owned TikTok,
and how it may be influenced by Russia via its alliance with
China?
Ms. McInnis. Thank you, Ranking Member. Ranking Member
Bilirakis and Ranking Member McMorris Rodgers, you have talked
extensively about the need to protect children online.
Although we have that Children's Online Privacy Protection
Act, banning the use of surveillance ads would, I think, help
protect children by banning the collection of personal
information, no matter their age, which I think would help
solve for some of the issues that we have been seeing online,
where dominant tech companies say, ``We are not sure if
children are on our platform, we don't actually know that they
are there.'' And so, if we are protecting everyone, we are
protecting children more.
In addition, I think having some privacy laws like the
general one you mentioned, or the Banning Surveillance
Advertising bill, which would--of course, would protect
privacy--would stem the ability of platforms like TikTok to
collect information, regardless.
Mrs. Rodgers. OK. Well, I thank you, and I appreciate your
engagement and your support on the legislation. Fundamentally,
we need a privacy framework, and so we will continue to work on
that.
Officer Duffey, our law enforcement officers are American
heroes, and I have personally heard some very sad tales of the
role Big Tech played in tragic events impacting our children,
and the pressures on first responders. In your--you reference
a--exigent circumstances in suicide intervention. Can you
elaborate on the subjective constraints that social media
platforms put on law enforcement to intervene in harm to
children?
And can you give us a sense which social media companies do
a better job in cooperating?
Mr. Duffey. Thank you, Representative. Hopefully, we have
corrected some of the audio issues.
But the
[inaudible] exigent, when we serve a company with exigent
circumstances, it is left at their determination. We present
them with the facts as we are given. Ultimately, each
individual company is the final say-so on whether they
determine there is a threat to a person's life or others----
Mrs. Rodgers. And which companies are cooperating?
Mr. Duffey. It varies. The last one that I referenced in my
statement was TikTok.
Mrs. Rodgers. OK. OK, thank you. I have run out of time. I
yield back.
Ms. Schakowsky. OK. Who is next?
My colleague from the Chicago area, Congressman Bobby Rush,
you are recognized for 5 minutes.
Mr. Rush. Well, thank you, Madam Chairman, for this
magnificent hearing. My question is directed to Ms. Nkonde.
Ms. Nkonde, you mentioned facial recognition in your
testimony. The error-prone nature of facial recognition
software, especially when it comes to identifying people of
color and women, is extremely dangerous.
In 2018 an ACLU study found that Amazon's facial
recognition software falsely matched 28 Members of Congress,
including me, with someone else's mug shot. We are seeing the
real-world impact of this when, in June 2020, New York--the New
York Times reported on the case of Robert Julian-Borchak
Williams of Michigan, who was arrested by Detroit police
because he was misidentified by facial recognition software.
Later it was revealed that this software was used ``almost
exclusively against Black people'' and was used even though the
Detroit police chief estimated that it misidentified
individuals 96 percent of the time. Clearly, this is a very
serious problem with very, very serious and very real
implications.
Ms. Nkonde, can you please speak to the role algorithms
play in these kinds of situations?
Does the solution to this problem currently exist?
If so, what needs to be done to implement them?
And if not, what do we need to do, as Members of Congress,
to address these problems?
Ms. Nkonde. Thank you for the question, Representative
Rush. Big questions. But yes.
So the way facial recognition works is through a computer
engineering protocol called computer vision. The training data
are pictures of people's faces, and then the algorithm will
take a face graph. So it basically measures your face in what
we call facial architecture: color of the skin, broadness of
the nose, circumference of the eyes, for example.
The issue within the Gender Shades report, which kind of
undergirded this idea that facial recognition technology did
not work in the Amazon, Microsoft, and IBM report, found that
the training data was using that of White men, which meant that
when those same systems were used to identify non-White people
like Mr. Williams, who you mentioned, it misidentifies us.
Therefore, one of the things that Congress could do
immediately is really take the Algorithmic Accountability Act
to markup. Because through impact assessments, the FTC would be
then able to figure out how--what is the efficacy of this
technology? Is it recognizing Black and Brown people? Where is
facial recognition being used?
Because many Americans believe it is just in law
enforcement, but they don't realize that when we use self-
checkout, for example, in pharmacies, they are using facial
recognition to figure out whether you have stolen toothpaste or
not, whether----
Mr. Rush. Ms. Nkonde, can you also include how it affects
us African Americans in the area of criminal justice and
banking?
Ms. Nkonde. Yes. In the area of criminal justice,
misidentification leads to misincarceration or arrest. And in
the area of banking or finances, the IRS has just been stopped
from using facial recognition in order to access our records.
And if you imagine that African Americans are not being
recognized, that would bar us from access to our own financial
information.
Mr. Rush. Thank you, Madam Chair. I yield back.
Ms. Schakowsky. Thank you, Mr. Rush.
Mr. Upton, you are recognized for 5 minutes for questions.
Mr. Upton. Well, thank you, Madam Chair, and I want to echo
the remarks of Cathy McMorris Rodgers. I hope that we can
develop a privacy standard. It is something that is bipartisan,
it is long overdue. I know it is tough to do, but I think we
need to proceed down that road.
For me, I am a dad, and I am also a granddad, so I sort of
like that Duck Duck Goose at the beginning.
[Laughter.]
Mr. Upton. I know where you were coming from, I think.
But all of us want to protect the most vulnerable, and that
is our kids and our grandkids. We want to make sure that there
is every tool in that toolbox so that we know what is going on,
and their lives can be protected, because it is no longer just
don't talk to strangers. There is a lot of strangers that are
out there. And that is why we need to make sure that those
tools are there.
And so, Mr. Duffey, I want to thank you for your service,
for coming before this committee to help us understand the ins
and outs of protecting our Nation's kids online. One of the
concerns that I hear about most from my constituents--and there
is a lot of them--is the way that Big Tech is impacting the
mental health of our kids.
So I just wonder if you can walk us through a step by step
of how you interact with a tech company, and how does that
change when there is a threat to a child that is viewed as
urgent?
And I would just recommend that you get as close to your
mike as you can, because the--despite this being the high tech
community, the connection is not all that great.
Mr. Duffey. Thank you, Representative. I am getting as
close as I can, so hopefully this audio sounds a little bit
better.
Mr. Upton. I knew it wasn't the Florida accent, I can
understand that better than Louisiana.
Mr. Duffey. Thank you. So really, that relationship with
big technology is only engaged upon an incident occurring. In
the case of a missing child, or the case of child sexual abuse
material, or a threat of violence, or an act of harm, we begin
that engagement with technology.
Outside of that, other than some of the big technology
companies, the startups and a lot of the newer companies, we
have very little engagement with them. Typically, our
engagement, when it begins, is through legal process as we
begin to try to investigate the issue at hand.
Mr. Upton. What is the value of the number of cases that
your department looks at every year, and has it been
increasing? What is--what has been the trend line?
Mr. Duffey. It has been increasing. Specifically as it
relates to the child sexual abuse material, we hear in the
State of Florida alone--law enforcement as a whole in the State
of Florida has received over 18,000 leads reported to us by the
National Center for Missing and Exploited Children alone in
2020. That means law enforcement has to put eyes on each one of
those leads to determine the ability to investigate
[inaudible] limited resources.
Mr. Upton. And where was that number--where were you, say,
10 years ago, or when you started? I know you have been with
law enforcement for a while. But as I hear from my law
enforcement folks, I mean, they just--they shake their head in
terms of some of the doors that get opened, and some of the
nasty stuff that really gets out there. How has that changed
from when you started?
Mr. Duffey. Well, exactly. When I started the--it was a--a
lot of what--the work we did was through the U.S. Postal
Service, because the online world didn't exist. And the online
world that existed was not as widely popular as it is today. So
we are seeing exponential increases as companies come online
and today's youth and others begin to leverage this platform to
communicate, which ultimately results in children being
sexually abused online, sextorted, and
[inaudible] their mental health.
Mr. Upton. Can you tell us one particular story of where
things worked out?
Mr. Duffey. There has been a few where we have gotten
incidences of somebody saying, ``I am watching a child being
sexually abused'' or a child was abducted and, through the use
of technology, we were able to work with the provider to
determine the location where that child or individual might be.
And through the use of technology, we were able to locate that
individual to prevent further abuse, or recover
[inaudible].
Mr. Upton. And are you able to interact that with Amber
Alert?
Mr. Duffey. We very much so are.
Mr. Upton. Well, thank you. Again, thank you for your
service.
I appreciate the hearing, and I yield back.
Ms. Schakowsky. I wanted to point out that there was much
more clarity with the response of our online witness because
you turned off your mic each time after you asked the question.
And I would recommend that for everyone who is asking questions
of our online witnesses.
The--and next, Ms. Castor, I recognize you for 5 minutes
for questions.
Ms. Castor. Well, thank you, Chair Schakowsky, for holding
this hearing, and thank you to all of our witnesses for
appearing today, and a special shout-out to Special Agent Mike
Duffey from the Florida Department of Law Enforcement.
Thank you, Special Agent Duffey, for your 25 years of
service, and especially on behalf of crimes against children
and online harm.
The subcommittee has now held several hearings on
legislation to hold Big Tech accountable, and the bills before
us today by Rep. Trahan and Rep Eshoo, and Rep. Clarke are
steps in the right direction. But I believe it is urgent that
we move legislation on online privacy issues, especially when
it comes to children.
Passing a core comprehensive privacy bill is central to
holding Big Tech accountable. And if we can adopt safeguards on
processing personal information, tech platforms will have less
of an incentive to use many of their manipulative and harmful
techniques that are currently deployed to increase engagement
and addict their users, many of which we have discussed in past
hearings.
So one of our witnesses--witness companies today,
DuckDuckGo, is a good example of this principle.
And with the online harm to kids being made plain over the
past few years, I work--I have been working for a number of
years to develop the Kids Privacy Act to provide parents the
necessary tools to protect their children, strengthen
enforcement so companies are held to account for improperly
collecting children's personal data, and for misuse of that
data. It also closes loopholes. It has--the bill has widespread
support. It has been endorsed by the leading child protection
organizations, parents, pediatricians, and privacy groups.
So to my Republican friends, we really need bipartisan
support on this effort. The UK is ahead of us. The EU is ahead
of us. And even the U.S. Senate, they are ahead of the House
when it comes to children's online privacy. They are now two
bipartisan bills over there. And while we all know that the
Senate is not known for action, they are ahead of us here. So
if they can come up with a bipartisan bill, so can we.
So I am really speaking out more as a mother than as a
Member of Congress. I have really--I have reached out to the
leadership, and I am doing so again because I am not going to
give up. I am--my door is open to any Member of Congress who
wants to enter into good faith negotiations over a bill that
protects children's privacy, their safety, and their health
online, especially in the face of everything that we know now
about crimes against kids, the rising rates of mental health
problems, and more.
I mean, the Facebook whistleblower was here--also made
plain that the Big Tech platforms, Facebook, Instagram, they
know about the harms, but they are more interested in getting
kids hooked and their profits than keeping kids safe online.
So thank you for indulging me on that. But please, my door
is open, and we really want to protect kids.
So my question for the witnesses today, each one of you,
yes or no, is a comprehensive Federal privacy law a critical
component of Big Tech accountability and protecting consumers
online?
Ms. Lehman, you can start.
Ms. Lehman. Yes.
Ms. Castor. Ms. McInnis?
Ms. McInnis. Absolutely.
Ms. Castor. Special Agent Duffey?
Mr. Duffey. Yes.
Ms. Castor. And Ms. Nkonde?
Ms. Nkonde. Yes.
Ms. Castor. And here is another yes-or-no. Should there be
special focus on protecting children's privacy, safety, and
health online, yes or no?
Ms. Lehman. Yes.
Ms. McInnis. Yes.
Mr. Duffey. Yes.
Ms. Nkonde. Yes.
Ms. Castor. Well, thank you.
And Special Agent Duffey, you--Representative Upton asked
you about a good news story, but tell us what you are seeing
right now because of the--just the widespread collection of
data on children and their interaction with online apps? What
tools do parents need? What do parents tell you right now that
they feel they need from policymakers here in Washington?
[Pause.]
Ms. Castor. Oh, Special Agent Duffey, your audio.
Mr. Duffey. Sorry about that. Representative, thank you.
Being a parent myself, and talking with other parents, the
biggest issue we hear is that they don't understand how to
implement parental controls. A lot of these companies implement
the process. The explanation of how to do it is not clear.
Ms. Castor. Thank you very much. It is obvious that we need
to rebalance the power here, and put the power in the hands of
parents, and not these Big Tech platforms.
Thank you. I yield back.
Ms. Schakowsky. Thank you.
Mr. Latta, you are recognized for 5 minutes.
Mr. Latta. Well, I thank my friend, the Chair, for today's
hearing, and I also thank our witnesses for appearing before us
today.
As part of the Republican Big Tech accountability platform,
many of the Members in this body have proposed a number of
reforms to the laws governing Big Tech. My proposal is to
remove section 230 liability protections from companies that
act as bad Samaritans and knowingly promote, solicit, or
facilitate illegal activity.
Broadly, I have serious concerns about some of the
activities that these companies are allowing to occur on their
platforms, whether it is explicitly permitting these
questionable activities or simply ignoring any illegal content
that they discover.
I believe the legislation before us today misses the mark.
H.R. 6796 would create a new bureau at the Federal Trade
Commission with new rulemaking and investigatory authorities to
define the code of conduct for online platforms. However, I do
not believe it is the FTC's duty or responsibility to be the
moderators of content on social media platforms. Rather, they
should act as the clearinghouse and inform consumers on Big
Tech's content moderation practices, including their
enforcement decisions and appeals decisions.
Additionally, the legislation would require the FTC to hire
500 staffers. I can only imagine this undertaking would set
back the FTC many years and further slow its rulemaking
drafting process and its ability to protect consumers.
Sadly, we aren't able to explore these concerns, as the
majority did not invite the FTC to testify today. Fortunately,
we do have the opportunity to hear from Mr. Duffey, a career
law enforcer who has dedicated his career to protecting
Americans from predators in the real world, and is now working
to protect Americans, especially children, from predators
online.
And Mr. Duffey, if I could begin my questions with you, in
your testimony it sounds like there were many instances of you
being reliant on the goodwill of these tech companies to fully
complete your law enforcement investigations. Would you speak
about your conversations with these companies, and how would
you characterize their concern for what is happening on their
platforms?
Mr. Duffey. Thank you, Representative.
With regards to your question, the conversation is,
sometimes with the newer tech companies, is it a loss. And what
I mean by that is they don't understand the pure volume of data
which they have, which makes it a challenge for us in law
enforcement to ask for the specific information that we are
referring to, specifically when we talk about the--what we will
call the word game.
When we begin to do our investigation, we serve each and
every company with the legal process asking for specific
content. If that content language that we are asking for
doesn't match up with the exact language that they have, we
begin this banter back and forth to try to identify the exact
content we are looking for, which is--which--transparency on
what they have would be greatly--you know, greatly enhance our
abilities to streamline some of our process.
Mr. Latta. Well, you know, one of the ways we can hold Big
Tech accountable is increase the transparency requirements.
You were just talking about transparency. Last year I
released a discussion draft to require companies to disclose
their content enforcement decisions related to child
pornography, child trafficking, cyber bullying, illegal sale of
drugs, foreign terrorism content, counterfeit products, revenge
porn, and doxing.
Do you believe that if law enforcement had more information
about how companies manage or conduct enforcement against these
types of activities, would it help you overall perform your
job?
Mr. Duffey. Yes, I believe it would. It would allow us to
have a full understanding. I think it would open that door, and
have the conversations with these tech companies that sometimes
stand to the side. Having a better understanding by law
enforcement would be a win for all.
Mr. Latta. Well, thank you very much.
And Madam Chair, before I yield back, I do have a letter to
the--for the committee from ANA, which I would like to ask
unanimous consent to put in the record.
Voice. We just have to check----
Ms. Schakowsky. Without objection.
[The information appears at the conclusion of the hearing.]
Mr. Latta. Thank you, and I yield back.
Ms. Schakowsky. Congresswoman Trahan, you are next,
recognized for 5 minutes.
Mrs. Trahan. Well, thank you, Madam Chair.
Throughout our country's history, bold reforms have been
born in moments of crisis. The Great Depression forced Congress
to create the Securities and Exchange Commission, tasked with
overseeing and regulating the market to protect investors. And
almost 80 years later, Congress passed the Dodd-Frank Act in
response to the 2008 financial crisis, strengthening consumer
protections against financial market abuse and creating
transparency and accountability requirements for the entire
financial system.
Yet today, crisis after crisis created by large technology
platforms have resulted in minimal Federal response. In fact,
crickets.
Under the leadership of Republican and Democratic-
controlled governments alike, a handful of U.S. companies have
become monopolies. They have optimized their platforms solely
for ad revenue and, in turn, they have become breeding grounds
for the spread of weaponized disinformation, hate speech, and
content that harms our children.
These issues have been closely examined. We have held
hearings with experts, we have yelled at executives, and we
have sent letters all saying the same thing: Do better. Yet the
end result has remained the same. Nothing changes. And
companies' stock prices hit new highs.
The focus of today's hearing may be on the legislative
proposals introduced by myself and my colleagues, but the
question is much simpler: How long can we continue this
inaction?
How long can we look at our children and say the change is
necessary, but we just haven't been able to enact it yet?
Congress has gotten off the sidelines in the past for
practically every other industry: cars, airplanes, and banks.
We employ key organizations that keep pace with new
developments, and inform regulations aimed to protect
consumers. We must get off the sidelines once again. Enough
listening to companies saying, ``Trust us, we have a process
for that.'' Enough internal bickering that ends any real chance
of progress. And enough watching Europe go first, and the
Senate.
I commend my colleagues on the committee, including our
panel's leadership, who recognize that we have everything we
need to act: the smoking guns, the historical precedent, and
the legislative text. All we need today is the willpower.
So Ms. Lehman, the Digital Services Oversight and Safety
Act creates a bureau at the FTC, staffed with experts employed
to issue rules related to public-facing transparency reports,
certified researchers with data access, and disclosures to the
Commission, so that we can shine sunlight on how consumer data
is collected and used. Could you please explain why
transparency requirements like these are so important?
Ms. Lehman. Absolutely, thank you. Transparency
requirements, particularly like those in your bill, we have a
variety of them, right? So we have users and advertisers and
individuals who need to understand what values the platforms
that they use operate on. They--so that parents can decide is
this platform--are these platforms' values coherent with what I
want my kids to be on?
And some of the most exciting components of the bill really
are that we have seen time and time again that there is an
allergy from the platforms to transparency and to
accountability, whether that is Facebook disabling crowd
sourcing, or disbanding the CrowdTangle team, whether that is
other--we have seen the list that--and so, having access to
understand, OK, what kinds of misinformation, how does it
spread, how can we fix these problems, we don't have the
context for that right now. And the parts of the bill that
shine that sunlight make that possible.
Mrs. Trahan. Well, thank you. And in your experience, how
quickly do social media companies change their products and
processes?
And why is it so important to have a bureau that is
flexible and nimble enough to quickly publish safety guidelines
or issue new rules for disclosures?
Ms. Lehman. Yes, I think particularly for--in this space
things can change instantaneously. I think we think we have
heard a little bit today about the Metaverse, and about, like,
what Web3 and AR and VR look like. If we think back to five or
six years ago, which is kind of a long time, in--sometimes in
legislative land, right?
You--what would--how would a--how could thinking through
live video--and the advent of live video was fairly
revolutionary, and the fact that video is happening on phones,
and how did that change, and what kinds--as we heard from Mr.
Duffey earlier, what kinds of harms can come from live video?
Those are the kinds of things we need to be able to pivot
instantaneously on, and--that we can't wait for.
Mrs. Trahan. I couldn't agree more. Certainly, there is so
much in the black box that we need to shine a light on so that
we can keep up. Not just the Congress, the FTC. I mean, if not
them, who is going to be armed with transparency in this
regard?
So I--the last thing--I know I am out of time, but I would
like to request unanimous consent to enter a report from the
NYU Stern Center for Business and Human Rights entitled,
``Enhancing the FTC's Consumer Protection Authority to Regulate
Social Media Companies.''
Ms. Schakowsky. Without objection----
Mrs. Trahan. Thank you, Madam Chair.
Ms. Schakowsky  [continuing]. So ordered.
[The information appears at the conclusion of the hearing.]
Ms. Schakowsky. OK. Mr. Guthrie, you are recognized for 5
minutes.
Mr. Guthrie. Thank you. Thank you, Madam Chair. I have a
letter from the U.S. Chamber of Commerce. It has been submitted
to your staff, but I would like to ask to enter into the
record.
Ms. Schakowsky. Without objection, so ordered.
[The information appears at the conclusion of the hearing.]
Mr. Guthrie. Thank you, Madam Chair. Thank you so much. And
thanks to everybody for being here today. And my questions are
for Mike Duffey.
And the United States has seen historic levels of opioid
abuse, leading to tragic deaths over the last several years.
According to the Kentucky Office of Drug Control Policy,
illicit fentanyl and its analogs were detected in more than 70
percent of all cases in Kentucky in 2020. The opioid crisis has
been exacerbated by deadly fentanyl being trafficked into our
communities through our southern border and on the social media
platforms millions of Americans, including our children, are
using.
The Energy and Commerce Committee has passed several bills
to address this epidemic. One bill--I know it is in a different
subcommittee, a subcommittee I have the honor of being ranking
member--but the HALT Fentanyl Act.
I mean, the idea that March 11th--after this expires--after
the CR expires, that illicit fentanyl will be street legal in
America is just wrong. I wish we could make it permanently
illegal. I do expect and hope that we can work together to at
least extend it, moving forward. But it is just absolutely
frustrating that we can't permanently schedule illicit fentanyl
that is--70 percent of Kentuckians who died of an overdose died
of illicit fentanyl in 2020.
But I am also concerned that--and where it ties into this
hearing--that illegal drugs are still available online through
illegal pharmacies, and even these widely popular social media
platforms. This type of illegal activity online is troubling
and inexcusable. I am working on a draft legislation as part of
the Republican Big Tech platform that will help prevent this
from happening on these sites by requiring internet platforms
to implement and maintain reasonable content moderation
policies and practices to address the illegal sale of drugs on
their platforms.
Additionally, the Federal Trade Commission and states
attorneys general would ensure enforcement of these policies.
So my question is for Mr. Duffey.
In your testimony, you discuss the challenges that tech
platforms present to law enforcement. Do you find that most
platforms have a formal process or framework by which to work
with law enforcement personnel?
And do you think legislation in this space will enhance law
enforcement's capabilities to track this illegal activity and
facilitate?
So what is the current status of these platforms, and what
would you like to see Congress do, Mr.----
Ms. Schakowsky. Unmute. That will help the--if you--we can
hear him better if you unmute. There we go.
Mr. Duffey. All right. Thank you, Representative. I do
believe that any type of framework would be to our advantage,
and a win for all.
Right now, as the----
[Audio malfunction.]
Mr. Duffey  [continuing]. Out of Big Tech, and how they
struggle to--they build a tool which is used by the general
population that becomes popular, and then gets misused. And
them addressing and having some framework and requirements as
to having a group that is able to handle the volume of law
enforcement compliance legal process that we serve, and
mandating that they build out these groups to respond to our
responses will not only help law enforcement investigative
efforts.
The other side of it is if these Big Tech corporations
enact encryption, then we are back to not being able to
investigate an opioid case where an individual has overdosed
and now we can't get into that mobile device to identify, nor
do we have the tools to try to get into this device to
determine who that person was communicating with to potentially
stop others from getting this drug, and work a criminal
investigation.
So having--first, having these companies be held
accountable to creating a outreach or law enforcement response
compliance group; second, allowing the law enforcement to have
the tools needed to help fight encryption. And I am not
indicating that--we are not asking for anything that we aren't
entitled to with proper legal process. If we can't get into
these locked devices, the person who overdosed, that child who
overdosed on the ground there, we can't identify where the
suspect may be.
Mr. Guthrie. Well, thank you, and I just have 20 seconds,
so I will yield back.
I don't have time for another question, so I will yield
back my time. Thank you, Madam Chair.
Ms. Schakowsky. The gentleman yields back, and next is Mr.
McNerney for 5 minutes.
Mr. McNerney. I thank the Chair, and I thank the gentleman
from Kentucky for yielding early.
[Laughter.]
Mr. McNerney. I thank the witnesses. Your testimony is very
compelling this morning.
Ms. McInnis, please describe for a minute or so how banning
surveillance ads would transform the online ecosystem.
Ms. McInnis. Absolutely, and thank you for the question,
Representative.
Right now, Facebook and Google control an ad duopoly in
online advertising sales because they have convinced the
advertising market and advertisers that behavioral ads are more
relevant and serve users with more targeted ads than others
would. There is no reason, however, why contextual
advertisements should not and could not be just as relevant as
behavioral ads.
DuckDuckGo has used contextual ads for our entire existence
to show users relevant offers in the moment. We think that, you
know, while you are searching for sneakers, you are likely
wanting to see ads about sneakers, and not an ad for a vacation
you might have taken a few months ago.
In addition, if this bill was enacted, many companies would
be prevented from the collection of online data, meaning that
their duopoly--Facebook and Google's duopoly--and ads would be
diminished. Companies would be able to compete against them
more forcefully in the market, and we would have more
innovative, contextual advertisement services for users. And
therefore, the ads would be more relevant and more useful to
users in the future.
Mr. McNerney. Thank you. And that was just a little more
than a minute.
Ms. Lehman, you argue that if companies are truly proud of
how effectively they protect consumers, you would expect them
to welcome independent investigators or researchers.
Unfortunately, that is not happening, which is why a
comprehensive approach to transparency created in the Digital
Services Oversight and Safety Act, H.R. 6796, is so important.
So please explain how increased transparency from this
legislation would result in concrete changes.
Ms. Lehman. Thank you for the question. So where platforms
fail to take responsibility, transparency can flag issues. It
can--and it can flag the extent of the issues, and how those
issues persist.
And the kinds of issues we have talked about today a little
bit, we have talked about misinformation, we have talked about
discriminatory advertising. We have talked about targeting ads
to children, whether that--and targeting ads to minors. And
there is a CR article last year about targeting gambling and
alcohol ads toward minors. This is the kind of thing that,
without transparency, we don't have a good sense of the scale
of the problem, and how best to combat it.
When we--with Representative Trahan's bill, we suddenly
have the ability to make clear where the problems are coming
from, and how they can best be stemmed.
Mr. McNerney. I forgot to say that I am going to switch to
DuckDuckGo tonight. Thank you.
Ms. Lehman, in your written testimony it states that
algorithms can be opaque, and even for the engineers that
designed them. I have designed algorithms. I know what you are
talking about. How can the Algorithm Accountability Act help
engineers be more thoughtful about their designs and testing
processes?
Ms. Lehman. Thank you. So with the Algorithmic
Accountability Act, it allows for--it makes possible impact
assessments for--and so it forces, again, the consideration
that platforms and companies utilizing algorithms don't
presently need to do.
And so, because there are so many different factors that
can go into our recommendations or algorithmic decisionmaking,
it forces considerations, and not changes, and not--but it--
that--it is kind of a form of transparency unto itself, but
it--forcing that forward is a crucial component of
understanding what factors go into things that can affect
everything from credit to housing to what ads we are seeing
online.
Mr. McNerney. Well, in other words, pre-planning and pre-
specifications can help make algorithms more accountable and
transparent----
Ms. Lehman. Yes----
Mr. McNerney I believe that.
Ms. Lehman. Yes.
Mr. McNerney. Ms. Nkonde, in your testimony you explained
how predictions made through machine learning create a feedback
loop that can change the course of society. Please elaborate on
that. That is an interesting comment.
Ms. Nkonde. So once a prediction has been made--for
example, a wrongful arrest in the case of Robert Williams, as
we talked about earlier, that person now has a false arrest
record. But more importantly, their biometric data is within
that system, and there is no way of taking back that bad
decisionmaking. So that person's life has been changed, not
just personally, but administratively. And they become part of
a data set that is feeding back wrong information.
Mr. McNerney. Thank you. I ran out of time, and I yield
back.
Ms. Schakowsky. The gentleman yields back, and--yes, Ms.--
next is Mr. Bucshon.
Mr. Bucshon. Thank you, Madam Chair, for calling this
hearing today. This is a really, really important subject.
In the last few years, multiple reports and incidents have
come out showing that America's youth has been harmed by the
often opaque and insufficient data privacy and content
moderation practices of tech platforms and other online
services. I think we can all agree that that is true. It is
extremely concerning.
I have four children. My youngest now, my daughter, is
getting ready to graduate high school. And during her high
school years we have had to navigate this. And in our family,
just like everyone else's, like so many other people from
Indiana, Hoosiers, they have had to grapple with what the fair
and safe terms of our online presence is with this in mind,
because, if anyone has teenagers, they know they will be
online, whether you know it or not. It is critical that kids
like my daughter learn to safely navigate these technologies,
prepare them for the 21st century life and workplace.
I was a surgeon before, and I would like to--I always like
to say these issues need to be addressed, I think, with a
scalpel, rather than a hacksaw, in that there are data and
usage restrictions worthy of examination by this committee,
absolutely. Unfortunately, I think the legislation today, which
is extremely well intended, probably takes more of a hacksaw
approach than I would like to see, outright banning targeted
advertising and opening a private right of action to let trial
lawyers sue small business and local advertisers. I doesn't, I
don't think, address the issue. Rather, we should examine
policies how the FTC can better do their job as the cop on the
beat to protect children from privacy dangers online.
We are also reviewing a bill today that would drastically
expand the government to increase the FTC by nearly one third
of its existing size.
So, Mr. Duffey, in your testimony you mentioned some of the
ways that predators and other bad actors try to pressure and
bully kids into providing information or materials that could
harm them for the rest of their lives by putting it in
cyberspace, and that never goes away.
Do you believe that requiring platforms to be more
transparent with their cyber bullying content moderation
practices would empower youth users and their parents to avoid,
oppose, or remove cyber-bullying content that can quickly get
out of hand and turn into exploitative or even illegal content?
Mr. Duffey. I do, thank you, Representative. I agree with
you 100 percent that some transparency with youth, law
enforcement, and parents would only add to the knowledge base
that--for everybody on how to better protect yourself, the
content, and what you are getting, what you are contributing
when you sign up for these sites.
And it would allow for us to have a wider scope to the
overall issue when it comes to sextortion and cyber-bullying,
along with the tactics used by these individuals.
Mr. Bucshon. Well, thank you very much. And even--I live in
Evansville, Indiana. And I can tell you, talking with local law
enforcement there, this is pervasive. It is surprising. Even in
rural counties I represent, bad actors every day doing this.
This is just not, you know, in cities, big cities and other
places. This is everywhere.
Ms. McInnis, many consumers have already shown to be
interested in not having their data tracked, as evidenced by
the success of DuckDuckGo browser. But there are, obviously,
those who have chosen to stay on the platforms that use their
data. Do you think that encouraging wider implementation of
privacy-by-design principles would give consumers the
confidence that their data is being responsibly used, and could
encourage more transparent practices in how a company uses a
consumer's data to alleviate the concerns and abuses you have
laid out by some of the advertisers currently?
Ms. McInnis. Thank you, Representative. Absolutely.
However, as the Chair and ranking member have stated, we
have waited too long for companies to self-regulate, and we
really should act now to incentivize them to act in the best
interests of their users.
Mr. Bucshon. Yes, I mean, so what you are saying is
implementation--wider implementation of privacy-by-design
principles should be driven by change in Federal law.
Ms. McInnis. Unfortunately----
Mr. Bucshon. Not voluntary.
Ms. McInnis. Unfortunately, sir, I just think that
companies won't be incentivized to do so before we incentivize
them with law.
In addition, I think competition reform bills would level
the playing field, allowing more companies like DuckDuckGo to
compete on privacy.
Mr. Bucshon. That is a good advertisement.
I yield back.
Ms. Schakowsky. The gentleman yields back.
Relevant to this subcommittee, the President is apparently
going to talk about social media and children's mental health
tonight, which is very important to us, yes.
And next we have my friend, Congresswoman Clarke, for 5
minutes.
Ms. Clarke. I thank you very much, Madam Chair and Ranking
Member Bilirakis, for holding this extremely important hearing
today. And I would like to thank you for including my
legislation, H.R. 6580, the Algorithmic Accountability Act of
2022, which I recently reintroduced, along with Senators Ron
Wyden and Senator Cory Booker.
My legislation takes common-sense and long-overdue measures
to protect consumers from harmful bias and discrimination
resulting from the widespread use of automated systems that are
governed by computer algorithms, artificial intelligence, and
machine learning.
Let me be clear: the problem isn't just Facebook,
Instagram, and TikTok. The problem is pervasive. The very same
automated technologies that are being misused and abused by
social media companies are similarly being used without
oversight by a wide range of industries to make critical,
split-second decisions about people's health care, housing,
finances, employment, and so much more. And while these
technologies reach conclusions based on calculations, these
calculations are the products of systems designed by humans,
subjecting them to a wide range of flaws that reinforce broader
societal discrimination, particularly against women and people
of color.
So Ms. Nkonde and it is so good to see you once again--
could you share an example or two of algorithmic systems
leading to discriminatory outcomes that have a major impact on
people's lives?
Ms. Nkonde. Yes, of course, Congresswoman Clarke. One of
the most recent examples is a pain management algorithm that is
used in over 1,000 health care settings in the United States
that was discriminating against Black American patients because
their health care costs were higher, in aggregate, than other
groups. And it wasn't because of care. I can certainly speak
about that more, but I know we don't have time.
And then the second example was one that was brought forth
by the Haas Business School at Berkeley that found that
mortgage recommendation algorithms were discriminating against
Black borrowers at the same rates as human beings
[were], because the inputs that were being used, the
questions that were being asked were asked discriminatory as it
were a human banker (sic).
So those are two of a myriad of examples I could offer.
Ms. Clarke. So thank you. It is clear to me that something
must be done to address the current lack of accountability and
transparency around how automated systems are being used. And
that is why my Algorithmic Accountability Act directs companies
to assess their automated decision systems for potentially
dangerous flaws such as inherent bias, safety risk, and
performance gaps.
Ms. Nkonde, would you agree that impact assessments are a
feasible and important first step in tackling this issue?
Ms. Nkonde. Yes. As it has been said earlier in this
hearing, we are looking at impact assessments around
algorithmic thinkers in the EU, and understanding the
downstream impact of these technologies will be--will enable
good faith actors on the public side to be able to decide
whether this is actually in line with existing law. Because the
harms I am describing are otherwise outlawed underneath civil
rights statute, which we should be following if we are going to
be--if we are going to follow rule of law in this country.
Ms. Clarke. And Ms. Lehman, do you agree, as well?
And how would these impacts assessments benefit consumers?
Ms. Lehman. Thank you, Congresswoman. A little bit, as we
mentioned earlier, having companies have to consider things
other than their profits, having them have to consider how the
systems they build have impacts on communities, on marginalized
communities, on consumers across the board, inherently, even if
it may provide more internal accountability and provide more
external accountability as well, I mean, it forces those
considerations in a way that we haven't previously seen.
Ms. Clarke. Well, thank you. And my legislation would
further direct companies to report their findings to the FTC
for review, and require the FTC to establish a public
repository of automated decision systems along with high-level
information such as data sources and how to contest decisions.
Ms. Nkonde, how would these transparency measures be
helpful to consumers and researchers alike?
Ms. Nkonde. They would protect our rights and enable us to
make informed decisions around what we are actually buying, and
the impact it will have on our lives as--on a whole.
Ms. Clarke. Well, let me thank you for your testimony here
today to all of our witnesses. It is time that we make a
change.
And with that, Madam Chair, I yield back. I have gone over
time.
Ms. Schakowsky. The gentlelady yields back, and now, Mr.
Dunn, you are recognized for 5 minutes.
Mr. Dunn. Thank you very much, Madam Chair. I appreciate
the opportunity to discuss Big Tech today.
You know, thanks to advancements in technology, it is
easier than ever to stay connected online. When the world went
into a lockdown with COVID, our relationships could nominally
be continued. And of course, essential businesses remained
open. So social media, with American Big Tech leading the way,
certainly was a boon to us, and we should absolutely continue
to uphold an economic system that allows for innovation and
open competition in the U.S. technological industry, especially
for small and medium-sized enterprises that are trying to break
into the market.
Unfortunately, social media platforms have also provided a
new space for bad actors, foreign adversaries to exploit the
social media channels, to target potential victims and, of
course, spread harmful propaganda. And these threats are
heightened during the crisis like we are seeing now in Europe.
Some of the Twitter accounts that were sharing information
about the Russian unprovoked attack in Ukraine were curiously
suspended from Twitter during the beginning days of the
invasion. Twitter has noted that these accounts were moved
removed by mistake, not due to Russian interference. And while
this may not be an example of pure Russian aggression, we know
similar reports will likely increase, and TikTok is already
reporting a surge in Russian propaganda.
The dangers of foreign adversaries using social media to
advance their agenda is clearly a very real danger. Social
media companies have a social responsibility to not allow
malign State influences on their websites, and I think our
national security depends on that.
So first question, Ms. McInnis, we know China uses search
engines to push conspiracy theories, and we know Russia is
currently using propaganda online to spread misleading
information in Ukraine. How does DuckDuckGo detect foreign
manipulation campaigns such as this?
And after detection, what steps do you take?
Ms. McInnis. Thank you for the question, Congressman.
First and foremost, the number-one thing that DuckDuckGo
does to help stem disinformation and misinformation online is
not collect your personal information. Companies that collect a
lot of personal information, such as Google, are able to use
that to further ensure that you are engaging in the product,
which leads often to the presentation of conspiracy theories or
other fringe videos that are presenting non-credible
information.
Second, DuckDuckGo got started as a search engine that was
providing users with instant answers at its top-of-the-search-
engine results page. We have continued to do that, especially
with regards to information that may be targeted by people with
ill intentions to provide misinformation and disinformation.
And what many may not know is that the top of the search
engine results page gets, by far, the most attention from any
user. So by presenting users with credible and authoritative
information at the top, we are ensuring that they are going to
find the right answers to their questions, and not be steered
away to another site that is looking to prey on their fear and
misinformation.
Mr. Dunn. Thank you for that, Ms. McInnis. Again, our
adversaries also want access to individuals' data, so they can
build algorithms to target, predict, and manipulate behavior in
the United States.
I am especially concerned about American companies that
have ties to the Communist Chinese Party, and what data they
are forced to share because of that relationship with China.
You know, I think consumers deserve more transparency.
How is their data used? Can you tell us a little bit more
about that data? When are they forced to share?
Ms. McInnis. Thank you for the question, Congressman. I am
not sure what data companies are forced to share, in part
because DuckDuckGo does not collect any personal user data as
people use our systems. So by----
Mr. Dunn. Well, thank you for not doing that. I
appreciate--time is drawing short, so I am going to ask you for
yes-or-no answers to these, if you will.
Do believe that the amount of individual data collected by
Big Tech companies is concerning?
Ms. McInnis. Very.
Mr. Dunn. Good. In general, is the data collected by Big
Tech companies used to influence individual behaviors?
Ms. McInnis. Absolutely.
Mr. Dunn. OK, good.
Mr. Duffey, we are going to give your technology another
shot here. From your experience, what tools could social media
sites implement to help users identify nefarious accounts?
[Pause.]
Mr. Dunn. Shouldn't have taken the risk, I guess.
Mr. Duffey. Thank you----
Mr. Dunn. [Inaudible.]
Mr. Duffey. I think the internal tools that they collect--
--
Mr. Dunn. Mr. Duffey, go ahead. What tools would you wish
you had from Congress?
[Pause.]
Mr. Dunn. A good computer----
[Laughter.]
Mr. Duffey. I would wish that the technology companies----
Mr. Dunn. What?
[Pause.]
Mr. Dunn. I appreciate the Chair's----
Mr. Duffey. Can you hear me?
Mr. Dunn  [continuing]. Forbearance. I yield back.
Ms. Schakowsky. Yes, I think we, unfortunately, lost him.
Next is Mr. Cardenas for 5 minutes.
Mr. Cardenas. Yes, thank you, Madam Chair Schakowsky, and
also Ranking Member Bilirakis, for having this important
meeting, and talking about these very good bills.
I would also like to thank my colleague, Representative
Lori Trahan, for introducing her bill, which would help shine a
light on the content moderation practices of Big Tech
companies.
Without adequate transparency, we don't have the
information we need to understand how disinformation spreads,
and how to hold these companies accountable when they fail to
make strong-enough measures to combat it.
This problem is particularly bad in Spanish-speaking
communities, where we have seen social media companies fail to
invest the resources necessary to fight Spanish-Language
disinformation. And it has come to my attention that Russia is
feeding disinformation about what is going on in the Ukraine,
specifically bombarding the Spanish-speaking community here in
the United States and around the world. So obviously, they are
trying to skew the truth, and trying to get certain communities
to believe that Putin is muy bueno, or a good guy, and he is
not.
Thanks to whistleblowers like Frances Haugen, a former
Facebook product manager, we know the Facebook--that Facebook
directed 87 percent of their investments on combating
misinformation to English language content, in spite of the
fact that only 9 percent of Facebook users are English
speakers. What a disparity.
These disparities are unacceptable, especially when
consuming misinformation can lead to real-world health, safety,
and financial consequences for communities. And also, in some
cases, people are actually dying based on what people are fed
and the actions that they take.
Ms. Lehman, would publicly publishing how these online
platforms moderate non-English language content hopefully put
some pressure on companies to correct the disparities and
content moderation investment between English content and
content in other languages? And if so, how so?
Ms. Lehman. Absolutely. And I think, in large part, because
those disparities, when they are said aloud, are so dramatic
and so disproportionate, and so just incredibly disheartening,
right?
As the statistic you just cited, if it is 87 percent, even
though it is making up 9 percent--even though English speakers
make up only 9 percent of that platform, all sorts--all
languages deserve the level of attention and the intensity and
the appropriate content moderation, because all consumers
deserve high standards of conduct moderation, and that when
they are using a platform online, they can expect that their
experience won't be different from someone else's simply
because of the language that they are using on that platform.
Mr. Cardenas. Thank you. We had Facebook in front of us,
and other companies, and they admitted that they could put some
more resources in there, but they choose not to.
For example, McInnis, wasn't it earlier today that somebody
asked about whether or not DuckDuckGo is actually profitable,
and you said yes? What number do you--did you say that your
revenue is $100 million?
Ms. McInnis. That is correct, Representative.
Mr. Cardenas. Is that per year?
Ms. McInnis. It is per year.
Mr. Cardenas. An annual basis? A year. Well, I would
venture to guess that $100 million is a slow day for Facebook/
Meta, Google, and Amazon.
So I pray that DuckDuckGo can actually continue to succeed
in this environment, because we are talking about companies
that are now net worth $1 trillion or more. And the reason why
I point that out is because they choose not to use good
practices. They choose not to have practices that are
respective of the consumer, respectful of the people who are
using their platforms.
And so I just want to commend you, Ms. McInnis, and please
tell the C-suite executives at DuckDuckGo that I hope and pray
that your model continues to work in this environment. And
unfortunately, you are a tiny player on this playing field.
Please take a second if you think that I am mischaracterizing,
what I just said.
Ms. McInnis. No, I appreciate your words, Representative,
and we are trying very hard to compete in this marketplace. But
as we have mentioned, Facebook and Google hold a duopoly in
advertising. And the Banning Surveillance Advertising Act would
help mediate that inadequacy in the market, and allow for more
companies to compete on privacy, just like DuckDuckGo.
Mr. Cardenas. Thank you so much. In 2020 a product risk
assessment calculated internally by Facebook found that Spanish
language misinformation detection on the platform remains,
``very low performance.''
In spite of that, the report's recommendation was to--and I
quote--``just keep trying to improve,'' and aim--and claimed
that--and I quote--``addition of resources will not help.''
That--nothing could be further from the truth.
And the last thing I will say is, without public
accountability, I think that these companies will continue to
do the wrong thing, and we do need to have Federal legislation
to rein them in for the sake of people's health and livelihood,
and for the lives of the people and the children who are badly
affected by these platforms.
I am sorry I went over my time, Madam Chair, I yield back.
Ms. Schakowsky. No, I thank the gentleman.
And Mrs. Lesko, you are recognized for 5 minutes.
Mrs. Lesko. Thank you, Madam Chairman. Before I begin my
question, I want to correct the record.
Earlier in the hearing Representative Castor had urged the
need for the House to have a bipartisan bill updating the
children's online privacy protection rule, and I agree we need
a bipartisan bill. But such a bill already exists.
Representative Walberg and Representative Rush introduced H.R.
1781, the PROTECT Kids Act, earlier this year. And I understand
Republicans have requested this bill to be included in the last
two legislative hearings, but still to no avail.
Furthermore, the bill was reintroduced from last year, so I
am happy to say to Representative Castor that the Senate is not
ahead of us on this particular issue.
I also want to add that I understand that Leader McMorris
Rodgers and Ranking Member Wicker have now asked the President
twice on engaging on a comprehensive national privacy and data
security bill--not just on children's privacy--and have not
heard back yet, or there hasn't been an attempt to build a
consensus with us Republicans.
So I think we do want to work in a bipartisan area, because
this is very important.
Well, again, I want to say thank you to the witnesses for
being here today.
Before I begin my questions, I want to express my
disappointment that representatives from the advertising
industry were not invited to testify at a hearing which
examines legislation that will severely impact the advertising
industry. It seems that they deserve a seat at the table, so we
can understand the effects.
I do want to be clear, however, I have the same concerns
that my colleagues have, in terms of Big Tech abusing their
power and escaping responsibilities for their wrongdoing.
However, H.R. 6416 will burden the small and medium-sized
enterprises that are looking to gain entry into a market that
heavily relies on advertising to be successful. The niche
products created by innovators looking to capitalize on the
American dream will not be able to grow or even survive under
this legislation.
I do not believe that was the intent of the sponsor, my
friend. I know you care about protecting Americans' privacies,
but let's not do it at the expense of small businesses. Yes,
abusers like Google must be held accountable, but we must be
sure to examine that actions taken to curb Big Tech's power
will not unduly burden small businesses.
The FTC has shown no interest in protecting good actors in
the market, so that duty now falls on us. But these are the
very same concerns that the Interactive Advertising Bureau, an
organization that should be on today's panel, explained in a
letter to this committee. And I ask that this letter--it is
rather a lengthy one--be submitted for the record.
And if Mr. Duffey can hear us----
Ms. Schakowsky. Without objection, so ordered.
[The information appears at the conclusion of the hearing.]
Mrs. Lesko. Thank you, Madam Chair.
Mr. Duffey, thank you again for your testimony, and for
your long career wearing the badge. You raised many issues that
we should be examining as we hold Big Tech accountable.
Central to our goal is the Federal Trade Commission and
their ability to inform Americans about resources they have
when their security has been violated. As a law enforcement
expert, what information would you want people to know about
when their safety is violated on social media platforms?
Mr. Duffey. Thank you, Representative. Hopefully, the audio
is coming through a little bit clearer.
To answer your question, I think today you would want to
know, when your safety is violated, what exactly is the data
that is--was available to that individual. Was it personal
information, as it relates to your phone number? Was it date of
birth information?
What--a lot of times, when we see these social media sites,
there is a growing concern amongst individuals that they are
being encouraged to link their profiles to other social media
sites. The question then becomes--is what data carries from
company to company, and how does that impact the citizens that
we serve every day?
So I think, while a lot of it is we are used to credit
reports being offered to people once they are--once they have
been compromised, from a social media perspective, were my
pictures taken? The big picture of what exactly was
compromised, so that I can make a self-assessment as a citizen
to what safety factors I am more concerned of.
Mrs. Lesko. Thank you.
And Madam Chair, I yield back.
Ms. Schakowsky. The gentlewoman yields back. And now,
Congresswoman Debbie Dingell, it is yours for 5 minutes.
Mrs. Dingell. Thank you, Chairman Schakowsky. Thanks for
holding this important hearing, and to all the witnesses for
testifying today.
In our December hearing on holding Big Tech accountable, I
discussed how these platforms prioritized profits over people
by keeping users engaged. These direct design choices have a
profound impact on children and adolescents who are using these
platforms, and we have seen an increasing connection between
time spent on new media and mental health issues we are seeing
in children and adolescents. In this increasingly digital age,
we need to be vigilant in re-evaluating how these design
choices impact children, and we need to work to prevent
subsequent harms online.
I am going to--I know previous questioners--my colleagues
have asked some of these questions, and I am going to try to
build on them. I want to start just by getting each of you on
record with the same question I asked at the subcommittee's
previous tech accountability hearing.
To the panel, are social media companies conducting
business online actively making the choice to prioritize
profits and engagement over combating disinformation, violent
content, and negative health outcomes for individuals and
children?
Just a yes or no answer, Ms. Lehman.
Ms. Lehman. Yes.
Mrs. Dingell. Ms. McInnis?
Ms. McInnis. Yes.
Mrs. Dingell. Ms. Nkonde?
Ms. Nkonde. Yes.
Mrs. Dingell. Mr. Duffey?
Mr. Duffey. Yes.
Mrs. Dingell. Thank you. OK.
Ms. Lehman, are there ways in which the design features or
algorithms of these platforms can take advantage of young
people in particular, leading to worse outcomes?
Do these design choices increase the likelihood that
children will be exposed to divisive, violent, hurtful, or
inappropriate content?
Ms. Lehman. Absolutely. So right now we--a lot of what is
most exciting about the transparency of things before us today
is that there is not enough information to have a good sense of
how exactly some of these platforms are making these design
choices. We understand they are, but we don't know the
intricacies of exactly how to fix it.
A lot of this legislation allows us that insight, and also
starts to force platforms to consider ways that their
algorithmic systems are designed, ways that they are--it
starts, as you said, when they are prioritizing profits over
people. It starts to combat that, it provides--because,
ultimately, they do need incentives to do so.
Mrs. Dingell. Thank you.
Ms. McInnis, what responsibility do tech companies have to
protect children from manipulative marketing content and
product recommendations and targeted advertising?
Should these companies have some level of responsibility
for a failure to protect our young users?
Ms. McInnis. Absolutely. And thank you for the question,
Representative.
We encourage tech companies to take more responsibility
over the kinds of tracking that they are using online, and how
that tracking encourages other third parties to target users
with misinformation, disinformation, and other offers that may
be inappropriate, especially for children.
Mrs. Dingell. Thank you.
Ms. Lehman and Ms. Nkonde, do you believe that, without
legislation, companies will take the necessary steps to protect
children from manipulative practices on their platforms?
How would the legislation under consideration today ensure
that regulators and researchers have the tools and information
necessary to protect children and adolescents using these
platforms?
Why don't we start with Ms. Nkonde?
Ms. Nkonde. I don't think that any of the companies are
incentivized to protect children because they are publicly
traded companies. Therefore, their first priority is to their
shareholders. This legislation would create that hand in the
same way that, in the age of robber barons, Congress had to
come in and create an incentive.
In terms of transparency, knowing the harm allows us to
have a targeted and appropriate remedy that can maintain
innovation and business in the United States, while protecting
our children, who, in my humble opinion, as a mother, are our
greatest asset.
Mrs. Dingell. Ms. Lehman, 20 seconds. Can you give us a
quick answer?
Ms. Lehman. Yes, I would echo Ms. Nkonde.
And we know from previous hearings that platforms don't
action, or don't action--maybe even three to 5 percent of hate,
of violence, and of incitement, these problems are rampant, and
I think if--certainly, the transparency would lend itself to
public accountability for that.
Mrs. Dingell. Thank you, Madam Chair. I yield back.
Ms. Schakowsky. The gentlewoman yields back.
And Mr. Pence, you are recognized for 5 minutes.
Mr. Pence. Thank you, Chair Schakowsky and Ranking Member
Bilirakis, and thanks--I thank the witnesses for being here
today. I am going to kind of give you a background a little bit
where I am at on this with some statements.
Like many of my colleagues, I am increasingly concerned
with the growth-at-any-cost mindset of Silicon Valley. Social
media platforms employ algorithms that promote inflammatory,
addictive content to elicit the strongest level of user
engagement. You know, as we have talked about with children,
that is a horrible thing.
More clicks lead to more data that fuels a more lucrative
online advertising business model. Efforts to keep users glued
to their screen is at the heart of this business model. You
know, we know that is true, because everybody has got their
nose in their phone all the time.
Our public disclosures have become overtly divisive. Our
younger generations have developed unhealthy addictions to
social media, and an individual's right to privacy is
disregarded.
The online environment of intrusive data collection
inflammatory content is not the only option. I am encouraged to
hear the conversation today and the level of agreement we have
on problems facing Hoosiers and all Americans as it pertains to
Big Tech. It is time that we start enacting in advance holistic
solutions that will rein in Big Tech and produce real results
to protect our constituents online.
Our constituents should have more control over how their
personal information is collected and used online, or--and this
is where I deviate a little bit--receive fair compensation when
their information is collected and sold, as it is happening
absolutely non-stop.
Ms. McInnis, in a previous hearing I discussed whether or
not a social media platform could still be profitable without
the ability to monetize content that has been adjudicated to be
harmful to users. Likewise, DuckDuckGo is a profitable company
that claims to not engage in surveillance advertising. Concerns
have been raised that, if Congress limits a platform's ability
to engage in automated surveillance advertising tools, small
businesses will suffer from a lack of reach, which--I wrestle
with that.
However, as you referenced in your testimony, research
shows only modest gains for the user of behaviorally targeted
ads. I hear that a lot from the small businesses that use
social media. Some it is good for, but the vast majority, it
says it gets nothing for me.
And yet, oddly, advertisers are very willing to pay a
premium for what they believe extends their reach. Is it reach,
or extends their data collection?
If the outcome for online advertising is similar to what
you can expect from other methods, the only difference becomes
the amount of information Google or Facebook or any of the
other Big Tech can gather from you and your business, or you
personally. What has the experience been like at DuckDuckGo for
small businesses seeking to advertise their products to
potential consumers, from your perspective?
Ms. McInnis. Thank you for the question, Representative.
Unfortunately, we syndicate our contextual ads through
Microsoft. So Microsoft would have the best data on how small
businesses have utilized the platform. That said, DuckDuckGo is
a smaller tech company that is competing against the giants. We
also use contextual ads, and we have used the contextual ads on
our services, as well. So we have proven that this is a
profitable model for us.
In addition, many small businesses have been using
contextual ads to their benefit.
Finally, as I mentioned earlier, the ability to
discriminate in the targeting of advertisements also enables
companies to discriminate in the opportunities that they
present to users, meaning that some smaller businesses may be
discriminated against in the provision of financial resources,
rental opportunities, housing agreements, and other things that
make their small business possible.
Mr. Pence. Well, I would even say--I appreciate that
comment. I would even say in sales, too, they get limited. It
depends on who is collecting the data, and who they are
dispersing that data to. It is a real problem for me.
Ms. Nkonde, what is your answer to that same question? And
that question was your experience with small businesses, the
value there. Are they making money off this data collection of
Big Tech?
Ms. Nkonde. I would actually agree with your assessment
that the returns are actually on the tech company side, in
terms of more data. Because with more data, you can create more
products, you have more insight. And the rate of return can be
negligible.
But that is not to say all the time. There are some
businesses that are able to benefit, but not in the same way
that the big giants do.
Mr. Pence. Yes, thank you for that. You know, I think it is
time we stop talking about being--us receiving some benefit for
all the data that is being collected on us.
And thank you, Madam Chair, I yield back.
Ms. Schakowsky. The gentleman yields back. And now my
colleague from Illinois, Congresswoman Kelly, for 5 minutes.
Ms. Kelly. Thank you, Chair Schakowsky, for holding this
hearing today, and for the witnesses appearing before us.
I am delighted to see that one of the bills recommended by
the E&C Racial Equity Working Group, the Algorithmic
Accountability Act, will be considered here today.
Every day people go online and search or buy products, and
all the while, little bits of their activity get collected and
put toward creating an online profile of a user's wants, likes,
and dislikes. In the real world, more and more decisions are
being made by artificial intelligence from data collected from
what we do online. Large amounts of data are combined to make
predictions with little knowledge of the underlying data used
to train the algorithm, or the potential bias that is encoded
in the system. One of the more difficult conversations to have
around AI--what it means for AI to be transparent or
explainable.
Ms. Nkonde, in an interview last year with Stanford's
Engineering's The Future of Everything podcast, you mentioned
that impact assessment information would be more useful than
opening up an algorithm and seeing what is inside. Can you
elaborate on this?
Is it either or--is it an either/or situation, or how would
the Algorithmic Accountability Act--how would it help biases in
how algorithms are designed and the outcomes they produce?
Ms. Nkonde. Congresswoman Kelly, I would suggest that,
because we are looking for accountability, the impact of the
technology is way more important than the longitudinal algebra
that it took to create the algorithm, because those statistical
models are going to be visible to very few people in the
population.
And from an oversight perspective, Congress just needs to
know, are these products in line of the laws of our land? And
for that you just need impact.
Ms. Kelly. Thank you.
Ms. Lehman, in your testimony you point out the imbalance
that the FTC often faces going up against some of the largest
tech companies in the world. The Digital Services Oversight and
Safety Act and the Algorithmic Accountability Act include
increased FTC staffs and funding, albeit at different levels.
Can you explain how the current imbalance of FTC staffing
harms the ability of the government to stop bad actors?
Ms. Lehman. Absolutely. Thank you, Congresswoman.
So the FTC today is a fraction of the size it was 40 years
ago, even while the economies from some of the largest
platforms alone were unthinkable at that time. And so the kinds
of resources that the FTC has to combat the systemic problems
throughout these--throughout this industry is--the fact that we
are still smaller than--compared to the outsized growth there,
it--we can only--consumers will only benefit from a fully
empowered FTC that can--that has the level of expertise and
funding to effectively go after these industries.
Ms. Kelly. Thank you.
And Ms. McInnis, do you have anything to add?
Ms. McInnis. We likewise support a fully funded and
resourced Federal Trade Commission.
One of the best things that the Federal Trade Commission
and, indeed, the U.S. Government could do right now is enforce
our existing rules and laws. And so we encourage the Federal
Trade Commission to not only enforce their laws, but also
consider where they could act to protect users, perhaps through
examining new rules under the Children's Online Privacy
Protective (sic) Act, or acting against patterns that we are
seeing online under their section 5 authority.
Ms. Kelly. Thank you. The European Union has already
started their work toward an AI law. Their bills differ in many
ways from the bills being considered today, but share some
important similarities.
Ms. Lehman, how would the Algorithmic Accountability Act
align with European efforts to regulate AI, and how do they
differ on taking risk-based approaches?
Ms. Lehman. Thank you, Congresswoman.
So in the EU there have been proposed legislation--there
has been proposed regulation that would put algorithms into
different risk buckets. And so this is where they differ
insofar as they would go so far as to ban algorithms in the use
of, like, the highest risk buckets, thinking more about, like,
social credit scoring. There would be more regulations around
how they can be used in some of the medium-risk buckets, and
transparency in the lowest-risk buckets.
We are a little bit earlier on in our AI regulation
discussions, and so are--in the U.S., the--we--the primary
mechanism we have right now in the Algorithmic Accountability
Act is impact assessments and that transparency. So getting a
sense of, OK, what are the effects of these systems.
Ms. Kelly. Thank you.
And with that, Madam Chair, I yield back.
Ms. Schakowsky. And next I call on Mr. Soto for his 5
minutes.
Mr. Soto, the floor is yours.
Mr. Soto. Thank you, Madam Chair. When we look at 7 in 10
Americans using social media platforms--that actually even
sounds pretty low for me; most people I know use social media,
right?
And the practice of sharing user data to target
advertisements has led to discrimination based upon race and
gender on occasion. Forty-two percent of Americans experienced
online harassment over the past year, forty-two percent. And
social media companies have consistently denied and blocked the
needed data for us to do independent research and government
research.
But there is hope, right? Because, while we know so many
Americans are using social media, privacy online is an issue
that affects most Americans. And the big headline from this
hearing today is that the Energy and Commerce Committee,
Democrats and Republicans, are coming together to propose
bipartisan agendas, bipartisan bills to protect America's
privacy online.
And Madam Chair, I thank you and the ranking member for--my
fellow Floridian, Gus Bilirakis, for your leadership on this,
the Banning Surveillance Advertising Act of 2021. It prohibited
advertisers from targeting advertisements based upon personal
information that links the consumer's connected device,
basically hunting down who you are by your device, rather than
by your preferences and what you avail yourself to online.
The second bill, the Algorithm (sic) Accountability Act of
2022, which would conduct impact assessments on the algorithms
that are mysterious to so many folks, and have regular
reporting of these results to the Federal Trade Commission to
make sure they are fair against discrimination, against anti-
consumer issues, against all sorts of nefarious things that can
happen if we leave it just to machines to make these decisions.
The third bill, Cooperation Among Police, Tech, and Users
to Resist Exploitation Act, or the CAPTURE Act. I want to give
a compliment to my fellow Floridian, the ranking member, Gus
Bilirakis, on this great bill on how social media companies
communicate, consult, and coordinate with Federal, State, and
local law enforcement to address illegal content and activity
online. We have to protect our kids, we have to protect our
families.
My wife is an assistant principal in central Florida, and
we see kids being exposed to adult content that has really hurt
their childhood, and has made it harder on educators, and
particularly on parents.
Increasing consumer education of law enforcement resources,
by Representative Mullin, that requires the FTC to work with
the attorney generals to develop educational programs to inform
the public on resources available, should they feel their
safety or security was violated. Right now, many Americans
don't know where to go, and this is a key part.
And then finally, the Digital Services Oversight and Safety
Act of 2022, which establishes the Bureau of Digital Services
Oversight and Safety, a long-time need at the FTC to beef up
their oversight of social media platforms under the FTC.
Thank you, Special Agent Duffey, for being here. Welcome
from Florida, at least virtually. We appreciate you testifying
today. What other types of criminal acts and dangers do you see
online from Floridians, from your role at FDLE?
Mr. Duffey. Thank you, Representative.
[Pause.]
Mr. Duffey. Can you hear me?
Mr. Soto. Yes, we could hear you. So what are the types of
criminal acts online that you end up seeing in Florida, and the
dangers?
Mr. Duffey. We see everything that you could think of, and
more. There is not a day that goes by that we aren't learning
something new. Everything from the drug trade to individuals
selling other--personal information from other people's
accounts. It has really become the modern day means of
communication amongst all criminal activity. They establish
groups using different platforms, leveraging encryption
services to hide behind a curtain that we are not privy to.
So it is increasingly becoming a challenge, as you
mentioned, with today's youth, and the content that they get
exposed to on a daily basis, the mental health impact that it
has on them, and the lack of oversight in the amount of content
that they view is growing each and every day.
So if you can think of a criminal activity, from selling
somebody's house that they live in, to compromising a bank
account, to title fraud, car fraud, it is all occurring in the
digital environment.
Mr. Soto. Thank you, Special Agent Duffey. Together we can
empower parents, families, educators, consumers, prevent
discrimination with this critical agenda.
And I yield back.
Ms. Schakowsky. The gentleman yields back. And now,
Congresswoman Craig, you are recognized for 5 minutes.
Ms. Craig. Thank you so much, Madam Chair, for yielding.
Ms. Lehman, I want to start by thanking you for providing
such thoughtful and helpful testimony across so many different
areas related to holding Big Tech accountable, and for calling
out Snapchat in your section 230 reform ideas.
I have raised my concerns about Snapchat serving as an
illegal marketplace for drugs in prior Big Tech hearings, and I
want to continue my focus on the issue during today's hearing,
as well.
As my colleagues may recall, I have a constituent in
Hastings, Minnesota, Bridgette Norring, who--she and her family
lost their son, Devin, to a fentanyl overdose, and is asking us
to do more. I am here today raising her voice and that of
countless other parents in hopes that we can come together and
find a solution.
Special Agent Duffy, thank you for what you do in law
enforcement every single day to help to crack down on these
illegal online sales. I strongly believe we need to give our
law enforcement agencies the tools, resources, and funding they
need to help protect our communities, both online and in the
physical world, from dangerous drug overdoses.
You may have seen recent news stories online talking about
changes Snapchat was taking to curb drug dealing on the app,
under pressure from parents like Bridgette and other parents
who called on the CEO to do more and do better. Based on your
experience, though, Agent Duffey, and in training those who use
apps to conduct investigations, I am hoping you can briefly
walk us through what the current process looks like when you
attempt to target a known drug dealer and hold them to account
on an app like Snapchat.
Mr. Duffey. Thank you, Representative.
When we begin these types of investigations with companies
like Snapchat, we are often times the ones being provided it
through a citizen that becomes concerned. And in some cases,
some companies will police themselves and identify content. But
ultimately, they are seemingly less proactive in trying to
remove some of this content, and leaving it for law enforcement
to become proactive on their own site, which is a challenge as
we struggle with retention of law enforcement officers around
the United States.
So these investigations begin with us identifying a
criminal activity. Then we begin to serve legal process, which
becomes the first hurdle in trying to identify what the company
has retained or what they haven't retained, which becomes the
first struggle.
And then, as we progress through the app, we end up at
internet service providers and cellular providers, because the
means of using these apps are done through mobile devices. And
so the mobile device itself, through the companies that provide
the service, we run again into another issue of data retention
and activity that may or may not be kept, along with these
criminal activities being used with a virtual private network,
meaning they are using a service that ultimately hides their
internet activity in which no records are often found.
So we have run into many roadblocks. I would say we have
less successes than we have incomplete cases because of those
things that I mentioned.
Ms. Craig. Special Agent Duffey, you mentioned that the
Snapchat or other platforms sometimes will proactively contact
law enforcement, but most of the time it is citizens who are
doing the trolling on these platforms and contacting you. And I
would assume that, often times, that is because a family member
has suffered some sort of catastrophic outcome, or at least
some harm.
What do you think the responsibility of the social media
platforms should be, in terms of being proactive on their
platform?
Mr. Duffey. Thank you. I think the responsibility is all in
their favor. They should be the ones content moderating. They
should be the ones communicating and learning from law
enforcement, who is learning from kids in school and friends
and family. Having that open conversation will only benefit
them.
Right now, we see many of the companies ignoring or not
having that communication with the public and law enforcement.
Things--they think that they can, in a sense, take it upon
themselves to learn it, or wait to become reactive.
So I think, if you are going to build a platform
environment for kids to operate on, then you need to be
responsible for taking action and learning what to look for.
Ms. Craig. Thank you so much. Amen to that. And with that,
I am sorry I am out of time, but
[inaudible] topic.
I yield back.
Ms. Schakowsky. The gentlelady yields back, and now I
welcome as a waive-on to our subcommittee Mr. Walden--I am
sorry, Walberg--for 5 minutes.
Mr. Walberg. Thank you, Madam Chair, for allowing me to
waive on to this hearing.
This is the third legislative hearing the committee has
held on holding Big Tech accountable. But once again, I believe
it fails to address one of the most pressing issues: it is
personal, of course, but the need for a comprehensive national
privacy and data security framework. Though many of the
proposals today are admirable, they include components that
should be considered in a comprehensive framework, not as a
piecemeal set of bills. Members of the committee need to get
back to our bipartisan work to create this framework. Privacy,
and children's privacy in particular, should be a no-brainer.
Despite what the gentlelady from Florida said earlier,
there already is a bipartisan COPPA bill in the House. I and my
good friend, Congressman Rush, introduced the PROTECT Kids Act,
which would update and modernize COPPA for the online behavior
and devices of today. The legislation was introduced last
Congress, and I would be glad to work with her on this issue.
This is another reason why I am disappointed that the
majority denied Republicans' second request to include this
legislation in today's hearing. I am committed to finding a
bipartisan agreement on COPPA as a part of a larger privacy
package, and I hope that the majority will work with us to find
common ground on our proposals.
Ms. McInnis, my PROTECT Kids Act adds precise geolocation
and biometric information as two new categories of personal
information which are protected for children under COPPA. I
believe that behavioral ads can be beneficial for adults, but
many of those benefits do not translate when it comes to kids.
And so how can companies like TikTok design their platforms to
better protect this type of information for children, without
the negative effects that a blanket ban on ad targeting would
have for small business, small and local businesses?
Ms. McInnis. Thank you for the question, Representative.
We encourage companies like TikTok to stem the amount of
data that they are collecting from all users, especially
children.
DuckDuckGo is a search engine, and so I can't speak to many
of the issues preventing social media companies, in particular.
But we do think that there is more that companies could be
doing proactively to protect users, and also ways they could be
adjusting their algorithm to ensure that they are not causing
the sort of mental health and other relevant harms that we have
seen Frances Haugen speak about with regards to Facebook that
we know we are being perpetrated online.
But regardless, the fact that this company can target users
and adjust the algorithm in order to kind of reach users in
this moment of mental health crisis, or kind of anything
related to that, means that they are collecting too much
information on us all, much less kids.
Mr. Walberg. Yes, yes. Thank you for that.
My legislation also raises the age for parental consent
protections for children online from 13 to 16 years of age. I
have lost their votes now, but I think it is responsible, a
responsible approach to take. I support raising the age of
COPPA.
But Mr. Duffey, in your testimony you highlight how
children and tweens frequently bypass parental consent
protections in order to socialize online. Can you elaborate on
the dangers that those between the ages of 13 and 16 face on
social media sites, and how those dangers may differ from those
under 13?
And also, what should Congress be considering to address
age restrictions online?
Mr. Duffey. Thank you, Representative. I would offer to the
committee that, when we talk about 13 to 16 years old, we--in
talking with mental health professionals, I would offer up
their thoughts and opinions and medical concerns, because the
content that you view at the age of 13 versus the content that
you are viewing at 16 is going to--what we have seen with the
kids is it is going to greatly impact potential behavior
[inaudible] forward with as they grow.
When we talk about the children today circumventing the
system, it is very much an issue, because I don't know that you
will ever stop youth from bypassing----
Mr. Walberg. And parents can be an asset.
Mr. Duffey. Yes, they very much can, but they need to be
educated, as well.
Mr. Walberg. OK, thank you. I see my time has expired.
And thank you for allowing me to waive on.
Ms. Schakowsky. Thank you, Mr. Walberg. And now I am happy
to have the opportunity to waive on to this subcommittee
Congresswoman Eshoo, who is the author and chief sponsor of the
Banning Surveillance Advertising bill that we are considering
today.
Ms. Eshoo, you are recognized.
Ms. Eshoo. Well, thank you, Madam Chair, for not only
holding this hearing, but welcoming me to waive on to the
subcommittee. I thank you for being with me on the Banning
Surveillance Advertising Act. If our colleague, Bobby Rush, is
still with us, I want to thank him, as well.
First I want to go to Ms. McInnis. Thank you for your
excellent testimony, and for your support of my legislation.
To my colleagues on both sides of the aisle, you may not
know, but my bill does go after the root of the social media
problem, which is a toxic business model. Critics say that
there can't be an internet economy without surveillance ads.
They have really poured it on, you know, that, you know, the
internet will implode without this.
But I view it another way. And I think so does DuckDuckGo,
because it is a counter-example. So I want to respond just
briefly to a few questions related to the criticism that
opponents of my bill cite very often. And if you could keep
your responses brief, I would really appreciate that.
So to Ms. McInnis, have you found that contextual
advertising to be less effective than behavioral advertising?
Ms. McInnis. We have not. We use contextual advertising
ourselves, not only to fund the business, but also to reach new
and potential users of the DuckDuckGo services.
In addition, I have spoken often about the need to invest
more in the contextual advertising model. Just because the ad
duopoly from Facebook and Google are asserting to us that
behavioral ads work better, it is not true that contextual ads
can't be just as relevant.
Ms. Eshoo. Do you hear major complaints from users that
your ads are not relevant enough?
Ms. McInnis. We syndicate our advertisements from
Microsoft's Bing.
We find that users are coming to us primarily for privacy
protection, and we offer them best-in-class privacy protection,
relevant results, and quality services, while also protecting
their privacy.
We are hopeful that, with a bill like the Banning
Surveillance Advertising Act of 2022, we will have more
competition in the contextual ads market, which will not only
enable more companies to compete against Facebook and Google's
ad duopoly, but also enable small businesses to go to more
advertisers, rather than just the two big duopoly--big
monopolies in town to source their advertisements.
Ms. Eshoo. Two more questions. Are small businesses able to
use contextual ads to reach their customers?
And the other question is will the internet break without
surveillance ads?
Maybe you should take that first.
Ms. McInnis. The internet will not break without
surveillance ads. And in fact, DuckDuckGo is proof positive
that you can have a successful and profitable company without
surveilling users.
In addition, small businesses can use contextual ads to
reach their users. And indeed, many do, because contextual ads
are cheaper, and usually provide users with the same amount of
revenue in return. We have cited some studies from researchers
like Alessandro Acquisti, pointing out that behavioral
advertising does not, in fact, result in much added revenue for
publishers. And we think that is also true for small
businesses.
Ms. Eshoo. Wonderful. Let me go to Ms. Nkonde.
Thank you for your powerful testimony, and for everything
that you are doing in this space. Can you State to members and
whomever is tuned in about the harms that ad targeting have
caused people, particularly people of color?
I think that this is--I know what the answer is, but I
would like to have you put it out on the table, so that people
have even more clarity about this.
Ms. Nkonde. So what we found in the election space is our
adversaries really take advantage of racial divides in this
country by targeting advertising online toward Black
communities, and we have heard earlier in this hearing Spanish-
speaking communities, when they want to divide and weaken us.
So what your bill actually does is add national security
protections on top of this, on top of all the other
protections, because, without that targeting--targeted
advertising, it breaks down that pathway.
Ms. Eshoo. Thank you.
And I yield back, Madam Chair, and thank you for having me
with you. It is a terrific subcommittee.
Ms. Schakowsky. The gentlelady yields back. I thank you--
thank her for her presence with us today.
I--seeing no more members who have questions, I want to
sincerely thank our witnesses for your participation today.
This was really a great hearing. All of you contributed so much
to the discussions that we need to be having, so I thank you.
And I want to remind members that, pursuant to committee
rules, they have 10 business days to submit additional
questions for the record to be answered by the witnesses who
have appeared to today.
And I certainly ask each witness to respond as promptly as
you can to any of the questions that you may receive.
With that, before we adjourn, I request unanimous consent
to enter the following documents into the record: a letter from
the Association of National Advertisers; a letter from the--
what is that?
Voice. Interactive.
Ms. Schakowsky. Interactive----
Voice. Advertising.
Ms. Schakowsky. I will start again. A letter from the
Interactive Advertising Bureau; a report from New York
University; a letter from the Software and Information Industry
Association; a letter from the FTC--a letter to the FTC; a
letter from the U.S. Chamber of Commerce.
And without objection, so ordered.
[Material submitted for inclusion in the record follows:]
Ms. Schakowsky. And I want to see if my ranking member had
anything he wanted to add.
Mr. Bilirakis. I am good. I appreciate everything you have
done today, and we hope we can----
Ms. Schakowsky. Put--yes, go ahead.
Mr. Bilirakis. No, thank you very much, Madam Chair. I
appreciate it very much, and I want to thank the witnesses, and
the panel.
And also, we want to get these bills moved forward with a
markup, and get them on the floor as soon as possible,
particularly when it comes to the social media and what it is
doing to our children. It is unacceptable. We have to hold
these companies accountable.
So thank you for bringing it to our attention today. And
again, I appreciate it very much, Madam Chair. I yield back.
Ms. Schakowsky. At this time, the subcommittee is
adjourned. Thank you.
[Whereupon, at 1:28 p.m., the subcommittee was adjourned.]
