- ARTIFICIAL INTELLIGENCE: ADVANCING INNOVATION TOWARDS THE NATIONAL INTEREST

[House Hearing, 118 Congress]
[From the U.S. Government Publishing Office]

ARTIFICIAL INTELLIGENCE: ADVANCING
INNOVATION TOWARDS THE NATIONAL INTEREST

=======================================================================

HEARING

BEFORE THE

COMMITTEE ON SCIENCE, SPACE,
AND TECHNOLOGY

OF THE

HOUSE OF REPRESENTATIVES

ONE HUNDRED EIGHTEENTH CONGRESS

FIRST SESSION

__________

JUNE 22, 2023

__________

Serial No. 118-18

__________

Printed for the use of the Committee on Science, Space, and Technology

[GRAPHIC NOT AVAILABLE IN TIFF FORMAT]

Available via the World Wide Web: http://science.house.gov

__________

U.S. GOVERNMENT PUBLISHING OFFICE
52-499PDF                  WASHINGTON : 2024

-----------------------------------------------------------------------------------

COMMITTEE ON SCIENCE, SPACE, AND TECHNOLOGY

HON. FRANK LUCAS, Oklahoma, Chairman

BILL POSEY, Florida                  ZOE LOFGREN, California, Ranking
RANDY WEBER, Texas                       Member
BRIAN BABIN, Texas                   SUZANNE BONAMICI, Oregon
JIM BAIRD, Indiana                   HALEY STEVENS, Michigan
DANIEL WEBSTER, Florida              JAMAAL BOWMAN, New York
MIKE GARCIA, California              DEBORAH ROSS, North Carolina
STEPHANIE BICE, Oklahoma             ERIC SORENSEN, Illinois
JAY OBERNOLTE, California            ANDREA SALINAS, Oregon
CHUCK FLEISCHMANN, Tennessee         VALERIE FOUSHEE, North Carolina
DARRELL ISSA, California             KEVIN MULLIN, California
RICK CRAWFORD, Arkansas              JEFF JACKSON, North Carolina
CLAUDIA TENNEY, New York             EMILIA SYKES, Ohio
RYAN ZINKE, Montana                  MAXWELL FROST, Florida
SCOTT FRANKLIN, Florida              YADIRA CARAVEO, Colorado
DALE STRONG, Alabama                 SUMMER LEE, Pennsylvania
MAX MILLER, Ohio                     JENNIFER McCLELLAN, Virginia
RICH McCORMICK, Georgia              TED LIEU, California
MIKE COLLINS, Georgia                SEAN CASTEN, Illinois,
BRANDON WILLIAMS, New York             Vice Ranking Member
TOM KEAN, New Jersey                 PAUL TONKO, New York
VACANCY
C  O  N  T  E  N  T  S

June 22, 2023

Page

Hearing Charter..................................................     2

Opening Statements

Statement by Representative Frank Lucas, Chairman, Committee on
Science, Space, and Technology, U.S. House of Representatives..    12
Written Statement............................................    13

Statement by Representative Zoe Lofgren, Ranking Member,
Committee on Science, Space, and Technology, U.S. House of
Representatives................................................    15
Written Statement............................................    16

Witnesses:

Dr. Jason Matheny, President & CEO, RAND Corporation
Oral Statement...............................................    17
Written Statement............................................    19

Dr. Shahin Farshchi, General Partner, Lux Capital
Oral Statement...............................................    24
Written Statement............................................    26

Mr. Clement Delangue, Co-founder & CEO, HuggingFace
Oral Statement...............................................    31
Written Statement............................................    33

Dr. Rumman Chowdhury, Responsible AI Fellow, Harvard
Oral Statement...............................................    38
Written Statement............................................    40

Dr. Dewey Murdick, Executive Director, Center for Security and
Emerging Technology
Oral Statement...............................................    46
Written Statement............................................    48

Discussion.......................................................    56

Appendix: Answers to Post-Hearing Questions

Dr. Jason Matheny, President & CEO, RAND Corporation.............    86

Dr. Shahin Farshchi, General Partner, Lux Capital................    97

Mr. Clement Delangue, Co-founder & CEO, HuggingFace..............   100

Dr. Dewey Murdick, Executive Director, Center....................   107

ARTIFICIAL INTELLIGENCE:
ADVANCING INNOVATION
TOWARDS THE NATIONAL INTEREST

----------

THURSDAY, JUNE 22, 2023

House of Representatives,
Committee on Science, Space, and Technology,
Washington, D.C.

The Committee met, pursuant to notice, at 10:02 a.m., in
room 2318 of the Rayburn House Office Building, Hon. Frank
Lucas [Chairman of the Committee] presiding.
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. The Committee will come to order. Without
objection, the Chair is authorized to declare recess of the
Committee at any time.
Welcome to today's hearing entitled ``Artificial
Intelligence: Advancing Innovation in the National Interest.''
I recognize myself for five minutes for an opening statement.
Good morning, and welcome to what I anticipate will be one
of the first of multiple hearings on artificial intelligence
(AI) that the Science, Space, and Technology Committee will
hold this Congress. As we've all seen, AI applications like
ChatGPT have taken the world by storm. The rapid pace of
technological progress in this field, primarily driven by
American researchers, technologists, and entrepreneurs,
presents a generational opportunity for Congress. We must
ensure the United States remains the leader in a technology
that many experts believe is as transformative as the internet
and electricity.
The purpose of this hearing is to explore an important
question, perhaps the most important question for Congress
regarding AI. How can we support innovation development in AI
so that it advances our national interest? For starters, most
of us can agree that this is in our national interest to ensure
cutting-edge AI research continues happening here in America
and is based on our democratic values. Although the United
States remains the country where the most sophisticated AI
research is happening, this gap is narrowing.
A recent study by Stanford University ranked universities
by the number of AI papers they published. The study found that
nine of the top 10 universities were based in China. Coming in
at 10th was the only U.S. institution, the Massachusetts
Institute of Technology. Chinese-published papers received
nearly the same percentage of citations as U.S. researchers'
papers, showing the gap in research quality is also
diminishing.
It is in our national interest to ensure the United States
has a robust innovation pipeline that supports fundamental
research all the way through to real-world applications. The
country that leads in commercial and military applications will
have a decisive advantage in global economic and geopolitical
competition. The frontlines of the war in Ukraine are already
demonstrating how AI is being applied to the 21st century
warfare. Autonomous drones, fake images, audio used for
propaganda, and real-time satellite imagery analysis are all a
small taste of how AI is shaping today's battlefields.
However, while it's critical that the United States support
advances in AI, these advances do not have to come at the
expense of safety, security, fairness, or transparency. In
fact, embedding our values in AI technology development is
central to our economic competitiveness and national security.
As Members of Congress, our job is never to lose sight of the
fact that our national interest ultimately lies with what is
best for the American people.
The Science Committee has and can continue to play a
pivotal role in the service on this mission. For starters, we
can continue supporting the application of AI and advance
science and new economic opportunities. AI is already being
used to solve fundamental problems in biology, chemistry, and
physics. These advances have helped us develop novel
therapeutics, design advanced semiconductors, forecast crop
yields, saving countless amounts of time and money.
The National Science Foundation's AI research Institutes,
the Department of Energy's world-class supercomputers, and the
National Institutes of Standards and Technology's (NIST's) Risk
Management Framework and precision measurement expertise are
all driving critical advances in this area. Pivotal to our
national interest is ensuring these systems are safe and
trustworthy.
The Committee understood that back in 2020 when it
introduced the bipartisan National Artificial Intelligence
Initiative Act of 2020. This legislation created a broad
national strategy to accelerate investments in responsible AI
research, development, and standards. It facilitated new
public-private partnerships to ensure the United States leads
the world in the development and use of responsible AI systems.
Our Committee will continue to build off of this work to
establish and promote technical standards for trustworthy AI.
We are also exploring ways to mitigate risks caused by AI
systems through research and development (R&D) of technical
solutions, such as using automation to detect AI-generated
media.
As AI systems proliferate across the economy, we need to
develop our workforce to meet changing skill requirements,
helping U.S. workers augment their performance with AI will be
a critical pillar in maintaining our economic competitiveness.
And while the United States currently is the global leader
in AI research, development, and technology, our adversaries
are catching up. The Chinese Communist Party (CCP) is
implementing AI industrial policy at a national scale,
investing billions through State-financed investment funds,
designating national AI champions, and providing preferential
tax treatment to grow AI startups. We cannot and should not try
to copy China's playbook, but we can maintain our leadership
role in AI, and we can ensure its development with our values
of trustworthiness, fairness, and transparency. To do so,
Congress needs to make strategic investments, build our
workforce, and establish proper safeguards without
overregulation, but we cannot do it alone. We need the academic
community, the private sector, and the open-source community to
help us figure out how to shape the future of this technology.
I look forward to hearing the recommendations of our
witnesses on how this Committee can strengthen our Nation's
leadership in artificial intelligence and make it beneficial
and safe for all American citizens.
[The prepared statement of Chairman Lucas follows:]

Good morning, and welcome to what I anticipate will be the
first of multiple hearings on artificial intelligence the
Science, Space, and Technology Committee will hold this
Congress.
As we all have seen, A.I. applications like ChatGPT have
taken the world by storm. The rapid pace of technological
progress in this field, primarily driven by American
researchers, technologists, and entrepreneurs, presents a
generational opportunity for Congress.
We must ensure the United States remains the leader in a
technology that many experts believe is as transformative as
the Internet and electricity.
The purpose of this hearing is to explore an important
question-perhaps the most important question for Congress
regarding A.I.-- how can we support innovative development in
A.I. so that it advances our national interest?
For starters, most of us can agree that it is in our
national interest to ensure cutting-edge A.I. research
continues happening here in America and is based on our
democratic values. Although the United States remains the
country where the most sophisticated A.I. research is
happening, this gap is narrowing.
A recent study by Stanford University ranked universities
by the number of A.I. papers they published. The study found
that nine of the top ten universities were based in China.
Coming in at 10th was the only U.S. institution--the
Massachusetts Institute of Technology. Chinese-published papers
received nearly the same percentage of citations as U.S.
researchers' papers, showing the gap in research quality is
also diminishing.
It is in our national interest to ensure the United States
has a robust innovation pipeline that supports fundamental
research, all the way through to real-world applications.
The country that leads in commercial and military
applications will have a decisive advantage in global economic
and geopolitical competition.
The frontlines of the war in Ukraine are already
demonstrating how A.I. is being applied to 21st-century
warfare--autonomous drones, fake images and audio used for
propaganda, and real-time satellite imagery analysis are small
tastes of how A.I. is shaping today's battlefields.
However, while it is critical the U.S. support advances in
A.I., these advances do not have to come at the expense of
safety, security, fairness, or transparency. In fact, embedding
our values in A.I.'s technological development is central to
our economic competitiveness and national security.
As Members of Congress, our job is to never lose sight of
the fact that our national interest ultimately lies with what
is best for the American people.
The Science Committee has and can continue to play a
pivotal role in the service of this mission. For starters, we
can continue supporting the application of A.I. in advancing
science and new economic opportunities.
A.I. is already being used to solve fundamental problems in
biology, chemistry, and physics. These advances have helped us
develop novel therapeutics, design advanced semiconductors, and
forecast crop yields, saving countless amounts of time and
money.
The National Science Foundation's A.I. Research Institutes,
the Department of Energy's world-class supercomputers, and the
National Institutes of Standards and Technology's Risk
Management Framework and precision measurement expertise are
all driving critical advances in this arena.
Pivotal to our national interest is ensuring these systems
are safe and trustworthy. This Committee understood that back
in 2020 when it ushered the bipartisan National Artificial
Intelligence Initiative Act of 2020 into law. This legislation
created a broad national strategy to accelerate investments in
responsible A.I. research, development, and standards.
It facilitated new public-private partnerships to ensure
the U.S. leads the world in the development and use of
responsible A.I. systems.
Our committee will continue to build off of this work to
establish and promote technical standards for trustworthy A.I.
We are also exploring ways to mitigate risks caused by A.I.
systems through research and development of technical
solutions, such as using automation to detect A.I.-generated
media.
As A.I. systems proliferate across the economy, we will
need to develop our workforce to meet changing skills
requirements. Helping U.S. workers augment their performance
with A.I. will be a crucial pillar in maintaining our economic
competitiveness.
While the United States currently is the global leader in
A.I. research, development, and technology, our adversaries are
catching up. The Chinese Communist Party is implementing A.I.
industrial policy at a national scale, investing billions
through state-financed investment funds, designating ``national
A.I. champions,'' and providing preferential tax treatment to
grow A.I. startups.
We cannot and should not try to copy China's playbook. But
we can maintain our leadership role in A.I., and we can ensure
it's developed with our values of trustworthiness, fairness,
and transparency.
To do so, Congress needs to make strategic investments,
build our workforce, and establish proper safeguards without
overregulation.
But we cannot do it alone.
We need the academic community, the private sector, and the
open-source community to help us figure out how to shape the
future of this technology.
I look forward to hearing the recommendations of our
witnesses for how this Committee can strengthen our nation's
leadership in artificial intelligence and make it beneficial
and safe for all American citizens.

Chairman Lucas. I now recognize the Ranking Member, the
gentlewoman from California, for her statement.
Ms. Lofgren. Thank you. Thank you, Chairman Lucas, for
holding today's hearing. And I'd also like to welcome a very
distinguished panel of witnesses.
Artificial intelligence opens the door to really untold
benefits for society, and I'm truly excited about its
potential. However, AI could create risks, including with
respect to misinformation and discrimination. It will create
risks to our Nation's cybersecurity in the near term, and there
may be medium and long-term risks to economic and national
security, some have even posited existential risks to the very
nature of our society.
We're here today to learn more about the benefits and risks
associated with artificial intelligence. This is a topic that
has caught the attention of many lawmakers in both chambers
across many Committees. However, none of this is new to the
Science Committee. As the Chairman has pointed out, in 2020,
Members of this Committee developed and enacted the National AI
Initiative Act to advance research, workforce development, and
standards for trusted AI. The Federal science agencies have
since taken significant steps to implement this law, including
notably NIST's work on the AI Risk Management Framework.
However, we're still in the early days of understanding how AI
systems work and how to effectively govern them, even as the
technology itself continues to rapidly advance in both
capabilities as well as applications. I do believe regulation
of AI may be necessary, but I'm also keenly aware that we must
strike a balance that allows for innovation and ensures that
the United States maintains leadership.
While the contours of a regulatory framework are still
being debated, it's clear we will need a suite of tools. Some
risks can be addressed by the laws and standards already on the
books. It's possible others may need new rules and norms. Even
as this debate continues, Congress can act now to improve trust
in AI systems and assure America's continued leadership in AI.
At a minimum, we need to be investing in the research and
workforce to help us develop the tools we need going forward.
Let me just wrap up with one concrete challenge I'd like to
address in this hearing. One is--it's the intersection of AI
and intellectual property. Whether addressing AI-based inputs
or outputs, it's my sincere hope that the content creation
community and AI platforms can advance their dialog and arrive
at a mutually agreeable solution. If not, I think we need to
have a discussion on how the Congress should address this.
Finally, research in infrastructure workforce challenges
are also top of mind. One of the major barriers to developing
an AI-capable workforce and ensuring long-term U.S. leadership
is a lack of access to computing and training data for all but
large companies and the most well-researched institutions.
There are good ideas already underway at our agencies to
address this challenge, and I'd like to hear the panel's input
on what's needed in your view. It's my hope that Congress can
quickly move beyond the factfinding stage to focus on what this
institution can realistically do to address the development and
deployment of trustworthy AI.
At this hearing, I hope we can discuss what the Science
Committee should focus on. I look forward to today's very
important discussion with stakeholders from industry, academia,
and venture capital. And as a Representative from Silicon
Valley, I know how important private capital is today to the
U.S. R&D ecosystem.
Thank you all for being with us, and I yield back.
[The prepared statement of Ms. Lofgren follows:]

Thank you, Chairman Lucas, for holding today's hearing. I
would also like to welcome our distinguished panel of
witnesses.
Artificial Intelligence opens the door to untold benefits
for society, and I'm truly excited about its potential.
However, AI could create risks including with respect to
misinformation and discrimination. It will create significant
risks to our Nation's cybersecurity in the near term. There may
be medium- and long-term risks to economic and national
security. Some have even posited even existential risks to the
very nature of our society.
We are here today to learn more about the benefits and
risks associated with artificial intelligence. This is a topic
that has caught the attention of many lawmakers in both
chambers across many committees. However, none of this is new
to the Science Committee. In 2020, Members of this Committee
developed and enacted the National AI Initiative Act to advance
research, workforce development, and standards for trustworthy
AI.
The Federal science agencies have since taken significant
steps to implement this law, including, notably, NIST's work on
the AI Risk Management Framework. However, we are still in the
early days of understanding how AI systems work and how to
effectively govern them, even as the technology itself
continues to rapidly advance in both capabilities and
applications. I do believe regulation of AI may be necessary,
but I am also keenly aware that we must strike a balance that
allows for innovation and ensures the U.S. maintains
leadership.
While the contours of a regulatory framework are still
being debated, it is clear we will need a suite of tools. Some
risks can be addressed by the laws and standards already on the
books. It's possible others may need new rules and norms.
Even as this debate continues, Congress can act now to
improve trust in AI systems and ensure America's continued
leadership in AI. At a minimum, we need to be investing in the
research and workforce to help us develop the tools we will
need going forward.
Let me just wrap up with a few concrete challenges I'd like
to address in this hearing. One is the intersection of AI and
intellectual property. Whether addressing AI-based inputs or
outputs, it is my sincere hope that the content creation
community and AI platforms can advance their dialogue and
arrive at a mutually agreeable solution.
Finally, research infrastructure and workforce challenges
are also top of mind. One of the major barriers to developing
an AI-capable workforce and ensuring long-term US leadership is
a lack of access to computing and training data for all but
large companies and the most well-resourced institutions. There
are good ideas already underway at our agencies to address this
challenge. I'd like to hear the panel's input on what's needed.
It is my hope that we in Congress can quickly move beyond the
fact-finding stage to focus on what this institution can
realistically do to address the development and deployment of
trustworthy AI.
At this hearing, I hope we can discuss what the Science
Committee should focus on.
I look forward to today's very important discussion with
stakeholders from industry, academia, and venture capital. As a
representative from Silicon Valley, I know how important
private capital is to the U.S. R&D Ecosystem. Thank you all for
being with us today.
I yield back.

Chairman Lucas. The Ranking Member yields back.
Let me introduce our witnesses for today's hearing. Our
first witness today is Dr. Jason Matheny, Chair, President, and
CEO (Chief Executive Officer) of RAND Corporation. Prior to
becoming the CEO, the doctor led the White House policy on
technology and national security at the National Security
Agency in the Office of Science and Technology Policy. He also
served as Director of the Intelligence Advanced Research
Projects Activity (IARPA), and I'd also like to congratulate
him for being selected to serve on the Selection Committee for
the Board of Trustees for the National Semiconductor Technology
Center.
Our next witness is Dr. Shahin Farshchi, the General
Partner of Lux Capital, one of Silicon Valley's leading
frontier science and technology investors. He invests in the
intersection of artificial intelligence and science and has co-
founded and invested in many companies that have gone on to
raise billions of dollars.
Our third witness of the day is Dr. Clement Delangue, co-
founder and CEO of HuggingFace, the leading platform for open-
source AI community. It has raised over $100 million, with Lux
Capital leading their last financing round, and counts over
10,000 companies and 100,000 developers as users.
Next, we turn to Dr. Rumman Chowdhury, Responsible AI
Fellow at Harvard University, and she is a pioneer in the field
of applied algorithmic ethics, which investigates creating
technical solutions for trustworthy AI. Previously, she served
as Director of Machine Learning Accountability at Twitter and
founder of Parity, an enterprise algorithmic auditing company.
And our final witness is Dr. Dewey Murdick, the Executive
Director of Georgetown Center for Security and Emerging
Technology (CSET). He previously served as the Chief Analytics
Officer and Deputy Chief Scientist within the Department of
Homeland Security (DHS) and has also co-founded an office in
predictive intelligence at IARPA.
Thank you, all witnesses, for being here today. And I
recognize Dr. Matheny for the first five minutes to present
your testimony and overlook my phonetic weaknesses.

TESTIMONY BY DR. JASON MATHENY,

PRESIDENT & CEO, RAND CORPORATION

Dr. Matheny. No problem at all. Thanks so much, Chairman
Lucas, Ranking Member Lofgren, and Members of the Committee.
Good morning, and thank you for the opportunity to testify. As
mentioned, I'm the President and CEO of RAND, a nonprofit and
nonpartisan research organization, and one of our priorities is
to provide detailed policy analysis relevant to AI in the years
ahead. We have many studies underway relevant to AI. I'll focus
my comments today on how the Federal Government can advance AI
in a beneficial and trustworthy manner for all Americans.
Among a broad set of technologies, AI stands out both for
its rate of progress and for its scope of applications. AI
holds the potential to broadly transform entire industries,
including ones that are critical to our future prosperity. As
noted, the United States is currently the global leader in AI.
However, AI systems have security and safety vulnerabilities,
and a major AI-related accident in the United States or a
misuse could dissolve our lead much like nuclear accidents set
back the acceptance of nuclear power in the United States.
The United States can make safety a differentiator for our
AI industry, just as it was a differentiator for our early
aviation and pharmaceutical industries. Government involvement
in safety standards and testing led to safer products, which in
turn led to consumer trust and market leadership. Today,
government involvement can build consumer trust in AI that
strengthens the U.S. position as a market leader. And this is
one reason why many AI firms are calling for government
oversight to ensure that AI systems are safe and secure: It's
good for their business.
I'll highlight five actions that the Federal Government
could take to advance trustworthy AI within the jurisdiction of
this Committee. First is to invest in potential research
moonshots for trustworthy AI, including generalizable
approaches to evaluate the safety and security of AI systems
before they're deployed; second, fundamentals of designing
agents that will persistently follow a set of values in all
situations; and third, microelectronic controls embedded in AI
chips to prevent the development of large models that lack
safety and security safeguards.
A second recommendation is to accelerate AI safety and
security research and development through rapid high-return-on-
investment techniques such as prize challenges. Prizes pay only
for results and remove the costly barrier to entry for
researchers who are writing applications, making them a cost-
effective way to pursue ambitious research goals while opening
the field to non-traditional performers such as small
businesses.
A third policy option is to ensure that U.S. AI efforts
conduct risk assessments prior to the training of very large
models, as well as safety evaluations and red team tasks prior
to the deployment of large models.
A fourth option is to ensure that the National Institute of
Standards and Technology has the resources needed to continue
applications of the NIST Risk Management Framework and fully
participate in key international standards relevant to AI, such
as ISO/SC 42.
A fifth option is to prevent intentional or accidental
misuse of advanced AI systems by requiring that companies
report the development or distribution of very large AI
computing clusters, training runs, and train models, such as
those involving over 10 to the 26th operations.
Second, include in Federal contracts with cloud computing
providers requirements that they employ know-your-customer
screening for all customers before training large AI models.
And third, include in Federal contracts with AI developers
know-your-customer screening, as well as security requirements
to prevent the theft of large AI models.
Thank you for the opportunity to testify, and I look
forward to your questions later.
[The prepared statement of Dr. Matheny follows:]
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. Thank you. And I recognize Dr. Farshchi for
five minutes to present his testimony.

TESTIMONY BY DR. SHAHIN FARSHCHI,

GENERAL PARTNER, LUX CAPITAL

Dr. Farshchi. Thank you, Mr. Chairman.
Chairman Lucas, Ranking Member Lofgren, and Members of the
Committee, my name is Dr. Shahin Farshchi, and I'm a General
Partner at Lux Capital, a venture capital firm with $5 billion
of assets under management. Lux specializes in building and
funding tomorrow's generational companies that are leveraging
breakthroughs in science and engineering. I have helped create
and fund companies pushing the state-of-the-art in
semiconductors, rockets, satellites, driverless cars, robotics,
and AI.
From that perspective, there are two important
considerations for the Committee today. One, preserving
competition in AI to ensure our country's position as a global
leader in the field; and two, driving Federal resources to our
most promising AI investigators.
Before addressing directly how America can reinforce its
dominant position in AI, it is important to appreciate how some
of Lux's portfolio companies are pushing the state-of-the-art
in the field. HuggingFace, whose founder Clement Delangue is a
witness on this panel, MosaicML, a member of the HuggingFace
community, is helping individuals enterprises train, tune, and
run the most advanced AI models. Mosaic's language models have
exceeded the performance of OpenAI's GPT-3. Unlike OpenAI,
Mosaic's models are made available to their customers entirely,
as opposed to through an application programming interface,
thereby allowing customers to keep all of their data private.
Mosaic built the most downloaded LLM (large language model) in
history MPT-7B, which is a testament to the innovations coming
into the open source from startups and researchers.
RunwayML is bringing the power of generative AI to
consumers to generate videos from simple text and images.
Runway is inventing the future of creative tools with AI,
thereby reimagining how we create so individuals can achieve
the same expressive power of the most powerful Hollywood
studios. These are just a few examples of Lux companies
advancing the state-of-the-art in AI, in large part because of
the vital work of this Committee to ensure America is
developing its diverse talent, providing the private sector
with helpful guidance to manage risk, and democratizing access
to computing resources that fuel AI research and the next
generation of tools to advance the U.S. national interest.
To continue America's leadership, we need competitive
markets to give entrepreneurs the opportunity to challenge even
the largest dominant players. Unfortunately, there are steep
barriers to entry for AI researchers and founders. The most
advanced AI generative models cost more than $100 million to
train. If we do not provide open, fair, and diverse access to
computing resources, we could see a concentration of resources
in a time of rapid change, reminiscent of Standard Oil during
the Industrial Revolution.
I encourage this Committee to continue its leadership in
democratizing access to AI R&D by authorizing and funding the
National AI Research Resource, NAIRR. This effort will help
overcome the access divide and ensure that our country is
benefiting from diverse perspectives that will build the future
of AI technology and help guide its role in our society. I am
particularly concerned that Google, Amazon, and Microsoft,
which are already using vast amounts of personal data to train
their models, have also attracted a vast majority of
investments in AI startups because of the need to access their
vast computing resources to train AI models, further
entrenching their preexisting dominance in the market. In fact,
Google and Microsoft are investing heavily in AI startups under
the condition that their invested dollars are spent solely on
their own compute resources.
One example is AI--is OpenAI's partnership with Microsoft.
Through efforts such as NAIRR, we hope that competitors of
Google, Microsoft, and Amazon will be empowered to offer
compute resources to fledgling AI startups, while perhaps even
endowing compute resources directly to the startups and
researchers as well. This will facilitate a more competitive
environment that will be more conducive to our national
dominance at the global stage.
Furthermore, Congress must rebalance efforts toward
providing resources with deep investment into top AI
investigators. For example, the DOD (Department of Defense) has
taken a unique approach by allocating funding to top
investigators, as opposed to the National Science Foundation,
which tends to spread funding across a larger number of
investigators. When balanced appropriately, both approaches
have value to the broader innovation ecosystem. However, deep
investment has driven discoveries at the frontier, leading to
the creation of great companies like OpenAI, whose dollars were
initially funded--whose founders were initially funded by
relatively large DOD grant dollars. Building on these successes
is key to America's continued innovation--its continued success
in AI innovation.
Thank you for the opportunity to share how Lux Capital is
working with founders to advance AI in our national interest,
bolster our national defense and security, strengthen our
economic competitiveness, and foster innovation right here in
America. Lux is honored to play a role in this exciting
technology at this pivotal moment. I look forward to your
questions.
[The prepared statement of Dr. Farshchi follows:]
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. Thank you. And I recognize Mr. Delangue for
five minutes for his testimony.

TESTIMONY BY MR. CLEMENT DELANGUE,

CO-FOUNDER & CEO, HUGGINGFACE

Mr. Delangue. Chairman Lucas, Ranking Member Lofgren, and
Members of the Committee, thank you for the opportunity to
discuss AI innovation with you. I deeply appreciate the work
you are doing to advance and guide it in the United States. My
name is Clement Delangue, and I'm the co-founder and CEO of
HuggingFace. I'm French, as you can hear from my accent, and
moved to the United States 10 years ago, barely speaking
English. With my co-founders, Julien and Thomas, we started
this company from scratch here in the United States as a U.S.
startup, and we are proud to employ team members in 10
Different U.S. States today. I believe we could not have
created this company anywhere else. I am living proof that the
openness and culture of innovation in the United States allows
for such a story to happen.
The reason I'm testifying today is not so much the size of
our organization or the cuteness of our emoji name HuggingFace.
And contrary to what you said, Chairman Lucas, I don't hold a
Ph.D. like all the other witnesses. But the reason I'm here
today is because we enable 15,000 small companies--startups,
nonprofits, public organizations, and companies--to build AI
features and workflows. Collectively on our platform, they have
shared over 200,000 open models 5,000 new ones just last week,
50,000 open datasets, and 100,000 applications, ranging from
data anonymization for self-driving cars, speech recognition
from visual lip movement for people with hearing disabilities,
applications to detect gender and racial biases, translation
tools in low-resource languages to share information globally,
not only with large language models and generative AI, but also
with all sorts of machine-learning algorithms with usually
smaller, customized specialized models in domains as diverse as
social productivity platforms, finance, biology, chemistry, and
more.
We are seeing firsthand that AI provides a unique
opportunity for value creation, productivity boosting, and
improving people's lives, potentially at the larger scale and
higher velocity than the internet or software before. However,
for this to happen across all companies and at a sufficient
scale for the United States to keep leading compared to other
countries, I believe open science and open source are critical
to incentivize and are extremely aligned with American values
and interests.
First, it's good to remember that most of today's progress
has been powered by open science and open source like the
Attention Is All You Need paper, the BERT paper, the latent
diffusion paper, and so many others. The same way without open
source PyTorch, TensorFlow, Keras, transformers, diffusers, all
invented here in the United States, the United States might not
be the leading country for AI.
Now, when we look toward the future, open science and open
source distribute economic gains by enabling hundreds of
thousands of small companies and startups to build with AI. It
fosters innovation and fair competition between all. Thanks to
ethical openness, it creates a safer path for the development
of the technology by giving civil society, nonprofits,
academia, and policymakers the capabilities they need to
counterbalance the power of big private companies.
Open science and open source prevents black box systems,
make companies more accountable, and help solve today's
challenges like mitigating biases, reducing misinformation,
promoting copyrights, and rewarding all stakeholders, including
artists and content creators, in the value creation process.
Our approach to ethical openness combines institutional
policies such as documentation with model cards pioneered by
our own Dr. Margaret Mitchell, technical safeguards such as
staged releases, and community incentives like moderation and
opt-in/opt-out datasets.
There are many examples of safer AI thanks to openness,
like Bloom, an open model that has been assessed by Stanford as
the most compliant model with the EU AI Act or the research
advancements in watermarking for AI content. Some of that you
can only do with open models and open datasets.
In conclusion, by embracing ethical AI development with a
focus on open science and open source, I believe the United
States can start a new era of progress for all, amplify its
worldwide leadership, and give more opportunities to all like
it gave to me. Thank you very much.
[The prepared statement of Mr. Delangue follows:]
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. Absolutely. And thank you, Mr. Delangue.
And I would note probably that there would be some of my
colleagues who would note that your version of English might be
more understandable than my Okie dialect.
But setting that issue aside, I now recognize Dr. Chowdhury
for five minutes for her testimony.

TESTIMONY BY DR. RUMMAN CHOWDHURY,

RESPONSIBLE AI FELLOW, HARVARD

Dr. Chowdhury. Thank you, Chairman Lucas, Ranking Member
Lofgren, and esteemed Members of the Committee. My name is Dr.
Rumman Chowdhry, and I'm an AI developer, data scientist, and
social scientist. For the past seven years, I've helped address
some of the biggest problems in AI ethics, including holding
leadership roles in responsible AI at Accenture, the largest
tech consulting firm in the world, and at Twitter. Today, I'm a
responsible AI Fellow at Harvard University. I'm honored to
provide testimony on trustworthy AI and innovation.
Artificial intelligence is not inherently neutral,
trustworthy, nor beneficial. Concerted and directed effort is
needed to ensure this technology is used appropriately. My
career in responsible AI can be described by my commitment to
one word: Governance. People often forget that governance is
more than the law. Governance is a spectrum, ranging from codes
of conduct, standards, open research, and more. In order to
remain competitive and innovative, the United States would
benefit from a significant investment in all aspects of AI
governance.
I would like to start by dispelling the myth that
governance stifles innovation. Much to the contrary, I use the
phrase ``Brakes help you drive faster'' to explain this
phenomenon. The ability to stop your car in dangerous
situations is what enables us to feel comfortable driving at
fast speeds. Governance is innovation.
This holds true for the current wave of artificial
intelligence. Recently, a leaked Google memo declared there is
no moat. In other words, AI will be unstoppable as open-source
capabilities meet and surpass closed models. There is also a
concern about the United States remaining globally competitive
if we aren't investing in AI development at all costs. This is
simply untrue. Building the most robust AI industry isn't just
about processors and microchips. The real competitive advantage
is trustworthiness.
If there's one thing to take away from my testimony, it's
that the U.S. Government should invest in public accountability
and transparency of AI systems. In this testimony, I describe
how. I make the following four recommendations to ensure the
United States advances innovation in the national interest:
First, support for AI model access to enable independent
research and audit; second, investment in and legal protections
for red teaming and third-party ethical hacking; third, the
development of a non-regulatory technology body to supplement
existing U.S. Government oversight efforts; fourth,
participation in global AI oversight.
CEOs of the most powerful AI companies will tell you that
they spend significant resources to build trustworthy AI. This
is true. I was one of those people. My team and I held
ourselves to the highest ethical standards, and as--and my
colleagues who remain in these roles still do so today.
However, a well-developed ecosystem of governance also empowers
individuals whose organizational missions are to inform and
protect society. The DSA's (Digital Services Act's) article 40
creates this sort of access for Europeans. Similarly, the U.K.
Government has announced Google DeepMind and OpenAI will allow
model access. My first recommendation is the United States
should match these efforts.
Next, new laws are mandating third-party algorithmic
auditing. However, there is currently a workforce challenge in
identifying sufficiently trained third-party algorithmic
investigators. Two things can fix this. First, funding for
independent groups to conduct red teaming and adversarial
auditing; and second, legal protections so these individuals
operating in the public good are not silenced with litigation.
With the support of the White House, I am part of a group
designing the largest-ever AI red teaming exercise in
collaboration with the largest open and closed source AI
companies. We will provide access to thousands of individuals
who will compete to identify how these models may produce
harmful content. Red teaming is a process by which invited
third-party experts are given special-permission access by AI
companies to find flaws in their models. Traditionally, these
practices happen behind closed doors, and public information-
sharing is at the company's discretion. We want to open those
closed doors. Our goals are to educate, address
vulnerabilities, and importantly, grow a new profession.
Finally, I recommend investment in domestic and global
government institutions in alignment with this third-party
robust ecosystem. A centralized body focus on responsible
innovation could assist existing oversight by promoting
interoperable licensing, conducting research to inform AI
policy, and sharing best practices and resources. Parallels in
other governments include the U.K. Center for Data Ethics and
Innovation, of which I'm a board member, and the EU Center for
Algorithmic Transparency.
There's also a sustained an increasing call for global
governance of AI system, among them, experts like myself,
OpenAI CEO Sam Altman, and former New Zealand Prime Minister
Jacinda Ardern. A global governance effort should develop
empirically driven, enforceable solutions for algorithmic
accountability and promote global benefits of AI systems.
In sum, innovation in the national interest starts with
good governance. By investing in and protecting this ecosystem,
we will ensure AI technologies are beneficial to all. Thank you
for your time.
[The prepared statement of Dr. Chowdhury follows:]
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. Thank you, Doctor.
And I now recognize Dr. Murdick for five minutes to present
his testimony.

TESTIMONY BY DR. DEWEY MURDICK, EXECUTIVE DIRECTOR,

CENTER FOR SECURITY AND EMERGING TECHNOLOGY

Dr. Murdick. Thank you, Chairman Lucas, Ranking Member
Lofgren, and everyone on the Committee, to be--have this
opportunity to talk about how we can make AI better for our
country.
There are many actions Congress can do to support AI
innovation, protect key technology from misuse, and ensure
customers or consumers are safe. I'd like to highlight three
today.
First, we need to get used to working with AI as a society
and individually. We need to learn what--when we can trust our
AI teammates and when to question or ignore them. I think this
takes a lot of training and time.
Two, we need skilled people to build future AI systems and
to increase AI literacy.
Three, we need to keep a close eye on the policies that we
do enact. We need to make sure that every policy is being--
action is being monitored and make sure it's actually doing
what we think it's doing and update that as we need to. This is
especially true when we're facing peer innovators, and
especially in a rapidly changing area like artificial
intelligence.
China is such a peer innovator, but we need to remember
they're not 10 feet tall, and they have different priorities
for their AI than we do. China's AI leadership is evident
through aggressive use of State power and substantial research
investments, making it a peer innovator for us and our allies,
never far ahead and never far behind either. China focuses on
how AI can assist military decisionmaking and mass surveillance
to help maintain societal control. This is very unlike the
United States, thankfully. Managing that control means they're
not letting AI run around all willy nilly. In fact, the
deploying of--the deployment of large language models does not
appear to be a priority for China's leadership precisely for
that reason. We should not let the fear of China surpassing us
deter our oversight of AI industry and AI technology. Instead,
the focus should be on developing methods that allow
enforcement of AI risk and harm management and guiding the
innovation and advancement of AI technology.
I'd like to return to my first three points and expand on
them a little bit. Going back to my opening points, the first
one was we must get used to working with AI via effective human
machine teaming, which is central to AI's evolution, in my
opinion, in the next decade. Understanding what an AI system
can and cannot do and should and shouldn't do, and when to rely
on them and when to avoid using them should guide our future
innovation and also our training standards.
One thing that keeps me up at night is when human partners
trust machines when they shouldn't, and there's interesting
examples of that. They fail to trust AI when they should or are
manipulated by a system. And there are some instances of that
also.
We've witnessed rapid AI advancements, and the convergence
between AI and other sectors promises widespread innovation in
areas from medical imaging to manufacturing. Therefore,
fostering AI literacy across the population is critical for
economic competitiveness. But also--and I think even more
importantly--it is essential for democratic governance. We
cannot engage in a meaningful societal debate about AI if we
don't understand enough about it. This means an increasingly
large fraction of the U.S. citizens will encounter AI daily.
So that's the second point; we need skilled people working
at all levels. We need innovators from technical and non-
technical backgrounds. We need to attract and retain diverse
talent from across our Nation and internationally. And
separately from those who are building the AI systems, these
future and current ones, we need comprehensive AI training for
the general population, K through 12 curricula, certifications.
There's a lot of good ideas there. AI literacy is the central
key, though.
So what else can we do? I think we can promote better
decisionmaking by gathering information now that we need to
make decisions. For example, tracking AI harms via incident
reporting is a good way to learn where things are breaking,
learning how to request key model and training data for
oversight to make sure it's being used in important
applications correctly. We don't know how to do that.
Encouraging and developing third-party auditing, an ecosystem,
the red teaming ecosystem, excellent.
If we are going to license AI software, which is a common
proposal we hear, we're probably going to need to update
existing authorities for existing agencies, and we may need to
create a new one, a new agency or organization. This new
organization could check how AI is being used and overseen by
existing agencies. It can be the first to deal with problems
directing those in need to the right solutions, either in the
government or private sector, and fill gaps in sector-specific
agencies.
My last point--and I see I'm going too long--we need to
make sure our policies are monitored and effectively
implemented. There's really great ideas in the House and Senate
on how to increase the analytic capacity to do that.
I look forward to this discussion because I think this is a
persistent issue that is just not going to go away, and CSET
and I have dedicated our professional lives to this. So thank
you so much.
[The prepared statement of Dr. Murdick follows:]
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

Chairman Lucas. Thank you, Doctor. And thank you to the
entire panel for some very insightful opening comments.
Continuing with you, Dr. Murdick, making AI systems safer
is not only a matter of regulations, but also requires
technical advances in making the systems more reliable,
transparent, and trustworthy. It seems to me that the United
States would be more likely than China to invest in these
research areas, given our democratic values. So, Doctor, can
you compare or expand on your comments earlier? Can you compare
how the Chinese Communist Party and the United States'
political values influence their research and development
priorities for AI systems?
Dr. Murdick. Sure. I think that the large language models,
which is the obsession right now of a lot of the AI systems,
provides a very interesting example of this. China is very
concerned about how it can destabilize their societal structure
by having a system that they can't control and might say things
that would be offensive about--they might bring up the
Tiananmen Square or might associate the president with Winnie
the Pooh or something, and that can be very destructive to
their way of--their societal control. Because of this, they're
really limiting control. They're passing regulations and laws
that are very constraining of how that's going. So that is a
difference between our societies and how we--what we view as
acceptable.
I do think the military control, the command-and-control
emphasis, as well as the desire to maintain control through
mass surveillance, if you look at their research portfolio,
most of where they're leading is--could be very well associated
with those types of areas, so I think those are differences
that are pretty significant.
We can go on, but I'm going to just pause there to make
sure other people have----
Chairman Lucas. Absolutely. Dr. Matheny, some advocated for
a broad approach to AI regulations such as restricting entire
categories of AI systems. In the United States, many agencies
already have the existing authorities to regulate the user
cases of AI in their jurisdiction. For example, the Department
of Transportation can perform set performance benchmarks that
autonomous vehicles (AVs) must meet to drive on U.S. roads.
What are your opinions regarding outcomes of user case-driven
approach to AI regulation versus an approach that places broad
restrictions on AI development or deployment?
Dr. Matheny. Thank you, Mr. Chairman. I think that in many
of the cases that we're most concerned about related to risks
of AI systems, especially these large foundation models, we may
not know enough in advance to specify the use cases. And those
are ones then where the kind of testing and red teaming that's
been described here is really essential. So having terms and
conditions in Federal contracts with compute providers actually
might be one of the highest leverage points of governance: We
could require that models trained on infrastructure that is
currently federally contracted involve red teaming and other
evaluation before models are trained or deployed.
Chairman Lucas. Mr. Delangue, in your opening--in my
opening statement, I highlighted the importance of ensuring we
continue to lead in AI research and development because
American-built systems are more likely to reflect democratic
values. Given how critical it is to ensure we maintain our
leadership in AI, how do you recommend Congress ensure that we
do not pass regulations that stifle innovation?
Mr. Delangue. I think a good point that you made earlier is
that AI is so broad and the impact of AI could be so widespread
across use cases, domain sectors, is--leads to the point that
for regulation to be effective and not stifle innovation at
scale, you need it to be, you know, customized and focused on
specific domains, use cases, and sectors where there are more
risks and then kind of like empower the whole ecosystem to keep
growing and keep innovating.
The parallel that I like to draw is with software, right?
It's such kind of like a broad, applicable technology that the
important thing is to regulate the final use cases in the
specific domains of application of software rather than
software in general.
Chairman Lucas. In my remaining moments, Dr. Farshchi,
advances in civilian AI--civilian R&D often help progress
defense technologies and enhance national security. How have
civilian R&D efforts in AI transformed in advances in defense
applications, and how do you anticipate that relationship
evolving over the next few years?
Dr. Farshchi. I expect that relationship to evolve in a
positive way. I expect it to--I expect there to be a further
strengthening of the relationship between the private sector
and dual use and government-targeted products. Thanks to the
open source, there are many technologies that otherwise would
have not been available, would it have to be--would have had to
be reinvented are now made available to build on top of. The
venture capital community is very excited about funding
companies that have dual-use products and companies that sell
to the U.S. Government. Palantir was an example where investors
profited greatly from investing in a company that was
targeted--that was targeting the U.S. Government as customers,
same with SpaceX. And so it's my expectation that this trend
will continue, that more private dollars will go into funding
companies that are selling to the U.S. Government and
leveraging technologies that come out of the open source to
build on top of to continue innovating the sector.
Chairman Lucas. Thank you. My time has expired, and I now
recognize the Ranking Member, Ms. Lofgren.
Ms. Lofgren. Thank you, Mr. Chairman. And thanks to our
panelists. This is wonderful testimony. And I'm glad this is
the first of several hearings because there's a lot to wrap our
heads around.
Dr. Chowdhury, as you know, large language models have
basically vacuumed up all the information from the internet,
and there is a dispute between copyright holders who feel they
ought to be compensated, others who feel it's a fair use of the
information. You know, it's in court. They will probably decide
it before Congress will.
But here's the question I've had. What techniques are
possible to identify copyrighted material in the training
corpus of the large language models? Is it even possible to go
back and identify protected material?
Dr. Chowdhury. Thank you for the excellent question. It's
not easy. It is quite difficult. What we really need is
protections for the individuals generating this artwork because
they are at risk of having not only their work stolen, but
their entire livelihood taken.
Ms. Lofgren. No, I understand but the question is----
Dr. Chowdhury. Yes.
Ms. Lofgren [continuing]. Retroactively, is it possible to
identify?
Dr. Chowdhury. It's difficult to, but yes, it can. One can
use digital imaging matching. But it's also important to think
through, you know, what we are doing with the data, what it is
being used for----
Ms. Lofgren. No, I understand that as well.
Dr. Chowdhury. Absolutely. Thank you.
Ms. Lofgren. One of the things I'm interested in, you
mentioned CFAA (Computer Fraud and Abuse Act), and that's a
barrier to people trying to do third-party analysis. I had a
bill--actually, after Aaron Swartz's untimely and sad demise, I
had a bill named after him to allow those who are making non-
commercial use to actually do what Aaron was doing. And Jim
Sensenbrenner was my co-sponsor, since retired, and we couldn't
get anywhere. There were large vested interests who opposed
that. Do you think if we approached it just as those who
register rather than be licensed with the government as non-
commercial entities that are doing redshirting, whether that
would be a viable model for what you're suggesting?
Dr. Chowdhury. I think so, yes. I think that would be a
great start. I do think there would need to be a follow up to
ensure that people are indeed using it for non-commercial
purposes.
Ms. Lofgren. Correct. I'm interested in the whole issue of
licensing versus registration. I'm mindful that the Congress
really doesn't know enough in many cases to create a licensing
regime. And the technology is moving so fast that I fear we
might make some mistakes or Federal agencies might make some
mistakes. But licensing, which is giving permission, would be
different than registration, which would allow for the capacity
to oversee and prevent harm. What's your thought on the two
alternatives, anyone who wants to speak?
Dr. Chowdhury. I can speak to that. I think overly onerous
licensing would actually prevent people from doing one-off
experimental or fun exercises. What we have to understand is,
you know, there is a particular scale of impact, number of
people being--you know, using a product that maybe would
trigger licensing, rather than saying everybody needs to
license. There are college students, high school kids who want
to use these models and just do fun things, and we should allow
them to do it.
Ms. Lofgren. You know, I guess I have some qualms--and
other Members may--you know, we've got large model AIs, and to
some extent, they are a black box. Even the creators don't
fully understand what they're doing. And the idea that we would
have a licensing regime, I think, is very daunting, as opposed
to a registration regime where we might have the capacity for
third parties to do auditing and the like. I'll just lay that
out there.
I want to ask anybody who can answer this question, when it
comes to large language models, generative AI, the computing
power necessary is so immense that we've ended up with
basically three very large private-sector entities who have the
computing capacity to actually do that. Mr. Altman was here, I
think, last month and opined when he met with us at the Aspen
Institute breakfast that it might not even be possible to catch
up in terms of the pure computing power. We've had discussions
here on whether the government should create the computing
power to allow not only private sector but academics to be
competitive. Is that even viable at this point, whoever could
answer that?
Dr. Farshchi. I can take that real quick. I believe so. I
think it is possible. Bear in mind that the compute resources
that were created by these three entities were initially meant
for internal consumption----
Ms. Lofgren. Correct.
Dr. Farshchi [continuing]. And they have been repurposed
now for training AI. And semiconductor development, which is
the core of this technology, there is ultimately a tradeoff
between narrow functionality and--or breadth of functionality
and performance at a single use case. And so if there was a
decision made to build resources that were targeted at training
large language models, for example, I think it would be
possible to quickly catch up and build that resource, the same
way we built specific resources during World War II for a
certain type of warfare and then again during the Persian Gulf
War. So I think it's--we as a Nation are capable of doing that.
Ms. Lofgren. I see my time has expired. I thank you so
much. And my additional questions we'll send to you after the
hearing, and I yield back.
Mr. Obernolte [presiding]. The gentlewoman yields back.
I will recognize myself for five minutes for my questions.
And I want to thank you for the really fascinating testimony.
But, Dr. Matheny, I'd like to start with you. You'd brought up
the concept of trustworthy AI, and I think that that's an
extremely important topic. I actually dislike the word
trustworthy AI because it imparts to AI something that it
doesn't have, human qualities. It's just a piece of software. I
was interested, Dr. Murdick, when you said sometimes human
partners trust AI when they shouldn't and fail to trust it when
they should, and I think that that is a better way of
expressing what we mean when we talk about AI.
But this is an important conversation to have because we in
Congress, as we contemplate establishing a regulatory framework
for AI, we need to be explicit when we say we want it to be
trustworthy. You know, we can talk about efficacy or robustness
or repeatability, but we need to be very specific when we talk
about what we mean by trustworthy. It's not helpful to use
evocative terms like, well, AI has to share its values, which
is something that's in the framework of other countries'
approach to AI. Well, that's great. You know, values, that's a
human term. What does that mean for AI to have values?
So the question for you, Doctor, is what do we mean when we
say trustworthy AI, and, you know, in what context should us as
lawmakers think about AI as trustworthy?
Dr. Matheny. Thank you. When we talk about trustworthiness
of engineered systems, we usually mean do they behave as
predicted? Do they behave in a way that is safe, reliable, and
robust given a variety of environments? So, for example, do we
have trust in our seatbelt? Do we have trust in the antilock
braking system of a car? Do we have trust in the accident
avoidance system on an airplane? So those are the kinds of
properties that we want in our engineered systems is, are they
safe, are they reliable, are they robust.
Mr. Obernolte. I would agree. And I think that we can put
metrics on those things. I just don't think that calling AI
trustworthy is helpful because we're already having this
perceptual problem that people are thinking of it in an
anthropomorphic way. And it isn't. It is just software. And,
you know, we can talk about the intent. When we deploy it, we
can talk about the intent when we create it of us as humans,
but to impart those qualities to the software I think is
misleading to people.
Dr. Chowdhury, I want to continue the Ranking Member's line
of questioning, which I thought was excellent, on the
intersection between copyright holders and content creators and
the training of AI because I think that this is going to be a
really critical issue for us to grapple with. And I mean,
here's the problem. The--if we say, as some content creators
have suggested, that no copyrighted content can be used in the
training of AI, you know, which from their point of view is a
completely reasonable thing to be saying, but if we do that,
then we're going to wind up with AI that is fundamentally
useless in a lot of different domains.
And let me give you a specific example, because I'd like
your thoughts on it. I mean, one trademarked term is Super
Bowl, right? The NFL (National Football League) would not like
someone using the word Super Bowl in a commercial sense. If you
own a bar, you have to talk about, you know, the party to watch
the big game, nudge, nudge, wink, wink, right? Which is, from
their point of view, completely reasonable. But if you
prohibited the use of the word Super Bowl in training AI, you'd
come up with a large language model that if you said what time
is the Super Bowl, it would have no idea what you were talking
about. You know, it would lack the--you know, the context to be
able to answer questions like that. So how do we navigate that
space?
Dr. Chowdhury. I think you bring up an excellent point. I
think these technologies are going to push the upper limits of
many of the laws we have, including protections for copyright.
I don't think there's a good answer. I think this is what we
are negotiating today. The answer will lie somewhere in the
spectrum. There will be certain terms. I think a similar
conversation happened about the term ``taco Tuesday'' and the
ability to use it widely, and it was actually decided you could
use it widely. I think some of these will be addressed on a
case-by-case basis. But more broadly, I think the thing to keep
an eye on is whether or not somebody's livelihood is being
impacted. It's not really about a word or picture. It is
actually about whether someone is taken out of a job because of
a model that's being built.
Mr. Obernolte. I partially agree. You know, I think it is--
it gets into a very dicey area when we talk about if someone's
job is being impacted because AI is going to be extremely
economically disruptive. And our job as lawmakers is to make
sure that disruption is largely positive for the median person
in our society. But, you know, jobs will be impacted. We hope
that most will be positive and not negative.
I actually think--and I am going to run out of time here,
but I--we have a large body of legal knowledge already on this
topic around the concept of fair use, and I think that that
really is the solution to this problem. There'll be fair use of
intellectual property in AI, and there'll be things that are--
clearly are not fair use or infringing. And I think that we can
use that as a foundation, but I'd love to continue the
discussion later.
Dr. Chowdhury. Absolutely.
Mr. Obernolte. Next, we'll recognize the gentlewoman from
Oregon. Ms. Bonamici, you are recognized for five minutes.
Ms. Bonamici. Thank you, Mr. Chairman, Ranking Member.
Thank you to the witnesses for your expertise.
I acknowledge the tremendous potential of AI but also the
significant risks and concerns that I've heard about, including
what we just talked about, potential job displacement, privacy
concerns, ethical considerations, bias, which we've been
talking about in this Committee for years, market dominance by
large firms and in the hands of scammers and fraudsters, a
whole range of nefarious possibilities to mislead and deceive
voters and consumers. Also, the datasets take up an enormous
amount of energy to run. We know--we acknowledge that. So we
need responsible development with ethical guidelines to
maximize the benefits and minimize the risks of course.
So, Dr. Chowdhury, as AI systems move forward, I remain
concerned about the lack of diversity in the workforce. So
could you mention how increasing diversity will help address
bias?
Dr. Chowdhury. What an excellent point. Thank you so much,
Congresswoman. So, first of all, we need a wide range of
perspectives in order to understand the impact of artificial
intelligence systems. I'll give you a specific example from my
time at Twitter. We held the first algorithmic bias bounty.
That meant we opened up a Twitter model for public scrutiny.
And we learned things that my team of highly educated Ph.D.'s
wouldn't think of. For example, did you know if you put a
single dot on a photo, you could change how the algorithm
decided where to crop the photo? We didn't know that. Somebody
told us this. Did you know that algorithmic cropping tended to
crop out people in camouflage because they blended in with
their backgrounds. It did what camouflage is supposed to do. We
didn't know that. So we learn more when we bring more people
in. So you know, open--more open access, independent researcher
funding, red teaming, et cetera, opening doors to people will
be what makes our systems more robust.
Ms. Bonamici. Absolutely, appreciate that so much.
And I want to continue, Dr. Chowdhury. I want to talk about
the ethics. And I expect that those in this room will all agree
that ethical AI is important to align the systems with values,
respect fundamental rights, contribute positively to society
while minimizing potential harms and gets to this
trustworthiness issue, which you mentioned and that we've been
talking about. So who defines what ethical is? Is there a
universal definition? Does Congress have a role? Is this being
defined by industry? I know there's a bipartisan proposal for a
Blue Ribbon Commission to develop a strategy for regulating AI.
Would this be something that they would handle or would NAIRR
be involved?
And also, I'm going to tell you the second part of this
question and then let you respond. In your testimony, you talk
about ethical hackers and your testimony explains the role that
they play, but how can they help design and implement ethical
systems? And how can policy differentiate between bad hackers
and ethical hackers?
Dr. Chowdhury. Both great questions. So, first, I want to
address the first part of what you brought up is who defines
ethics. And, you know, fortunately, this is not a new problem
with technology. We have grappled with this in the law for
many, many years. So, you know, I recognize that. Previously,
someone mentioned that we seem to think a lot of problems are
new with technology. This is not a new problem. And usually
what we do is we get at this by a diversity of opinions and
input and also ensuring that our AI is reflective of our
values. And we've articulated our democratic values, right, for
the United States. We have the blueprint of the AI Bill of
Rights. We have the NIST AI Risk Management Framework. So we've
actually as a nation sat down and done this.
So to your second question on ethical hackers, ethical
hackers are operating in the public good, and there is a very
clear difference. So what an ethical hacker will do is, for
example, identify a vulnerability in some sort of a system, and
often, they actually go to the company first to say, hey, can
you fix this? But often, these individuals are silenced with
threats of litigation, so what we need to do is actually have
increasing protections for these individuals who are operating
in the public good, have repositories where people can share
this information with each other, and also allow companies to
be part of this process. For example, what might responsible
disclosure look like? How can we make this not an adversarial
environment where it's the public versus companies but the
public as a resource for companies to improve what they're
doing above and beyond what they're able to do themselves?
Ms. Bonamici. I appreciate that. And I want to follow up on
the earlier point about--yes, and I'm aware of the work that's
been done so far on the ethical standards. However, I'm just
questioning whether this is something that--does it need to be
put into law, to regulation? Does everyone agree? And could
this--is there hope that there could be some sort of universal
standard?
Dr. Chowdhury. I do not----
Ms. Bonamici. Ethical standard.
Dr. Chowdhury. I do not think a universal ethical standard
is possible. We live in a society that reflects diversity of
opinions, thought, and we need to respect that and encourage
that. But how do we prevent--how do we create the safeguards
and identify what is harmful? In social media, we think a lot
about what is harmful content, toxic content, and all of this
lives on the spectrum. And I think any governance that's
created has to respect that society changes, people change, the
words and terms we use to reflect things change, and our ethics
will change as well. So creating a flexible framework by which
we can create ethical guidelines to help people make smart
decisions is a better way to go.
Ms. Bonamici. And just to confirm, that would be voluntary,
not mandatory? Oh, I see my time has expired. I must yield
back. Thank you. Could you--can she just answer that question,
just yes or no?
Mr. Obernolte. Certainly.
Dr. Chowdhury. Yes.
Ms. Bonamici. Thank you.
Mr. Obernolte. The gentlewoman yields back.
We'll go next to my colleague from California. Congressman
Issa, you're recognized for five minutes.
Mr. Issa. Thank you. I'll continue right where she left
off, Dr. Chowdhury. The--boy, the last answer got me just
perfect because--so it's going to be voluntary? So who decides
who puts that dot in a place that affects the cropping?
Dr. Chowdhury. I'm going to continue my answer. So it can
be voluntary, but in certain cases, in high risk cases, in high
impact----
Mr. Issa. We don't do voluntary here in Congress. You know
that.
Dr. Chowdhury. I do. But I also do recognize that the way
that we have structured regulation in the United States is
context-specific, you know, financial authorities----
Mr. Issa. OK. Well, let's get a context because----
Dr. Chowdhury. Yes.
Mr. Issa [continuing]. And it's for a couple of you. If
you've got a Ph.D., you're eligible to answer this question.
And if you've published a book, you're eligible to answer this
question. Now, the most knowledgeable person on AI up here at
the dais that I know of is sitting in the chair right now, so--
but he happened to say fair use. And of course, that gets my
hackles up as the Chairman of the Subcommittee that determines
what fair use is. Now, how many of you went to college and
studied from books that were published for the pure purpose of
your reading them, usually by the professor who wrote them?
Raise your hand. OK. Nothing has changed in academia.
So is it fair use to not pay for that book, absorb the
entire content, and add it to your learning experience? So then
the question--and I'll start with you because both Twitter time
and academia time, and so on. Today, everyone assumes that the
learning process of AI is fair use. Is there any basis for it
being fair use rather than maybe a standard essential
copyright, one that must be licensed? But if you're going to
absorb every one of your published books and every one of the
published books that gave you the education you have and call
it fair use, are we in fact turning upside down the history of
learning and fair use just because it's a computer?
Dr. Chowdhury. I think the difference here is--there's a
difference between me borrowing a book from my friend,
learning, and my individual impact on society----
Mr. Issa. Oh, because it's cumulative, because you've
borrowed everyone's book and read everyone's book?
Dr. Chowdhury. Well, and because it's at scale. I cannot
impact hundreds of millions of people around the world the way
these models can. The things I say will not change the shape of
democracy.
Mr. Issa. So--I'll accept that. Anyone want to have a
slightly different opinion on stealing 100% of all the works
from--of all time, both copyrighted and not copyrighted, and
calling it fair use and saying because you took so much,
because you stole big, that, in fact, you're fine? Anyone want
to defend that?
Dr. Chowdhury. I'm sorry, I don't think that at all. I
think it's wrong to do that. I think it's wrong to----
Mr. Issa. So do--is one of the essential items that we must
determine the rights of the information that goes in to be
properly compensated, assuming it's under copyright?
Dr. Chowdhury. Yes.
Mr. Issa. You all agree with that? OK. Put them in your
computers and give us an idea of how we do it because,
obviously, this is something we've never grappled with. We've
never grappled with universal copyright absorption. And
hopefully, since all of you do publish and do think about it
and the RAND Corporation has the dollars to help us, please
begin the process because it's one of the areas that is so--
it's going to emerge incredibly quickly.
Now, obviously, we could switch from that to--we're talking
about job losses and so on. One of my questions of course is
if--for anyone who wants to hypothecate this, if we put all of
the information in and we turn on a computer and we give it the
funding of the Chinese Communist Party and we say patent
everything, do we in fact eliminate the ability to--or--to
independently patent something and not be bracketed by a
massive series of patents that are all or in part produced by
artificial intelligence? I'm not looking at 2001: A Space
Odyssey or Terminator. I'm looking at destruction of the
building blocks of intellectual property that have allowed for
innovation for hundreds of years. Most courageous, please.
Dr. Murdick. Yes, I don't know why I'm turning the mic on
at this moment. But I think there's a core--for the--both two
questions you asked, I think money actually matters. The reason
that we haven't litigated fair use fully is there hasn't been
much money made in these models that have been incorporating
all the copyrighted material----
Mr. Issa. Trust me, Google was sued when they were losing
money, too.
Dr. Murdick. But I do think the fact that there's a view
that there's a market will change the dynamics about how
because people are saying, wait, you're profiting off of my
content. And there will be--so I do think money changes this a
little bit. And I think--and it goes to the second question,
too. I think the pragmatics of money will change the dynamics.
Anyway, it's a fairly simple observation on this point.
Mr. Issa. Thank you. And I apologize for dominating on
this, but I'm going to Nashville this weekend to meet with a
whole bunch of songwriters that are scared stiff, so I--anyone
that wants to follow up? Yes, if the Chairman doesn't mind?
Mr. Delangue. Yes, I think an interesting example, some
people have been describing HuggingFace as some sort of--kind
of like a giant kind of like library. And in a sense, we accept
that people can rent books at the library because it
contributes to kind of like public and global kind of like
progress. I think we can see the same thing there where if we
kind of like give access to this content for open source, for
open science, I think it should be accepted in our society
because it contributes to public good. But when it's for
private, commercial interests, then it should be approached
differently. And actually, in that sense, you know, open
science and open source is some sort of a solution to this
problem because it gives transparency. There is no way to know
what copyright content is used in black box systems like most
of the systems that we have today in order to take a decision--
--
Mr. Issa. That knocking says the rest for the record. He's
been very kind. Thank you.
Mr. Delangue. Thank you.
Mr. Obernolte. The gentleman yields back.
We'll go next to the gentlewoman from Michigan. Ms.
Stevens, you're recognized for five minutes.
Ms. Stevens. And thank you, Mr. Chair. I'm hoping I can
also get the extra minute given that I am so excited about what
we're talking about here today. And I want to thank you all for
your individual expertise and leadership on a topic that is
transforming the very dialog of society and the way we are
going to function and do business yet again at the quarter 21st
century mark after we've lived through many technological
revolutions already before us.
And to my colleague who mentioned some very interesting
points, I'll just say, we're in a race right now. I'm also on
the Select Committee on Competitiveness with the Chinese
Communist Party. And I come from Michigan if you can't tell
from my accent. And artificial intelligence is proliferating
with autonomous vehicle technology. And we're either going to
have it in this country or we're not. That's the deal, right?
We could ask ourselves, what happened to battery manufacturing
and why we're overly reliant on the 85% of battery
manufacturing that takes place in China when we would like it
to be Michigan manufacturers, when we'd like it to be United
States manufacturers? And we're catching up, so invest in the
technology.
I mean, certainly, our witness here from the venture
capital company with $5 billion is going to be making those
decisions, but our Federal Government has got to serve as a
good steward and partner of the technology proliferation
through the guardrails that Dr. Chowdhury is talking about or
our lunch will be eaten. This is a reality. We've got one AV
manufacturer that is wholly owned in the United States. That's
Cruise, a subsidiary of General Motors, and they've got to
succeed. And we've got a pass the guardrails to enable them to
make the cars because right now they can only do 2,500.
So the question I wanted to ask is--because yesterday, I
sent a note. It's a letter to the Secretary of State, Mr.
Blinken, and I asked about this conversation, this point that
has come up multiple times in the testimony about how we're
going to dialog at the international level to put up these
proper AI guardrails.
And so, Dr. Chowdhury, you've brought this up in your
testimony. And what do you think are the biggest cross-national
problems in need of global oversight with regard to artificial
intelligence right now?
Dr. Chowdhury. Thank you. What a wonderful question,
Congresswoman. I don't think every problem is a global problem.
The important thing to ask ourselves is simply what is the
climate change of AI? What are the problems that are so big a
given country or a given company can't solve it themselves?
These are things like information integrity, preserving
democratic values and democratic institutions, CSAM, child
sexual abuse material, radicalization. And we do have
organizations that have been set up that are extra-governmental
to address these kinds of problems. But there's more--what we
need is a rubric or a way of thinking through the biggest
problems and how we're going to work on them in a
multistakeholder fashion.
Ms. Stevens. Right. And, Dr. Murdick, could you share
with--which existing authorities you believe would be best to
convene the global community to abide by responsible AI
measures?
Dr. Murdick. Well, I'm not sure I can name chapter and
verse----
Ms. Stevens. Treaties----
Dr. Murdick. Well, so I think just one point about the core
goal. The United States plus its allies is stronger than China.
We--by a lot of different measures you look at, you know,
everything from research production, to technology, to
innovation, to talent-space, size, to companies, to--you know,
to investment levels, I think it's important that we're
stronger together if we can work together. If we're ever going
to implement any kind of export controls effectively, we have
to do them together, as opposed to individually.
So there's a lot of venues, international multiparty bodies
that work on a variety of things. There's a lot of treaties.
There's plurilateral, there's multilateral agreements, and I
think we do have to work together to be able to implement
these.
Ms. Stevens. Yes.
Dr. Murdick. And I think any and all of them are relevant.
Ms. Stevens. Well, and allow me to just say Mr. Delangue,
we have a great French-American presence in my home State of
Michigan, and we'd love to see you look at expanding to my
State. We've got a lot of exciting technology happening. And,
as I mentioned with autonomous vehicles, just to level set
this, we've got CCP-owned autonomous vehicle companies that are
testing in San Francisco and all along the West Coast, but we
cannot test our technology in China. We cannot sell our
technology, our autonomous vehicle technology in China. That's
an example of a guardrail.
I love what you just said, Dr. Murdick, about working with
our allies, working with open democratic societies to make,
produce, and innovate and win the future.
Thank you so much, Mr. Chair. I yield back.
Mr. Obernolte. The gentlewoman yields back.
We'll go next to the gentleman from Ohio. Mr. Miller,
you're recognized for five minutes.
Mr. Miller. Thank you, Mr. Chairman and Ranking Member
Lofgren, for holding this hearing today.
Dr. Murdick, as you highlighted in your written testimony,
AI is just not a field to be left to Ph.D.'s and engineers. We
need many types of workers across the AI supply chain, from
Ph.D. computer scientists, to semiconductor engineers, to lab
technicians. Can you describe the workforce requirements needed
to support the entire AI ecosystem, and what role individuals
with technical expertise play?
Dr. Murdick. Now, this is a great question, and I will try
to be brief because it's such a fascinating conversation that
when you look at the AI workforce, it's super easy to get
fixated on your Ph.D. folks. And I think that was a big
mistake. One of the first applications that I started seeing
discussed was a lawn--a landscaping business using--the
proprietor would--had trouble writing, and they started using
large language models to help communicate with customers to
give them proposals when they couldn't do it before. That
proliferation--now, I don't mean that in a proliferation way.
Sorry. That expansion of capabilities into--and users that are
much wider than your standard users is really the space we are
living today.
So in the deployment of any AI capability, you have your
core tech team, you have your team that surrounds them or
implements the software engineers and other things like that.
You have your product managers. When I worked in Silicon
Valley, the product managers were my favorite human beings
because they could take a technical concept and connect it to
reality in a way that a lot of people couldn't. And then you
have your commercial and marketing people. You have a variety
of different technologies showing up at the intersection of
energy manufacturing. And so I think this AI literacy is
actually relevant because I think all of us are going to be
part of this economy in some way or another. So the size of
this community is growing, and I think we just need to make
sure that they're trained well and they have that expertise to
be able to implement implementations that are respectful of the
values that we've been discussing.
Mr. Miller. Yes. Thank you for that answer. I'm a big
believer that technical education and CTE (career and technical
education) should be taught at a K through 12 level for every
American to go ahead for an alternative career pathway. And I
believe that the American dream has been distorted within this
reality for younger generations of what they think that that
is. It used to be, you know, a nice house, a white picket fence
and to provide for your family and a future generation. Now,
it's a little bit different and distorted in my view, so thank
you very much for that answer.
Another question, what role will community and technical
colleges play in AI workforce development in the short term,
you know, more recently than we think down the road? What about
your recommended 2-year programs are well-suited to recruit and
train students in AI-related fields?
Dr. Farshchi. I'm a product of the California community
college system, so I can comment on that. So I feel like the
emphasis in the community college system right now, at least
what I experienced back in the late 1990's, was two things, was
preparation for transfer into a four-year institution and
vocational skills to enter into an existing kind of technical
workforce. And I feel like there's an opportunity in the
community college system, which is excellent, by the way, which
is the reason why I'm sitting here today, to also focus on
preparing an existing workforce instead of just being
vocational into existing jobs, into jobs that are right over
the horizon.
So in the middle, in between the kind of get ready to
become a mechanic, to get ready to transfer to UC (University
of California) Berkeley, in between those two things, there are
jobs that are going to be evolving as a result of AI,
therefore, upskilling you as an existing worker to these new
jobs that will be emerging in the next couple of years. So I
think that in-between kind of service to the community through
the community college system I think would be extremely
valuable the community.
Dr. Murdick. Just to add to that, I think that the--I spent
after high school 12 years in school. That's a lot of time to
grind away at a particular technology and concept. As the world
accelerates, I don't think that that kind of investment is for
everyone and never really was for everyone, but I think it
needs to be more and more focused. And I think community
colleges provide the opportunity to pick up skills. Maybe you
graduated as an English major and now you're like, wait, I'd
really like to use data science and that kind of environment of
quick training, picking up the skills, stacking up
certifications so that you can actually use that is a perfect
venue for community colleges to be able to execute rapid
training and that adaption--adaptation that's necessary in a
very quick world working--wow, excuse me--a rapidly moving
economy.
Dr. Chowdhury. If I may add quickly, at the--at our
generative AI red teaming event, we're working with--funded by
the Knight Foundation, working with CDI (Center for Digital
Information) to bring hundreds of community college students
from around America to hack these models. So we--I absolutely
support your endeavors.
Mr. Miller. Nice. Well, thank you all very much. Thank you,
Mr. Chairman. And I have a few more questions I'm going to
enter into the record, but thank you very much. This is a joy.
I yield back.
Mr. Obernolte. The gentleman yields back.
We'll hear next from the gentleman from New York. Mr.
Bowman, you're recognized for five minutes.
Mr. Bowman. Thank you so much, Mr. Chairman. And thank you
to all the witnesses. Hey, y'all, I'm over here. Thank you to
all the witnesses for being here today and for sharing your
expertise and your testimony.
So America has a series of challenges that I would describe
as national security challenges. We have millions of children
going to bed hungry every single day. We have gun violence as
the No. 1 killer of children in our country. We have crippling
inequality and a racial and economic caste system that
persists. We have issues with climate change. We have issues of
children who go to school in historically marginalized and
neglected communities not receiving the same education as
children who go to schools in wealthier or more suburban
communities.
My question is--and I'll start with Dr. Murdick--can you
speak to the risks that we face with people intentionally and
unintentionally using AI to exacerbate these issues? What must
be done to ensure that these critical areas are improved?
Because, to my mind, if we're not using AI to better humanity,
what the heck are we using it for? It's not just about
commercialization. It's not just about military, but my--it's
about our collective humanity and solving the crippling
challenges that persist in our country before we even think
about China or another foreign adversary.
Dr. Murdick. I really appreciate your stepping back and
saying, you know, as--in governance, you have to look at the
full priorities of what we're trying to do, and just obsessing
about a little gnat of a technology is not the right way of
thinking. Looking at where it integrates with our entire
societal needs is really relevant. And I think there are a lot
of benefits to AI that can help with some of those issues.
However, your point about intentional misuse of technology to
tear apart our society, I think, both from internal as well as
external, there's the attack surface if you may is higher, and
being able to have tools that can make that--those attacks
easier to operate or more cost-effective is a very risk--big
risk.
So it's often referred to in the category of
disinformation, misinformation about how this is used, and it
can be done in different modalities. It can be done in text, it
can be done in images, it can be done in audio, and I think
being able to figure out how to manifest this is actually a
democratic process discussion, which is why I think that the AI
literacy needs to be brought up. We have to be engaging in
this. And because there's a lot of creativity needed, it's
really, really hard to detect fake text. It's hard to know what
was created by a system and what isn't. There's stories in the
news where people thought they had a method and they tried to
punish all their students because they thought they had figured
out a tool to be able to determine whether or not they can
detect it. It didn't work so well.
So the problem is it's going to be a--requires our full
stakeholder--of all the people in the United States to be
participatory, giving ideas, figuring out how to do this, also
that awareness of images. And I think image and audio really is
a problem because we're not--we're used to trusting everything
we hear and we see. Everything we read, maybe not so much. So I
think there's a situational awareness that we need to bring up
across our entire population to help.
I can go on, but hopefully, that's helpful.
Dr. Chowdhury. If I may?
Mr. Bowman. Please, yes.
Dr. Chowdhury. What you're bringing up is a concept of
algorithmic bias. And why this is critically important to talk
about today is to build on a point from earlier. This is where
regulation comes in. We already have laws today that prohibit
certain implementations and certain outcomes from happening,
independent of the technology or the solution being used. But
in addition, we're going to have to identify where the gaps
are, that existing regulation is insufficient, and create new
ones.
So a couple of reasons this happens, one is just a lack of
diversity in creating these technologies. These technologies
are gate-kept, not just by education, but literally
geographically gate-kept. People who sit in California try to
dictate to school districts in New York how their schools
should be run, and that's not how this should work. And second,
this techno solutionism, this idea that humanity is flawed and
technology will save us all. And that is also incorrect.
Humanity is something to be rejoiced in and something to be
nurtured and kept, you know--we need to make sure that people
are having the best kinds of lives that they can, mediated by
technology.
So algorithmic bias starts with datasets, it continues with
implementation, and it continues with the lack of ability to
understand and have model visibility. And it continues with a
lack of good governance.
Mr. Bowman. Thank you so much. I ran out of time. I have a
few more questions. Mr. Chair, can I submit the questions for
the record? Absolutely. I yield back. Thank you.
Mr. Obernolte. The gentleman yields back. We'll go next to
the gentleman from Texas. Mr. Babin, you're recognized for five
minutes.
Mr. Babin. Thank you, Mr. Chairman. And thank you,
witnesses, for being here as well.
Dr. Murdick, would you--this question is directed at you.
By many metrics, China has caught up or surpassed the United
States in research and commercial capabilities. As Chairman
Lucas referenced in his opening statements, Chinese
universities are publishing many more research papers than U.S.
institutions, and they receive nearly the same share of
citations as U.S. papers. How concerned are you with the pace
of AI progress in China, and what can we do about that?
Dr. Murdick. Wonderful question. And I--just a quick
comment on methods of how we determine who's leading. Research
is an easy place to start. It's an easy place to count. I
think, you know, I just want to say what is probably very
obvious. You know, patenting, investments, talent, numbers, job
postings, all these other datasets are really important to take
into account. Also, it's really important to mention scale. I
think in 2021, China's population was 4 1/4 times the U.S.
population. So therefore, just on sheer statistics, you're
going to see quantity numbers going to be outstripped by China.
So that's just some observations about the measures about how
concerned I am, which I think is a very interesting question.
My image of going forward--I've been following China
since--for over 15--well over 15 years, and it's been written
on the wall for a long time that they were going to be a pure
innovator. We've known it was coming. So the metaphor that I
come into is a kind of a grappling situation. China and the
United States and its allies will be grappling with its
different values and different approaches for the foreseeable
future. And when you're in a grappling, you don't freak out
when someone, you know, puts an arm on a shoulder. You know how
to respond, and it's a give-and-take. And I believe that that
metaphor kind of expresses how I see this going, so I'm not
going to freak out about them having a larger number. It's--
we're going to have to just figure out how to walk through
this, which is why I mentioned the policy monitoring concept is
whatever we do, we need to make sure it's working and be able
to adapt because they're going to counter whatever we do at any
given instance.
Mr. Babin. All right. OK. Thank you. The American research
enterprise currently operates in a transparent and open manner
with basic research being published without restrictions. How
can we maintain a balance between ensuring the openness of
basic AI research, while also mitigating potential threats to
national security that might arise from sharing such
information? And this is for Dr. Matheny and Dr. Murdick as
well. How can we solve this problem?
Dr. Matheny. Thanks so much. I think that for the largest
of so-called foundation models where we're just learning about
how they can be misused, how they can be used either in
developing cyber weapons, biological weapons, and massive
disinformation attacks, we should be really cautious. And so
before publishing or open sourcing the largest models, I
recommend that we require some amount of red teaming and
evaluation in order to understand the risk scenarios before we
unleash this thing into the world and we're unable to take it
back.
Mr. Babin. I got you. One quick thing, I believe that all
of us here today agree that it's important for the United
States to lead in technological advances, especially in the
cutting-edge field of AI. Unfortunately, I've seen instances
where overregulation hinders our ability to compete globally,
thereby forcing growth overseas or allowing others to step in
and lead. And I've seen--also seen that the private-public
partnerships and how successful they've been have put the
United States in the driver's seat and look at our space
industry as an example.
So, Dr. Murdick, how do we make sure that we do not
overregulate, but rather innovate through the facilitation of
bottom-up, consensus-based standards rather than top-down
regulations? And can you speak to the role NIST has played in
enabling innovation, particularly for AI, and how we can
leverage that approach going forward in the rest of the time
that we have?
Dr. Murdick. The distributed innovation system within the
United States is extremely important. I think Congress has
levers of--I think one of the core tenets of this hearing was
how do we figure out how to innovate going forward. And I think
that there--we need all areas to be innovated in. And, for
example, compute has helped move us forward. Data has helped
move us forward. Talent helps us move, and we need to make sure
whatever investment we do, it looks at like a balanced
portfolio of all those to be able to move forward.
In terms of like the space industry, I think it's really
exciting to see how we have led, but we have made policy
actions that have damaged parts of our industry, for example,
with the satellite industry, when we put in places--put in
rules that we could not deal with China. We actually--the size
of the U.S. market dropped, and Japan and European countries
developed ITAR- (International Traffic in Arms Regulations-)
free satellite technology that allowed them to increase their
market share.
So I think we have to be very--going back to that
wrestling--grappling concept, we have to be very cognizant of
everything we do will quickly be adapted and tried to be used
against us, and we have to be able to adjust our policies and
monitor them. So I think that monitoring action of like how
well is this working is actually a core part of anything that
Congress is going to do going forward.
Mr. Babin. Thank you.
Mr. Delangue. If I can add on the previous points about
kind of like ensuring that open source and open science is
safe, I think it's important to recognize that when we keep
things more secret, when we try to hurt open science, we
actually slow down more the United States than China, right? I
think that's what's been happening for the past few years. And
it's important to recognize that and make sure we keep our
ecosystem open to help the United States.
Mr. Babin. Merci. Thank you, yield back.
Mr. Obernolte. The gentleman yields back.
We'll hear next from my colleague from North Carolina, the
gentlewoman. Congresswoman Ross, you're recognized for 5
minutes.
Ms. Ross. Thank you very much, Mr. Chairman and Ranking
Member Lofgren, and to all of the witnesses for being here
today.
I will not go into intellectual property. I serve on the
same Committee with all of them.
As we all know, artificial intelligence permeates all areas
of business, industry, the arts, and influences decisions that
we make in our daily lives. In my district, North Carolina
State University launched the AI Academy in 2020 that will
prepare up to 5,000 highly qualified AI professionals from
across the Nation through their workforce development program.
The AI Academy is one of the Labor Department's current 28
public-private apprenticeship programs, and I look forward to
hearing from you about that. I know we've talked a little bit
about community colleges.
But my first question is about cybersecurity. I'm concerned
that we're not ready for AI-enabled attacks. I mean, we're not
even ready for the attacks that we have already. And one of the
best defenses against phishing emails is that they're often
poorly drafted with misspellings. But generative AI provides an
easy solution for malicious actors.
And so, Dr. Matheny and Dr. Murdick, and then anybody
else--but I do have another question, so be quick--are current
cybersecurity standards and guidelines equipped to be able to
handle AI-enabled cybersecurity and attacks, and what could the
Federal Government do?
Dr. Matheny. Thank you for the question. We're not equipped
yet. And I think that this has been, I know, a priority for
DHS/CISA (Department of Homeland Security/Cybersecurity and
Infrastructure Security Agency) to take this on. And there's a
lot of thoughtful work there looking at the impact of AI on
cybersecurity, including advances in spearfishing, which can be
made more cost-effective through the use of these language
models, but also through the use of these large language models
to generate not human language, but computer code. So these
cogeneration tools can be used to create offensive cyber
weapons. And it's possible that in the future those cyber
weapons could be quite capable and very cost-effective and
generated at scale, a scale that right now isn't possible even
for State programs, so I think that's quite worrisome.
AI can also, though, enable stronger cyber defenses. And so
figuring out how to invest in AI capabilities that will
ultimately create an asymmetric advantage for defense over
offense is an important research priority.
Ms. Ross. OK. And, Dr. Murdick, could you be very brief?
Because my next question is for you.
Dr. Murdick. I think your sense of this being an important
priority is actually right because I think AI plus
cybersecurity is probably one of the earliest spaces where
we're going to see AI manifesting. And I'll just agree with
Jason otherwise, good points. We need to work on this.
Ms. Ross. Great, thank you. And, Dr. Murdick, to you,
American leadership begins with a strong workforce, as we've
discussed, one that nurtures both domestic talent and attracts
the best global minds. Many international students come here to
conduct innovative research in emerging technologies, and a
report from your organization revealed that two-thirds of
graduate students in AI-related programs are international
students. And so although they want to stay here, our
immigration laws keep them from staying here. I've done a lot
of work with the so-called documented DREAMers who came here
with their parents, and then at 21 have to self-deport with all
of the investment that we have made in their education. So can
you discuss better ways to not just attract but retain this
talent, particularly in the AI space? And then, if there's
time, anybody else.
Dr. Murdick. So I will give one point. I think we've,
clearly through our own surveys, seen that people want to stay
in the United States. It's one of our strongest strengths. The
United States attracting high quality talent is the thing that
has driven a lot of our innovation, and being able to pull from
the world's best is really key. So we see this from China, we
see this from all countries, and I think we need to very much
invest in this and continue to invest in it.
I think a lot of the inhibitors are bureaucratic. And I do
think there's--for our high-skilled talent base, we've seen
China implement some really innovative--not China, Canada,
sorry, Canada----
Ms. Ross. Yes.
Dr. Murdick [continuing]. Invest in some really interesting
ways of when someone comes to the--to Canada and they meet
their--they give everybody in their family the opportunity to
start working the same day that the person who was approved.
That's pretty amazing, and it really can make a big decision
when you're trying to decide whether to move to Canada or the
United States or some other place. And so I think those kinds
of innovations are strongly within the--your hands, and I think
you can use them very effectively.
Ms. Ross. Thank you so much, and I yield back.
Mr. Obernolte. The gentlewoman yields back.
We'll go next to my colleague from Georgia. Mr. McCormick,
you're recognized for five minutes.
Mr. McCormick. Thank you, Mr. Chair, and thank you to the
witnesses for your thoughtful answers.
As Members of the House Committee on Science, Space, and
Technology, it is our responsibility to address the challenges
and opportunities presented by this rapidly advancing
technology. Artificial intelligence has the potential to
revolutionize our national security strategies and capabilities
by harnessing AI's power to analyze vast amounts of data. We
can enhance early threat detection, intelligent--intelligence
analytics, and decisionmaking processes. However, we must
proceed with caution, recognizing the ethical considerations,
algorithmic biases, and risks associated with AI deployment.
As policymakers, we must strike a delicate balance between
fostering innovation and protecting our national security
interests. This requires investing in research and development
to safeguard AI systems against adversarial attacks, ensuring
transparency and accountability, and collaborating with
international partners to address emerging challenges and
establishing norms. By doing so, we can harness and--the
transformative potential of AI to strengthen our defense
capabilities, while safeguarding our values and security.
In fact, that whole statement was produced by AI at
ChatGPT, which I thought was fun. And matter of fact, it
probably could have said it a lot better than me, too. That's
where we're at right now.
I think it's kind of funny. I'm about to replace my entire
legislative staff with ChatGPT and save a lot of money--sorry,
guys--since they came up with that. And I say that tongue-in-
cheek, but I thought you made a really, Dr. Chowdhury, I
thought you made a huge statement when you said we are looking
at AI like our savior and like it's going to replace us. And I
really thought it was great to say that you're not going to
save us, and also the government's not going to save us either,
by the way. I want to put that in. Maybe that's my little
political statement.
But it's interesting. It is time to replace some of us. I'm
an ER physician, and I'm watching radiologists being replaced.
There'll be in a supervisory capacity, and you're going to see
pathologists next. And eventually, we just had a recent survey
that says I'd rather interact with AI than a physician because
they give me easy-to-understand answers and they're nicer to
me. And they're not in a rush. So I think it's just a matter of
time, right? We're seeing that happen right now.
And in the defense industry, I'm worried because I've seen
those documentaries like Terminator and Star Trek. And I
understand that we have the potential for systems to actually
start outthinking us and actually out-reacting and has a
potential to damage us.
So I'm curious, what kind of guardrails should we put in
place to keep us not only employed but actually safe as far
as--Dr. Chowdhury, since you had the most insightful comments
so far, I'll let you start.
Dr. Chowdhury. I appreciate that. So I think you're talking
about two things here. One is job displacement, which has
happened with many technologies in the past. And I think some
of my colleagues on this panel have raised the need for
upskilling, retraining, easier access, lower barriers to entry,
investment in jobs programs. Frankly, these are almost
standard, but now we need to apply them to technology.
And the second part is really parsing out what we mean by
risks and harms. So some of the Terminator type narratives, we
are very, very far from these things. But there are harms that
we are seeing today, people being denied medical care or
insurance because of an incorrectly specified model. We have
examples of people of color being denied kidney transplants
because an algorithm is incorrectly determining whether or not
they should be on a kidney transplant list. These are harms
that are happening today. We can't even figure out how to fix
these now. We don't have enough to fix these now. We are so far
away from a Terminator, and really what we should be focusing
on are the harms in technology we're building today.
Mr. McCormick. Great, thanks. One thing--I'm not even sure
who to ask this for, but I'll let the panel decide who's the
best person. I have a real sincere concern that 100% of our AI
chips right now are produced in Taiwan, with the posturing we
have in China talking about they are going to take over Taiwan.
Having an adversarial country owning 100% of the production of
the most influential technology in world history deeply
concerns me. Now, I know AMD has some processes that they want
to produce AI outside of Taiwan in the next couple years, but
in the meantime, what do we do? I feel like all of our eggs are
in one basket. Please.
Dr. Matheny. Yes, RAND has done a lot of work on this topic
because it's been quite concerning about what the economic
impacts would be and the national security impacts if a Taiwan
invasion occurred and disrupted the microelectronics supply
chain, given that we're dependent for 90% of our advanced
microelectronics, the most leading-edge chips coming from
Taiwan. And it would be an economic catastrophe. So among the
policy options that we have to deal with that is to deter an
invasion of Taiwan by ensuring that Taiwan has the self-
defenses needed for a so-called porcupine defense.
Mr. McCormick. Great tie-in to my ask. Thank you very much.
And with that, I yield.
Chairman Lucas. The gentleman yields back.
The Chair recognizes the gentleman from Illinois, Mr.
Sorensen, for five minutes.
Mr. Sorensen. I'd like to begin by thanking Chairman Lucas
and Ranking Member Lofgren for convening this hearing and for
our witnesses today.
Building AI systems responsibly is vital. I believe
Congress is instrumental in providing the guardrails with which
AI can be implemented. However, there's much that the private
sector must do as well. Dr. Chowdhury, how should we audit AI
systems to ensure that they do not provide unintended output?
How do we ensure that companies that create the algorithms are
using accurate datasets when training the system?
And also, I've met with Amazon, Microsoft, and Google and
each have different stances on the need for guardrails within
their companies. One representative of one of these companies
says it's actually Congress' job. So do we need a system like
the European Union's AI Act, which includes the concept of
triangles of risk? And how can Congress learn from the
Europeans concept?
Dr. Chowdhury. You are speaking to my heart. This is what
I've been spending the past seven years of my life doing.
Interesting to note that I'm not a computer scientist by
background. I'm a social scientist, so I fundamentally think
about impact on people in society.
I would actually direct you to think about the way the
Digital Services Act is constructed where they've actually
defined five areas. These are including things like impact on
elections and democracy, impact on things like mental health,
you know, and more like socially developed issues and
developing audits around them for companies that have the at-
scale level of impact. Also, I am an audit consultant for the
Digital Services Act helping them construct these
methodologies.
I will add that it is not easy. And not only for
traditional machine learning and AI models but in generative
AI, now we have a whole other ballgame. So really, what the
investment is needed here is in this workforce development of
critical thinkers and algorithmic auditors.
Mr. Sorensen. Great. Follow up question, you know, when we
go to a search engine or we have a video conference, do you
believe that there should be safeguards so that consumers
understand if the data that they're receiving is organic and
believable and, most important, trustworthy?
Dr. Chowdhury. Yes.
Mr. Sorensen. Thank you. Dr. Farshchi, first of all, I'd
like to say that I was a little nervous when I first had my
electric car, my Chevy Volt, drive itself down the road, when
it came up to that first curve and it did it itself, all right?
But I had a steering wheel to be able to control it to take
over.
Nine days ago, I met with local union leaders from our bus
systems in Bloomington and Rockford, Illinois, and they had
concerns about autonomous bus systems. What does it mean for
the safety of those that are on their buses or the safety of
those folks that are standing on a sidewalk? AVs are
complicated technology with extensive programming. How do we
ensure safety if we're going to put children on school buses?
How do we protect our autonomous vehicles from those that might
want to hack them and cause problems?
Dr. Farshchi. So the technology is still very early, but
there are lessons that we've learned in the past that have
helped create a certain level of safety for complicated
machines and technologies. And I think the best lesson there is
in aviation. So in aviation, there were certain guidelines that
had to be met for an airplane to be able to fly. And now,
aviation has become one of the most safe forms of
transportation.
I feel like we're still kind of in the, you know, early
part of the 20th century on the AV side. We still don't know
exactly what the failure modes of these machines are. There is
still rapid iteration going on. They're perhaps 98% safe, but
they have to be 99.999% safe. And then once the technologies
are matured, then, when we identify what the weak points of
these technologies are, we will come up with a framework to
audit the technologies to make sure that, OK, if we--if a
vehicle passes these tests, just like what we have, for
example, with crashworthiness, if a vehicle passes these tests,
we have this level of certainty that this vehicle will be safe
in most circumstances.
And I think that safety should be higher than that of a
human driver. So if a human driver--because humans make errors.
If a human driver makes an error that leads to an accident
every 100,000 miles, then these vehicles have to be audited to
be safe for at least 100,000 miles. And once they pass that
threshold, then we should consider them for use. I think we're
still two steps away from being able to identify that
regulatory framework to be able to audit these machines to be
sure that they are safer than human drivers.
Mr. Sorensen. Thank you very much for your testimony today,
and I yield back the balance of my time.
Chairman Lucas. The Chair recognizes the gentlelady from
North Carolina, Mrs. Foushee, for five minutes.
Mrs. Foushee. Thank you, Mr. Chairman. And thank you all
for your testimonies here today.
My first question is to Dr. Chowdhury. And I think that I
was really struck while reading your testimony by a particular
line where you say, ``It is important to dispel the myth that
governance stifles innovation,'' that--during your career that
you have found--and you talked about this a little earlier,
governance is innovation. Can you please elaborate on that
notion that governance is innovation in the context of AI, and
how can the Federal Government and certainly Congress fulfill
our oversight duties to get this right?
Dr. Chowdhury. Thank you. And I would love to elaborate on
this. And here's where we make a distinction between what is
research and what ends up being applied. In my time helping
Fortune 500 companies implement AI systems, 99% of the time,
the reason they did not implement it is because they could not
reliably trust or predict what the outcome was. They did not
know if they were operating within the appropriate safeguards,
if they would be breaking the law, what it even meant to
possibly break the law, so they just decided to not do it. So
by creating the appropriate set of standards, guidelines,
laws--and this is all within the remit of Congress--you
actually help innovation by showing companies where the safe
area to innovate and play is.
Mrs. Foushee. Would anyone else like to speak to this?
OK. So also, with the rise of artificial intelligence,
researchers are increasingly in need of computing and data
resources at scale. Unfortunately, there is a general lack of
access to AI resources outside of the large tech companies.
This has resulted in a steep resource divided between the top
tech companies and smaller AI firms. Dr. Farshchi, and perhaps
Mr. Delangue, can you speak to the needs of enabling innovation
for smaller companies to access computer--computing, rather,
and data resources?
Mr. Delangue. Yes, I can start there. I think it's
important to realize that it's not just compute. I think
there's been a very interesting study by the Center for
Security and Emerging Technology saying that, you know, for
researchers to thrive and for companies to thrive, they need
not only compute, but good data and system access for AI. So
when we look at kind of like providing the resources for all
companies to thrive with AI, I think we need to be kind of like
looking at all of that, compute, people, and system access and
provide more transparency.
What we've seen on the platform and the HuggingFace
platform is that when you give these tools to companies, they
thrive and they manage to use AI responsibly, right? We've seen
everywhere from kind of like marble cutters, small businesses
in the United States using AI to detect material, printshop
using image generation to generate image and generate kind of
like T-shirts or phone covers using AI. So I think by like
enabling and giving access to these resources more broadly, we
can enable all companies to make use of AI.
Dr. Farshchi. Congresswoman, just to add, I think the
Federal Government has a role to play here. There's a bit of
ambiguity right now going back to the conversation earlier
about licensing versus registration. It's still the wild west.
Companies don't know--they need to train their models. They
don't know exactly if they would be doing something illegal or
doing something unethical by using certain datasets. If the
government were to make data available and instruct companies
and researchers to use that data source and remove this
ambiguity, I think that would be--to Clement's point, I think
that will be a huge step forward for these researchers.
Mrs. Foushee. Thank you, Mr. Chairman. I yield back.
Chairman Lucas. The gentlelady yields back.
The Chair now recognizes the gentlelady from Colorado, Ms.
Caraveo, for five minutes.
Ms. Caraveo. Thank you, Chairman Lucas, and to you and
Ranking Member Lofgren for this very exciting hearing on AI,
which I think we've all been following closely. And as a doctor
as well, as Dr. McCormick mentioned earlier, I'm following
advancements that are happening in the healthcare sector. New
forms of biomedical technology and data are transforming the
way that we research, diagnose, and treat health issues. And
there was a lot of enthusiasm using AI to combine different
forms of data such as those from genomics, healthcare records,
medical imaging to provide clinicians with critical insights to
make clinical decisions for individual patients.
So, Dr. Matheny, can you describe what you think the
technological benchmarks are needed to realize benefits,
especially in the medical field in the next two, five, 10 years
and beyond?
Dr. Matheny. Thank you for the question. I think
applications to medical diagnostics, to personalized medicine,
to home healthcare are profound. And I think that establishing
the sort of evaluative framework that we're going to need in
order to assess the added value of these technologies over
current practice is one that's really important for the FDA
(Food and Drug Administration), for Medicare, for Medicaid, to
be able to evaluate what advantages these technologies bring.
I really liked the expression that with brakes, we can
drive faster, and this is the history of technology innovation
in the United States is that we have had government testing and
evaluation as a propellant for innovation because when
consumers can trust that technologies are safe, they use them
more. And that allowed the United States to lead in
pharmaceuticals and in other health technologies because of
that framework.
Ms. Caraveo. I'm going to kind of expand on your notion of
trust. Dr. McCormick also mentioned an article in The New York
Times talking about how doctors are using ChatGPT to
communicate with their patients and looking for a more
empathetic way of responding to them. As somebody that trained
in medicine, that at first made me chuckle, then it made me
worried a little bit. And then as I read the article, I must
admit that the answers that it came up with were actually quite
good and in keeping with some of the training around
communication that we use. But I think if I was a patient and
realized that my doctor was not answering something themselves
but using a separate technology to create more empathy and
communication between us, I'd also be a little bit concerned.
So looking at that, and then also realizing in the article
that--in my thoughts, something like AI would be very good at
the radiology aspect, right, making sure that it was reading
mammograms faster, for example. But it also pointed out that it
was leading to a lot of false positives that were leading to
unnecessary tests. So how in the future, as patients, as
providers, can we ensure confidence in the different kinds of
applications that AI and these technologies are going to be
used for in medicine? And that's really for anybody, starting
with Dr. Matheny.
Dr. Matheny. I do think that more testing in which the
comparison arm is current practice so that we can evaluate does
it have the same precision recall as existing, for example,
diagnostic practices, is going to be essential. I think society
as a whole is going to be working to adapt on what we sort of
recognize as being sincere communication when more and more
communication will be generated by language models.
But I do think that one benefit of some of these models is
that they don't get tired, they don't get stressed. So if
you're a care provider and you're working under sleep
deprivation and time pressure, you might not be giving as much
explanation as is really needed for a patient, so there could
be real benefits there.
Dr. Murdick. Just to add to that, I think this concept of
human machine teaming and trust is really important here. I had
the fortune of working with the armed forces for the first part
of my career, and I watched the training culture that they had.
They knew--they trained well with their teammates, and they
knew what their colleague was going to do and not do because
they had spent that time training. And I think there's a lot of
interesting measures and metrics for designing systems that
maximize that trust. So imagine a doctor or actually part of
their training, learning how to work with these systems,
spending those hours so that they know when they can rely on
it, when they can't rely on it. I just think it's an important
framing for the future of AI to make sure that that's part of
the team and is calibrated correctly, that trust, and we can
use it appropriately.
Ms. Caraveo. I really appreciate your answers. In
particular, I think, Dr. Matheny, I thought about burnout, as
well as we've come particular out of a pandemic where we're
going to be facing more physician shortage. What are the
applications to read over patient charts, to compile
information, to write long notes that doctors don't necessarily
need to spend hours on, and so very, very much appreciate those
comments.
Chairman Lucas. The Member's time has expired.
The Chair now recognizes the gentlelady from Pennsylvania,
Ms. Lee, for five minutes.
Ms. Lee. Thank you, Mr. Chairman. And thank you to our
witnesses for your time and expertise on this critical area of
technological innovation.
Advancing innovation in western Pennsylvania have become
synonymous when we view the technological developments we're
providing to the Nation. For example, Pittsburgh is America's
hub for autonomous vehicles research and development, evidenced
by the numerous self-driving vehicles you'll find on our
streets. Carnegie Mellon University in my district brought home
$20 million from NSF to develop AI tools specifically to
address the design and utilization of ethical humancentric AI
tools that will help improve disaster response and aid public
health.
As an environmental justice advocate, I've seen how ethical
AI has been used to monitor, predict, and combat environmental
conditions that are ravaged by corporate polluters. AI can mean
new possibilities for clean air and water and livable
communities. We know that smart city initiatives, innovations
that leverage AI and data analytics, will help to improve the
quality of life for citizens and enhance sustainability.
However, despite the numerous possibilities for AI
integration into every facet of the American economy and
everyday life, there also exists serious concerns that I don't
take lightly. As some of my colleagues have mentioned this
morning, we're really discussing privacy rights, the ethical
implications of AI technology, the continuing war against
disinformation and misinformation. As a proud Representative
from western Pennsylvania, I'd be remiss to not discuss the
implications AI will have on our labor and our workforce. AI is
exciting, sure, but we must exercise caution to ensure that we
provide access and opportunities for skills training and
education to every single worker so they're not left behind
throughout and by this revolution.
Last week, I introduced an amendment that encapsulates my
legitimate concerns on the disparate impacts these technologies
have on people that look like me. I would like to commend
Chairman Lucas and this Committee for their express commitment
to ensure that the advancement of AI technology in our society
does not result in Black folks and otherwise marginalized
communities being used as sacrificial lambs. We can all agree,
striking a balance between harnessing the benefits of AI and
addressing its challenges is crucial to ensuring AI truly has a
positive impact. Striking that balance begins here, of course.
So, Mr. Delangue, what are the untapped potentials of AI
that could substantially improve combating environmental
injustices and daily living standards of ordinary citizens?
Mr. Delangue. So first, I really appreciate your points. I
think one of the important things that we need to realize today
is that AI needs to be more inclusive to everyone, more
transparent. Something that we've seen, for example, is that by
sharing models, datasets, you actually allow everyone to be
able to discuss them, contribute to them, report biases that
maybe the initial model builders wouldn't see. So I think it's
really, really important to invest a lot in more equity, more
inclusiveness for AI.
To your second question on the environment, I've been
really interested in all the research that has been done around
carbon emission from AI because that's a very important problem
that we'll have to deal with in the future. So, for example,
there's this model called Bloom that has been developed by big
science that HuggingFace participated in that published
actually the carbon emissions generated by the training of the
model. And that's something that also I think we should
incentivize more in order to see the potential impact on carbon
emissions of AI.
Ms. Lee. Thank you. And I agree with your first point,
obviously.
Just last year, Shudu, a Black AI-generated model was
featured in campaigns for Balmain, for Elise, and even Vogue.
The creator said the model was inspired by human models like
Naomi Campbell and Alex Weck. We know the fashion industry has
long been discriminatory toward Black women and other women of
color.
My question, Dr. Chowdhury, what needs to be done to ensure
that AI technologies are not taking away work from Black folks
in industries that are already White-dominated?
Dr. Chowdhury. I think investing in retraining programs and
compensation programs for individuals whose jobs will be taken
away is key and critical here. We can't just leave people at
the whims of massive companies that don't care or don't think
or don't even know anything about them.
Ms. Lee. Do you have an opinion or an idea of how we can
create safeguards to establish the rights of individuals over
the use of their image, their voice, and their likeness in AI
technologies?
Dr. Chowdhury. I think this is something that Congress
should take very seriously and think through. But I do--for
example, myself, I am concerned about my image being on the
internet and being part of training data or, you know, an image
being generated that looks something like me but maybe isn't
me, or, more seriously, the ability to create deepfakes that
look like people, so we need these protections for all
individuals. And in particular, I want to say that women and
people of color are the primary targets of deepfake-generated
photos.
Ms. Lee. Thank you. I could go on and on, but that is my
time, so I yield back, Mr. Chairman.
Chairman Lucas. The gentlelady has expired--time has
expired.
The gentleman from California, Mr. Lieu, is recognized for
five minutes.
Mr. Lieu. Thank you, Chairman Lucas. Thank you for holding
this important hearing, and it's been very informative.
Generalized large language models are incredibly expensive
to create and to operate. There was an article earlier this
month in the Washington Post. The title of it was ``AI Chatbots
Lose Money Every Time You Use Them.'' And it goes on to say the
cost of operating the systems is so high that companies aren't
deploying their best versions to the public. Estimates are that
OpenAI lost over $500 million last year.
So my first question is to Dr. Farshchi. Do you think
OpenAI can be profitable?
Dr. Farshchi. So I am not close to OpenAI, so I don't know
what their commercialization plans are.
To the comments that were made earlier regarding the
productivity of AI, I believe AI can eventually become
productive and enhance human output and create a net positive
economic impact on society. But then as it relates to OpenAI--
--
Mr. Lieu. I'm not asking that.
Dr. Farshchi. I don't have an answer for you,
unfortunately.
Mr. Lieu. In any of these large language models, could they
actually commercially be profitable, given how much it costs to
develop them and how much it costs to operate them on a daily
basis?
Mr. Delangue. So I think it's important to realize that
large language models are just like a small portion of what AI
is. What we're seeing as--in terms of like usage from, you
know, companies is more the use of smaller, more specialized,
customized models that are both kind of like cheaper to use and
more environmentally friendly. And that's what I see the future
of AI is, more than one large language model to rule them all.
An example of that is when you build a customer chatbot for a
bank, you don't need the model to tell you about the meaning of
life, so you can use a smaller, more specialized model that is
going to be able to answer your question.
Mr. Lieu. Thank you. So my next question is for you. I'm a
recovering computer science major. I, on balance, generally
support open source. Some of your testimony was about having
America continue to be a leader in this area. I'm just curious
how America is a leader if it's all open source where our peer
competitors simply copy it.
Mr. Delangue. So you look at the past, I think the reason
why America is the leader today is because of open science and
open source. If there wasn't open science and open source, the
United States probably wouldn't be the leader. And I think
actually in the past few years the fact that it got less open
is leading to, you know, the leadership of the United States
diminishing.
Mr. Lieu. But we don't quite have open science, right? We
have an entire patent system where you can't copy that science.
Mr. Delangue. I would argue that most of today's progress
is powered by the open research papers that have been published
in the past that I mentioned like the Attention Is All You Need
paper. And actually most of the commercial companies today
exploiting AI are actually based on these papers, right?
Mr. Lieu. So--right----
Mr. Delangue. The ChatGPT, the T is transformers. It's the
famous----
Mr. Lieu. Right.
Mr. Delangue [continuing]. Open----
Mr. Lieu. No, I got that. I'm fine with research and
development and so on being open.
So I'll just give you a story. Last year, I talked to a CEO
in my district who was creating a new chip that's faster. He's
an immigrant. And at some point, I said, how fast is this chip?
And he says 50,000 times faster. I was like, whoa. And it
occurred to me that he never would have gone to Moscow, right?
No one wakes up and says I want to go to Moscow. No one really
wakes up and says I want to go to Beijing, where if you say
something bad about President Xi Jinping or you get too
powerful, they kidnap you and reeducate you. But they'll come
to United States, not just because we have talent and
resources, but because we have a whole vast set of intellectual
property laws that we enforce, and we have the rule of law, and
we don't let people copy this.
So I'm just so interested in how you can have America be
the leader and then have--I mean, if we just sort of say to all
these AI companies, just make everything open source, I don't
even know how they monetize that. And if people can just copy
it--it's just--it is sort of interesting to me, so I would like
to hear more about that later. I do want to move to another
topic.
And this is to Jason. You mentioned in one of your
recommendations, additional funding for NIST to make sure they
have the capacity to do their AI risk framework. So I assume
you think it is a good framework? I think it's a good
framework. I looked through it and thought about it quite a
bit. It's pretty generalized, so you don't--it's not that
prescriptive, so any company could actually adopt it. What is
your view of just forcing companies to think about AI by, let's
say, anytime the Federal Government contracts with someone, we
require them to go through the NIST AI framework?
Dr. Matheny. Thank you. I do think that we need an approach
in which the safety and reliability of systems is assessed
before they're broadly deployed. And right now, we don't have
such a system. And there are a few ways for the Federal
Government to help. One is not only requiring that for direct
Federal contractors but also making it a term and condition
that any compute provider that has a Federal contract would be
required to place those conditions on any company that's using
its computing infrastructure. So sort of similar to the Common
Rule in which a Federal contract with a research organization
requires them that any organization that they're working with,
even if it's not on a Federal contract, follows the Common
Rule, say, for human subjects research. We could require the
same for safety testing for AI.
Mr. Lieu. Thank you. I yield back.
Chairman Lucas. The gentleman yields back.
Seeing no other Members present, I want to thank the
witnesses for your valuable and in some ways exceptional
testimony and the Members, of course, for their questions.
The record will remain open for 10 days for additional
comments and written questions from Members. This hearing is
adjourned.
[Whereupon, at 12:08 p.m., the Committee was adjourned.]

Appendix

----------

Answers to Post-Hearing Questions
[GRAPHICS NOT AVAILABLE IN TIFF FORMAT]

[all]
