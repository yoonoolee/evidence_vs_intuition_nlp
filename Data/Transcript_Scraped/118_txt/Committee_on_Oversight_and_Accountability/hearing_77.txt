- TOWARD AN AI-READY WORKFORCE

[House Hearing, 118 Congress]
[From the U.S. Government Publishing Office]

TOWARD AN AI-READY WORKFORCE

=======================================================================

HEARING

before the

SUBCOMMITTEE ON CYBERSECURITY, INFORMATION
TECHNOLOGY, AND GOVERNMENT INNOVATION

of the

COMMITTEE ON OVERSIGHT
AND ACCOUNTABILITY

U.S. HOUSE OF REPRESENTATIVES

ONE HUNDRED EIGHTEENTH CONGRESS

SECOND SESSION

__________

JANUARY 17, 2024

__________

Serial No. 118-85

__________

Printed for the use of the Committee on Oversight and Accountability

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

Available on: govinfo.gov
oversight.house.gov or
docs.house.gov

_______

U.S. GOVERNMENT PUBLISHING OFFICE

54-570 PDF                   WASHINGTON : 2024

COMMITTEE ON OVERSIGHT AND ACCOUNTABILITY

JAMES COMER, Kentucky, Chairman

Jim Jordan, Ohio                     Jamie Raskin, Maryland, Ranking
Mike Turner, Ohio                        Minority Member
Paul Gosar, Arizona                  Eleanor Holmes Norton, District of
Virginia Foxx, North Carolina            Columbia
Glenn Grothman, Wisconsin            Stephen F. Lynch, Massachusetts
Michael Cloud, Texas                 Gerald E. Connolly, Virginia
Gary Palmer, Alabama                 Raja Krishnamoorthi, Illinois
Clay Higgins, Louisiana              Ro Khanna, California
Pete Sessions, Texas                 Kweisi Mfume, Maryland
Andy Biggs, Arizona                  Alexandria Ocasio-Cortez, New York
Nancy Mace, South Carolina           Katie Porter, California
Jake LaTurner, Kansas                Cori Bush, Missouri
Pat Fallon, Texas                    Jimmy Gomez, California
Byron Donalds, Florida               Shontel Brown, Ohio
Scott Perry, Pennsylvania            Melanie Stansbury, New Mexico
William Timmons, South Carolina      Robert Garcia, California
Tim Burchett, Tennessee              Maxwell Frost, Florida
Marjorie Taylor Greene, Georgia      Summer Lee, Pennsylvania
Lisa McClain, Michigan               Greg Casar, Texas
Lauren Boebert, Colorado             Jasmine Crockett, Texas
Russell Fry, South Carolina          Dan Goldman, New York
Anna Paulina Luna, Florida           Jared Moskowitz, Florida
Nick Langworthy, New York            Rashida Tlaib, Michigan
Eric Burlison, Missouri
Mike Waltz, Florida

------
Mark Marin, Staff Director
Jessica Donlon, Deputy Staff Director and General Counsel
Raj Bharwani, Senior Professional Staff Member
Lauren Lombardo, Senior Policy Analyst
Peter Warren, Senior Advisor
Mallory Cogar, Deputy Director of Operations and Chief Clerk

Contact Number: 202-225-5074

Julie Tagen, Minority Staff Director

Contact Number: 202-225-5051
------

Subcommittee on Cybersecurity, Information Technology, and Government
Innovation

Nancy Mace, South Carolina, Chairwoman
William Timmons, South Carolina      Gerald E. Connolly, Virginia
Tim Burchett, Tennessee                  Ranking Minority Member
Marjorie Taylor Greene, Georgia      Ro Khanna, California
Anna Paulina Luna, Florida           Stephen F. Lynch, Massachusetts
Nick Langworthy, New York            Kweisi Mfume, Maryland
Eric Burlison, Missouri              Jimmy Gomez, California
Vacancy                              Jared Moskowitz, Florida
Vacancy                              Vacancy

C  O  N  T  E  N  T  S

----------
Page

Hearing held on January 17, 2024.................................     1

Witnesses

----------

Dr. William Scherlis, Professor of Computer Science, Carnegie
Mellon University
Oral Statement...................................................
Ms. Timi Hadra, Client Partner and Senior Executive for West
Virginia, IBM
Oral Statement...................................................
Dr. Costis Toregas (Minority Witness), Director, Cyber Security
and Privacy Research Institute, The George Washington
University
Oral Statement...................................................

Written opening statements and statements for the witnesses are
available on the U.S. House of Representatives Document
Repository at: docs.house.gov.

Index of Documents

----------

* Levin House Testimony; submitted by Rep. Mace.

* Statement for the Record, Center for AI and Digital Policy;
submitted by Rep. Connolly.

* Statement for the Record, Partnership for Public Service;
submitted by Rep. Connolly.

* Statement for the Record, Johns Hopkins University; submitted
by Rep. Lynch.

Documents are available at: docs.house.gov.

TOWARD AN AI-READY WORKFORCE

----------

Wednesday, January 17, 2024

U.S. House of Representatives

Committee on Oversight and Accountability

Subcommittee on Cybersecurity, Information Technology, and Government
Innovation

Washington, D.C.

The Subcommittee met, pursuant to notice, at 2:16 p.m., in
room 2154, Rayburn House Office Building, Hon. Nancy Mace
[Chairwoman of the Subcommittee] presiding.
Present: Representatives Mace, Timmons, Langworthy,
Burlison, Connolly, Khanna, and Lynch.
Also present: Representatives Krishnamoorthi and Beyer.
Ms. Mace. Good afternoon. The Subcommittee on
Cybersecurity, Information Technology, and Government
Innovation will now come to order. Good afternoon and welcome,
everyone.
Without objection, the Chair may declare a recess at any
time.
And I will now recognize myself for the purpose of making
an opening statement.
Good afternoon, and welcome to this hearing of the
Subcommittee on Cybersecurity, Information Technology, and
Government Innovation. Today, America is indisputably the
global leader in artificial intelligence, which is why you all
are here today. This Nation has led the way on AI. During its
liftoff stage, American companies and institutions have
developed the most sophisticated AI models, like ChatGPT we
hear so much about. American companies, universities, and
research institutes are producing the bulk of the cutting-edge
research that is pushing forward the frontiers of knowledge in
the field.
Going forward, maintaining America's edge in AI will be key
to our continued national security and economic prosperity, but
we cannot take that lead for granted. We are now entering a
stage of widespread AI adoption. According to business leaders,
AI technologies will be integrated broadly into the economy,
both here and abroad. A large global survey of employers said
they are highly likely to adopt AI over the next 5 years. They
expect AI to create a lot of job churn but to ultimately lead
to a 25 percent net increase in jobs. That is why they also
identified AI and Big Data as among their top priorities for
workforce upskilling. That means, going forward, keeping our
global edge in AI will increasingly depend on the global
competitiveness of a broader American AI workforce.
We will still need to have the best computer scientists and
software engineers. That report found AI and machine learning
experts are projected to be among the fastest-growing jobs in
the country. But we will also need to fill a much broader pool
of AI-related work roles requiring various skill sets. There
will be opportunities for both new workforce entrants and
employees looking to retool and upskill.
But this transition will also test our training pipeline,
and America's pipeline in the STEM fields is a concern. Take
cybersecurity, which is also critical to our national and
economic security. We have a shortage of 700,000 cybersecurity
workers across the private and public sector, and why is that?
We know that our traditional education system does not produce
nearly enough degreed grads in the field to fill the need. We
also know that that shortfall would be much worse if not for
the appearance of nimble educational alternatives that include
short-term boot camp programs that issue non-degree
credentials, like certifications and badges.
Our witnesses today will testify about how these sorts of
flexible, targeted programs that reduce borrowing needs are now
being used to train young people for AI-related roles and to
upskill older workers. We know that China is making a
multipronged push to lead in AI talent. They have been trying,
largely without success, to lure back home Chinese nationals
graduating from top U.S. computer science programs, but they
domestically produce many more STEM grads than we do, and the
Chinese Ministry of Education has approved in recent years
hundreds of new university AI programs, according to Georgetown
University's Center for Security and Emerging Tech, which also
found that AI is the most popular new major in China.
Before I yield, I want to speak to the Federal workforce.
It is critical the Federal Government has an appropriate AI
workforce. We have a bill on the Committee reported last year
that requires Federal managers to be trained on AI so the
government can deploy it wisely. We had the Defense
Department's chief AI officer testify before this Subcommittee,
and it is clear DOD is at least making progress in this space,
but the Office of Personnel Management is another story. It was
tasked 3 years ago by Congress with identifying AI talent gaps
across the Federal civilian AI workforce and with creating a
new AI job series for Federal workers. It has done neither of
these things, so we are still waiting.
And then with that, are we going to waive on two Members?
I am going to ask unanimous consent for Representative
Krishnamoorthi from Illinois and Representative Beyer from
Virginia to be waived onto the Subcommittee for today's hearing
for the purposes of asking questions.
And without objection, so ordered.
And with that, I will yield to the Ranking Member for his
introductory remarks.
Mr. Connolly. I thank the Chair. A 2020 World Economic
Forum study found that AI-generated machines could disrupt an
estimated 85 million jobs globally by next year. Though that
sounds scary, and is, the study also suggested that AI adoption
could, on the other hand, create as many as 97 million new
jobs. AI has to be a tool used to enhance the job, not replace
the worker. If done correctly, we can create a new job sector
that equitably spreads the benefits of AI to all parts of
society while remaining a global technological leader.
One requirement necessary for the U.S. to remain a global
leader in AI is to build and invest in a robust workforce and
talent pipeline that draws from every corner of the American
educational system. We must prepare future technologists from
the moment they enter elementary school and attract talent from
all places, including community colleges, 4-year colleges, and
trade schools. That is why I co-led the Chance to Compete Act
with Virginia Foxx of North Carolina, which would allow
agencies to hire based on one's ability to do the job. We
believe the Federal Government should reward those based on
merit rather than affiliation. Giving people the opportunity to
retrain and reskill into new fields and professions based on
achievement and ability will help unlock massive amounts of
unrealized talent.
To remain competitive with the private sector, the Federal
Government must nurture its own talent pipeline. One way we can
do that is to provide students with the opportunity to
participate in meaningful paid internships. Much of the private
sector, including large firms like IBM, Microsoft, Google,
Nvidia, are already excelling in this process. The Federal
Government must model internship programs and find ways to get
great talent into agencies, whether it is for a quick stint or
a lifelong career. Even a short time in government can be
valuable and provides an opportunity to share knowledge between
both the public and private sectors.
The legislation I have introduced, the Building the Next
Generation of Federal Employees Act, would do just that by
increasing the availability and quality of paid internships
across the entire enterprise of the Federal Government. I hope
this is a source of bipartisan interest on the Committee as we
work to reintroduce in the coming weeks. We also cannot forget
about our current Federal employees and must provide them with
technical and conceptual AI training resources. The President's
executive order, which we had a hearing about a few weeks ago,
makes it clear knowing how to use technology can be great, but
knowing how to use technology responsibly is paramount.
I have co-led the AI Training Expansion Act with Chairwoman
Mace, which would expand the access and curriculum of these
educational programs to employees up and down the
organizational chart. We must make this training more than just
an AI awareness exercise and ensure that such training enables
employees to harness the power of AI in order to do their jobs
smarter, faster, and to greater effect.
Another way that government could benefit from AI is
through joint ventures between the public and private sectors
that drive toward collaborative solutions. Many have likened
the AI revolution to the next space race, which is why Congress
created the National Artificial Intelligence Resource Task
Force in 2020 to explore ways to effectively foster AI research
and application. Among the findings of the task force's January
2023 final report was the need to increase the diversity of
talent in AI by ``supporting the needs of researchers and
students from diverse backgrounds who are pursuing
foundational, use-inspired, and translational AI research. The
report recommended that we look for this talent in academic
institutions, nonprofits, startups, and small businesses.
President Biden's Fiscal Year 2023 budget requested $1.8
billion in non-defense R&D related to AI, including successful
public/private partnerships. The budget also sought funding for
critical resources for NIST, the National AI Research
Institutes, and Federal agencies as they implement the
Administration's recent groundbreaking AI directives. Those
bodies are responsible for developing guidelines for evaluating
and red teaming, promoting ethical and trustworthy systems and
technologies, and contributing to innovative solutions. We must
invest in educational resources and teachers who can help
students prepare for the future and help employers identify
those who will lead the AI workforce and AI innovation. We need
a workforce that will use AI ethically and equitably, ensuring
AI is used to benefit American families, communities, and
businesses across the country.
I look forward to a productive discussion today with our
witnesses, specifically, how we can better prepare, not
replace, our current workforce for the possibilities of AI.
Thank you.
Ms. Mace. Thank you. I am pleased to now introduce our
witnesses for today's hearing. Our first witness is Dr. William
Scherlis, Professor of Computer Science at Carnegie Mellon
University. Our second witness is Ms. Timi Hadra, Client
Partner, and Senior Executive for West Virginia at IBM, and our
third witness is Dr. Costis Toregas, Director of the
Cybersecurity and Privacy Research Institute at the George
Washington University. We were also going to be joined today by
Dr. Richard Levin, a former president of Yale University and
Senior Advisor at Coursera. Unfortunately, Dr. Levin fell ill
over the weekend and could not be here in person today. We want
to wish him well and hope for a speedy recovery. In lieu of his
attendance, I ask unanimous consent to enter his testimony into
the record.
So, without objection, so ordered.
Welcome, everyone. We are pleased to have you this
afternoon.
Pursuant to Committee Rule 9(g), the witnesses will please
stand and raise their right hands.
Do you solemnly swear or affirm that the testimony that you
are about to give is the whole truth and nothing but the truth,
so help you God?
[A chorus of ayes.]
Ms. Mace. Let the record show that the witnesses all
answered in the affirmative. We appreciate all of you being
here today and look forward to your testimony. We will remind
the witnesses that we have read your written statements, and
they will appear in full in the hearing record. Please limit
your oral statements to 5 minutes. As a reminder, please press
the button on the microphone in front of you so that it is on,
and the Members up here can hear you. When you begin to speak,
the light in front of you will turn green. After 4 minutes, it
turns yellow, and when the red light comes on, your 5 minutes
has expired, and I will ask you to wrap it up very politely.
So today, I will now recognize Dr. Scherlis to please begin
his opening statement.

STATEMENT OF DR. WILLIAM L. SCHERLIS

PROFESSOR OF COMPUTER SCIENCE

CARNEGIE MELLON UNIVERSITY

Dr. Scherlis. Chairwoman Mace, Ranking Member Connolly,
Members of the Subcommittee, thank you for the opportunity to
participate in this important hearing. I am William Scherlis, a
professor of computer science at Carnegie Mellon, but I should
also mention that I have government experience, the honor of
serving two tours at DARPA, the Defense Advanced Research
Projects Agency, with a mission to advance innovations in
information technology for national security, including AI and
cybersecurity.
We are at a critical time with modern AI, neural networks
for machine learning, large language models, LLMs, ML. This
places, as you note, extraordinary demands on our workforce.
How do we harness the power of AI while avoiding the pitfalls?
How do we stay current with the exceedingly rapid pace of
innovation? How do we stay ahead of competitors? That is why
H.R. 4503, that you together have introduced, is so vital for
the Nation.
Many have spoken about the potential for modern AI, and I
am very excited about this, so many applications, but for our
conversation today, we need to understand the whole picture,
and this includes some not so obvious weaknesses and
vulnerabilities, the pitfalls, and you have mentioned some of
those as well. We hear about issues of bias and fairness and
accuracy. Even with fully correct training data, we can get
wrong answers--machine learning, mis-categorizing LLMs,
hallucinating--and we struggle to explain.
We also have cyber adversaries. AI turns out to be an easy
target. Undergraduates learn to spoof neural nets for face
recognition. It is unclear whether this spoofing is fully
preventable. LLM providers provide guardrails, so systems do
not do bad stuff--inventing new kinds of fraud, for example,
but researchers know how to bypass those guardrails.
So, there are four ways that we address these challenges.
First is what we call AI engineering. AI is actually generally
a capability within a system, and systems operate within
operational workflows. AI engineering is about designing and
testing the systems, the workflows, and the AI inside. Our
engineering college at Carnegie Mellon has master's degree
programs for AI engineering, each for a specific engineering
discipline. The Software Engineering Institute, a CMU FFRDC for
the Defense Department, develops AI engineering practices for
the DOD and others. So, that is the first.
The second is we need to continuously make improvements to
the machine learning and LLM algorithms. The third is, in
addition to that, we have to aggressively develop new kinds of
AI technologies that will take us beyond the purely statistical
neural nets. Remember, we are just getting started with AI, but
the fourth point, and most important, is awareness, what the AI
workforce needs to understand. Now, AI researchers have many
opinions on this, but that is the way it should be, and my
point is that there is a wide range of AI-related skills and
expertise, data wranglers, LLM prompt writers, interaction
designers, systems engineers.
The executive order that was mentioned, 14110, on
trustworthy AI highlights the role of AI red teams to mitigate
weaknesses and avoid hazards. They are like cyber red teams but
with sophisticated AI skills. Everybody at all levels needs to
understand the pitfalls as well as the benefits, and when we
put this together, the roles and skills that they need, we see
a very broad range of needs for the AI workforce.
At Carnegie Mellon, we help the AI workforce in many ways.
Degree programs, we have AI Ph.D.s, and I am going to say this,
that go back to the 1950's. We have the Nation's first
undergraduate degree in AI. We have about 2 dozen master
programs. The curricula draw from computer science, statistics,
data science, math, but also ethics, psychology, humanities,
and the arts. Nondegree programs are essential to broaden
access and to scale up. We partner with the Army Futures
Command and the Army AI Integration Center on offerings that
are tailored to the huge diversity of Army AI developers and
users.
At the other extreme, for K through 12, we have CS Academy,
now used for free by 7,000 teachers in all 50 states.
Pittsburgh Public Schools asked for this kind of capability. We
developed it, and it took off. We also engage the workforce
directly. Hotel and transit workers, for example, directly
participate in our programs to ensure that the AI that they use
aligns with their experience with benefits both to the
employees and the employers, and we have government-focused
executive education. With Learning by Doing, students gain
hands-on AI experience tailored to their mission.
So, to summarize, AI offers tremendous opportunity and
tremendous challenges, many of which are non-obvious. Success
with AI depends on unique skills and expertise. There are many
kinds of AI roles and applications, which means many kinds of
workforce needs. Our CMU experience illustrates a few ways to
meet many of those needs, but we need to keep pace because it
is a very fast-moving environment. The proposed legislation to
expand AI training----
Ms. Mace. We are over time. Thank you.
Dr. Scherlis [continuing]. Vital for the Nation.
Ms. Mace. Thank you, and then, Ms. Hadra, you are
recognized for 5 minutes.

STATEMENT OF MS. TIMI HADRA

CLIENT PARTNER AND SENIOR EXECUTIVE

FOR WEST VIRGINIA

IBM

Ms. Hadra. Good afternoon, Chairwoman Mace, Ranking Member
Connolly, and distinguished Members of the Subcommittee. Thank
you for the opportunity to testify today. My name is Timi
Hadra, and I am a client partner at IBM and have been
supporting Federal agencies for over 20 years. I also serve as
IBM's Senior Executive for West Virginia, where I lead our
Regional Innovation Center, driving innovation and promoting a
skills-first talent perspective.
IBM has been a proud partner to the U.S. Federal Government
for decades, helping agencies use technology to accomplish
their missions, meet new challenges, and drive innovation.
Today, we have over 4,300 IBMers working alongside Federal
workers. IBM has been at the forefront of innovations, such as
AI, for decades. You may recall in 2011, IBM's Watson won
Jeopardy and ushered AI and machine learning into the living
rooms of America. And now, in an era of accelerated, generative
AI adoption, IBM's AI platform for enterprise, Watson X, is
helping business and governments manage their data with trusted
governance and innovative, open-source solutions.
Today, I will share how IBM has helped prepare people to
work alongside AI, created new pathways for technology jobs,
and reskilled our own workforce to maximize opportunities
created by technologies. And last, I will share our
recommendations on how the Federal workforce can be AI ready.
First, let me underscore IBM's commitment to responsible
development and deployment of AI. When harnessed and deployed
responsibly with ethics at its core, AI can enrich and advance
human ingenuity in ways that could solve the most pressing
problems of our time. We are also mindful of the impacts new
innovations have on society and what skills people will need to
work with emerging technologies, and here are three examples.
In 2021, IBM unveiled a global commitment to help skill 30
million people by 2030, and most recently, we pledged to train
2 million people in AI by 2026. This will be accomplished
through IBM's SkillsBuild, our online platform with free
coursework for teachers, students, and adult learners. It
offers free coursework in AI fundamentals, chatbots, AI ethics,
and generative AI. We are collaborating with universities and
leveraging our network of experts to build faculty and student
AI capacity. I encourage you to share with your constituents
the free AI courses that they can start taking today.
Ten years ago, IBM embarked on a skills first journey,
creating opportunities for well-paid jobs for those without
college degrees, including apprenticeships for technology jobs.
Today, more than 50 percent of our U.S. job postings no longer
require a 4-year degree, and almost 20 percent of our U.S.-
based hires do not have college degrees. As generative AI
begins to transform industries, skills play an even more
crucial role in meeting the talent need of employers. That is
why IBM places skills at the center of our people strategy. IBM
requires employees to complete at least 40 hours of learning
annually and provides the tools for that learning. IBMers with
the highest learning hours are 20 percent more likely to move
to a new role and 44 percent more likely to get a promotion.
As AI adoption accelerates, we believe Congress must double
down on actionable ways to ensure the Federal workforce is
ready to safely procure, govern, and work alongside AI. It is
time to put implementation into high gear. Specifically, we
believe there are two key ways Congress can help advance an AI
ready workforce. First, foster a culture of upskilling and
lifelong learning. The Federal Government must place skills at
the center of the people strategy and invest in quality,
relevant, and accessible tools for the span of each worker's
career. Second, scale skills first hiring in the Federal
Government and on Federal contracts.
AI is here. It is redefining work, and it will require more
people to work with technology. IBM looks forward to continuing
to work with Congress to advance a risk-based approach to
regulating AI while ensuring Americans, including the Federal
workforce, have access to skills training in the era of AI
adoption. Thank you for the opportunity to testify today.
Ms. Mace. Thank you. I now recognize Dr. Toregas to please
begin your opening statement.

STATEMENT OF DR. COSTIS TOREGAS

DIRECTOR

CYBER SECURITY, AND PRIVACY RESEARCH INSTITUTE

GEORGE WASHINGTON UNIVERSITY

Dr. Toregas. Chairwoman Mace, Ranking Member Connolly, and
Members of the Subcommittee, I appreciate the opportunity to
testify today. I am Dr. Costis Toregas, Director of the
Cybersecurity and Privacy Research Institute of the George
Washington University, and a fellow of the National Academy of
Public Administration, chartered by Congress in 1984 to help
address critical societal challenges.
I applaud your efforts to focus attention on the critical
workforce shortages looming ahead in the artificial
intelligence field and the impact such shortages may have on
American strength and prosperity. The truth is that while AI
has been around for many years, as we heard, from the 50's,
interest and concern peaked only recently as its use has become
easy for the general public, and they know. Rather than talking
about what is AI or how can we develop or regulate AI, I am
going to focus my remarks on how can we develop a workforce
pipeline that can use AI to strengthen the U.S. economy.
We have very few markers to help us answer this third
question. We know that we do not yet have adequate numbers of
teachers and faculty to teach AI. We are not sure how we can
test students and workers for AI readiness. AI courses in high
schools, community colleges, and universities are not organized
around national best practice, so we are at the beginning of
the adoption curve for this powerful technology, and things are
kind of messy. So, how should we proceed?
I suggest that we look at another critical workforce
pipeline, that of cybersecurity that you are very familiar
with, and learn from the 10-plus years of investments we have
been making and focusing on developing larger numbers of cyber-
ready workers. The systems, the networks, the incentives, the
educational strategies we created can help us develop AI
counterparts and perhaps even use the same performers, the same
strategies to good effect.
In my written testimony, I have identified several
cybersecurity experiences to consider: the importance of
diversity in the workforce, the challenge of organizing
educational programs across disciplines, the rapid change of
the underlying technology itself, and, most importantly, the
difficulty of confronting 50 diverse educational programs at
the state level that may approach the workforce issue and the
educational streams supporting it differently. These all
contribute to a difficult learning curve for cybersecurity
workforce, and yet, we learned a lot, and I want to encourage
you to look at the strong lessons learned in the cybersecurity
world and support their transfer in the AI domain rather than
spend valuable time and resources building AI responses from
scratch.
Let me now offer five action suggestions for your
subcommittee. One, develop a statistical capacity at national
level to track current numbers of students and teachers in AI
by region, as well as estimated AI workforce needs of
government and industry in the future. We will not know if we
are succeeding if we cannot measure our outcomes. Two,
encourage states to harmonize AI programs for K through 12
through national conversations of experts and discussions of
circular frameworks and rubric, and promote the broad notion of
a digital citizenship program for all students that will
include not just AI, but digital literacy, cybersecurity,
privacy, and civics in the digital era.
Three, support the development and maintenance of curricula
focused not only on the ``what is AI?'' or ``how can AI be
improved?'' but, rather, how can AI be used in this space. Good
candidates for execution are the more than 1,000 locally based
and supported public, independent, and tribal community
colleges, and they can be strong performers in the new field of
AI workforce development. They are able to change courses
quickly and adopt AI-focused curriculum, degrees, or
certificates far faster than other types of educational
institutions.
Four, focus on the need for additional AI educators
--we need teachers, which we do not have now--and establish
support programs that incentivize their attraction and
retention at high school, community college, and university
level. My good friend Janelle Strzok, who is the Chairwoman of
Women in Cybersecurity, has three words to describe how you can
support a network of individuals. She says, connect, inspire,
and guide. We need to do that for our faculty, for our
teachers.
And then finally, help launch a tripartite partnership
between private sector, education, community, and government
around workforce development issues in AI. We currently speak
in isolation, and we need to come together and create viable
solutions with all stakeholders at the table. The mandate could
include establishing a long-term vision and the steps necessary
to align academic performance to industry needs. Industry and
academia must come together. You can help do that. Thank you
very much.
Ms. Mace. Thank you so much, and I will now recognize
myself for 5 minutes.
My first question goes to Ms. Hadra. Thank you for being
here today. Your testimony states that robust implementation of
the AI and Government Act by the executive branch would be a
key first step in preparing the Federal workforce for a new way
to work using AI. In that law Congress adopted 3 years ago,
Congress told the Office of Personnel Management to find out
how many employees at each Federal agency have AI skills and
how many more we need. It also recommended OPM create an AI job
series. OPM has done none of that. How can Congress ensure the
Federal workforce is AI ready if we do not know the current
skill level in agencies or their future needs?
Ms. Hadra. Thank you for that question, and a good place to
start, I mentioned in my written testimony and in my oral
testimony about IBM SkillsBuild. This is a free online platform
for students, teachers, and adult learners, so it applies to
our Federal workforce. It is a place that we can get started
quickly to start having our Federal workforce learn the AI
skills that they need, while each agency works to employ the
requirements in the OMB memo.
Ms. Mace. Has IBM attempted to determine how many of its
employees are AI qualified company-wide?
Ms. Hadra. I do not have the specific numbers, and I can
get those for you on our company-wide numbers.
Ms. Hadra. But we have a curriculum that we implemented in
the summer of 2023 that all IBMers were required to complete. I
think it was about 12 hours of curriculum, and the thing that
we did on top of----
Ms. Mace. So, you have a metric that you are tracking, at
least with your employees' qualifications.
Ms. Hadra. Right.
Ms. Mace. Mm-hmm.
Ms. Hadra. And then have them put that to use. I mentioned
in my written testimony about our Watson X challenge, which was
in August 2023, where after they took the curriculum, we had
teams come together and solve a business problem using our
Watson X platform, so that they were not only taking courses on
learning the AI technology but also how to put it to work.
Ms. Mace. Right. Why do you think it is so hard for the
Federal Government or certain agencies to adopt this kind of
nimble attitude toward AI?
Ms. Hadra. That is a great----
Ms. Mace. Smart and nimble, right.
Ms. Hadra. It is a great question, and it is a challenging
topic, but I think sometimes, as some of my fellow panelists
mentioned, we are not talking across agencies. Maybe we are not
talking across industry. I think it is important for the
Federal Government to look at what industry has done and try to
implement and use the tools that are out there, like our
SkillsBuild, like the other platforms that are widely
available, and start getting those in use sooner.
Ms. Mace. We need a Khan Academy of AI is what we need. You
also mentioned in your testimony that Federal contractors are
rarely able to place an individual without a 4-year degree on a
technology services contract, regardless of their
qualifications. Are you saying that the terms of the Federal
contracts IBM is asked to sign prohibit the work be done by
those without college degrees? Does that inhibit your ability
to fill those roles?
Ms. Hadra. Yes, it is an issue, and let me explain. In my
experience with Federal contracts, a lot of times you have
labor category descriptions and requirements that say, if you
have a cybersecurity analyst, these are the minimum
qualifications that they must meet. In some cases, we have seen
change, so it is not that the regulations are not being
implemented, but it is just not enough. So, as an example, that
cybersecurity analyst, it may say as an entry level, you need a
bachelor's degree plus 1 year of experience, or it might say
bachelor's degree, or you can substitute 4 years of experience.
But our cybersecurity apprenticeship program is a 6-month
curriculum. Those people do not have 4 years of experience, but
they are immersed in 6 months, and they are ready to hit the
ground running on those programs. And because they do not meet
that minimum qualification, we are not able to put them on that
contract.
Ms. Mace. They might even be more qualified than someone
with a 4-year degree because they put that skill set into
practice. I am going to move on as I am running out of time,
and I apologize. Dr. Scherlis, my last question today is
Carnegie Mellon and other leading universities now offer a
variety of AI-related education options, including traditional
degrees, boot camps, certifications, et cetera. Are there
effective alternatives to traditional undergraduate and
graduate degrees that will be instrumental in preparing the
American workforce to be AI ready? What do you advise?
Dr. Scherlis. Thank you for that question. I mentioned one
example, which is working directly with the workforce. In this
case, it was related to hotels and transit workers, right, and
working directly with the workers and their employers so that
when the employers are building AI systems, they can craft
those systems in a way that aligns well with the workers'
experience and knowledge and how the workers can evolve in
those roles. So, that is one example of direct outreach.
Ms. Mace. Thank you. Thank you so much, and I have run out
of time, so I will hand the microphone over to recognize my
colleague from Virginia for his 5 minutes.
Mr. Connolly. Thank you, and before the clock starts, can I
ask unanimous consent to enter a statement from Johns Hopkins
University and the Center for AI and Digital Policy into the
record?
Ms. Mace. Without objection.
Mr. Connolly. I thank the Chair.
Mr. Connolly. Dr. Toregas, you stated in your testimony
that standard terminology for AI skills is still under
development. Why is it still under development, and why is it
even important?
Dr. Toregas. Thanks for that question. It is important
because, otherwise, we cannot classify and promote specific
skills for specific jobs, and we met the same circumstance in
cybersecurity. It has taken them at least 8 years to develop a
typology for cybersecurity skills development. We also have
several agencies, each of which promote their own typology. As
a consequence, I think it is vital with AI, since we are still
in the beginning, to jump first and to insist that there be
some kind of a mechanism that defines what are the skills, how
do you prove that you have those skills, and how do they mesh
to job requirement.
That is the last thing I said when I talked about industry.
It is very difficult sometimes for an academic institution to
hear what industry needs.
Mr. Connolly. Yes.
Dr. Toregas. So, sometimes it gets lost in translation, so,
therefore, surrogates come in. Do you have a bachelor's degree?
OK, good. You are good to go. That should not be the case.
Mr. Connolly. Yes.
Dr. Toregas. It should be the case that we align to
specific skills and specific preparations. That is what is
needed in standardization. Thank you.
Mr. Connolly. Yes. Yes. That is actually something that has
always fascinated me, that gap between lab bench research,
science, academia, and what IBM needs, and trying to translate
that is not always easy. We certainly saw that in the
commercialization of technology once the cold war was over.
DARPA, where you were, Dr. Scherlis, you know, had lots of
technology that was classified. For example, one no longer
classified is sound cancellation technology, you know, used for
submarines. Well, we are using it all the time now in the
commercial sector, but it took somebody to understand the
application, and who are those people. And they may not need a
degree. They may have common sense and private sector
experience, knowing what the environment demands.
Dr. Scherlis, you were at DARPA. You have sort of been at
the beginning of this, well, beginning of this explosion of
this fascination with AI. What keeps you up at night? What
worries you about where we could go with AI?
Dr. Scherlis. So, I have a concern regarding the various
pitfalls that I mentioned in my statement and elaborated in
some detail in the written statement. My concern is not about
those pitfalls, but my concern is, rather, that those who are--
because the AI applications are so compelling and so
transformative, not just enhancing productivity but creating
new ways of doing business, that we get so enamored of those
that we do not have that awareness of what the pitfalls are,
and we get stuck, and we get surprised. And that is one of the
characteristics, in fact, of the world of cybersecurity, is
that we build systems. We can measure what the system does and
how long it takes to build that system and how much it costs.
We cannot easily measure how secure it is, and so we let
security attributes kind of evolve and unfold over time, so
that is a significant challenge.
And I think that part of this AI education and training
process is to help people be aware of the various pitfalls, the
weaknesses and vulnerabilities, and also the mitigations, the
various techniques that we can use as we engineer systems and
as we place systems into workplace contexts to use those
systems safely.
Mr. Connolly. Yes. Sometimes, though, I have seen it in
cyber. I have looked at cyber protection programs, but they
also train people in cyber techniques we would prefer they not
be trained in, but to protect, you have to, but it can be used
for good and bad once you have trained somebody.
Dr. Scherlis. Right. Actually, I just want to make one more
point, which is the point about measurement of cybersecurity
risk, the point about measurement of similarly AI risk
educational outcomes. These are all hugely challenging research
questions. As we think about putting programs in place, I think
it is important to think also about what kind of research we
can do to measure and assess outcomes, both for the systems and
also for the people who are entangled with those systems,
recognizing how fast the technology is evolving.
Mr. Connolly. I was going to ask Dr. Toregas the same
question. Real quickly, what keeps you up at night?
Dr. Toregas. Three things. Equity, the fact that in many
cases, human beings are not before AI impacts operations, and
then the replicability. You cannot force a scientist who is
into AI to replicate an experiment because AI works in
mysterious ways, so, therefore, we cannot be sure of how
exactly is it working. Those are the three things.
Mr. Connolly. Thank you.
Dr. Toregas. Thank you.
Ms. Mace. Thank you. I will now recognize Mr. Timmons for 5
minutes.
Mr. Timmons. Thank you, Madam Chair. I think the challenge
we are facing is bifurcated in two areas. One is the existing
workforce. How do we elevate the capacity of our existing
workforce to maximize the benefits that AI offers our economy,
our businesses? And I think that is separated into two areas.
One is government because government generally does not adopt
best practices as fast as the private sector. I think that is
an understatement. The other is, how do we develop undergrad
and graduate degree programs that will increase our capacity to
further develop AI and advance the potential positive impacts
that it has? And I think that part is relative to our
competitiveness in the international community, and the first
part is making sure the U.S. economy is continuing to be
competitive in the global economy.
Dr. Scherlis, would you agree that that is a good way of
looking at it? How do we increase our capacity of our workforce
and separate that between the government and the private
sector, and then how do you develop our capacity long term to
be a leader in this in the global community.
Dr. Scherlis. So, thank you for that question. So, I will
agree that there is some separation, but I think the separation
is largely, arguably, a cultural outcome that in government, it
is really important for us to have strong programs of
professional development and advancement for our technology
workforce. Otherwise, people do not stay current. And I think
that it may be that in many firms in the private sector,
certainly in the tech sector, that is an active element of the
employer handshake.
But I think, you know, when I look at what is going on, for
example, in the Defense Department, for example, our
conversations with the Army, the role of leadership and the
ability of leadership to think in imaginative ways, to think
about what kinds of risks can we safely take, that can bubble
down through an organization and affect, for example, who is
involved with acquisition, with engineering, with developing
strategy, with planning. All those kinds of positions are going
to be affected by AI, so if we create an environment where we
are receptive to change, we are receptive to education and
learning, I think that would make a significant difference.
Mr. Timmons. Would you agree that human nature generally is
going to facilitate increased usage of AI? I mean, if it can
make it easier for you, might as well try to use it. A lot of
people out there have tasks that they do not want to do in
their job and in their lives, and if you can press the easy
button, you are likely going to do it. So, a lot of it is
awareness in that respect. Is that fair?
Dr. Scherlis. Well, that is why awareness of the AI
pitfalls is so important because if we make it very easy to
adopt AI for purposes for which it may not be well matched,
then all of those problems of bias and fairness and
vulnerability to adversarial attacks, all of those bubble up.
So, the adoption process, on the one hand, should be attentive
to the potential to not just improve productivity but to create
new capabilities on the one hand, but on the other hand, to do
that in a way where we are attentive and alert to the risks and
safety issues----
Mr. Timmons. And I guess I want to talk about those risks.
Dr. Toregas, you mentioned this. If you use ChatGPT and you ask
a question, and then you say, answer that question if you are
in Saudi Arabia or if you are in Yemen, it is actually quite
different. Certain words are entirely gone. The word that they
use as ``ethics'' instead of ``equity,'' instead of ``DEI,'' it
actually is culturally different. So, I mean, how do we have
this conversation in a productive way without kind of imposing
our cultural values on the planet? Does that question make
sense? If your biggest concern is equity and your view of
equity and your values are very dissimilar from other cultures
that are also going to be using AI, how do you reconcile that?
How do you deal with that?
Dr. Toregas. Thank you for that question. It has no simple
answer, alas. I would say that the very technology of AI
includes a session where AI learns--you kind of stuff the
machine with things--before you even use it. What you stuff the
machine with is vital, and many different countries are
beginning to use AI by including and incorporating training
regimens that reflect their own values, so their AI is
different from our AI. There is no independent AI. So, as a
consequence, I think, ultimately, as a society, we are going to
have to learn to reflect in what we expect of AI our own
values.
Mr. Timmons. Thank you for that. I yield back, Madam Chair.
Ms. Mace. Thank you. I now recognize Mr. Lynch for 5
minutes.
Mr. Lynch. Thank you, Madam Chair. Initially, I would like
to ask unanimous consent to enter into the record a statement
from the Partnership for Public Service, a nonpartisan,
nonprofit organization dedicated to better government and
stronger democracy. I assume that is OK. I was looking for
unanimous consent on this, yes.
Ms. Mace. Yes.
Mr. Lynch. OK. Great. Thank you.
Mr. Lynch. I assume that, but lately you do not want to,
you know, leap to conclusions. First of all, I want to thank
the witnesses for your good work in helping the Committee with
its task.
So last year, the Biden Administration launched a National
Cybersecurity Workforce and Education Strategy, and then
recently, as recent as last month, the National Science
Foundation launched a new initiative called the Education AI
Initiative. So, we have seen some work within our university
systems across the country. I know that in my own district,
UMass Boston University of Massachusetts, Boston, which is home
to the Paul English Applied Artificial Intelligence Institute,
has begun their work with a goal of, you know, attracting
students for that specific discipline. In Bridgewater State
University, also on the edge of my district, they have created
a first-of-its-kind cybersecurity program with the help of
Federal grants to try to inculcate students in that curriculum,
and as well, Northeastern University in my district has the
Institute for Experiential AI, which helps actually solve AI
research problems.
So, we are beginning to see energy and resources being put
into this effort, but one of the gaps is the availability of
teachers in this discipline. I mean, I founded a charter school
in Massachusetts, and while we might have 90 applicants for a
teaching position in English, we might have 11 applicants for a
similar position in math and science. And so, what is happening
is, especially in the Boston area, and it is happening
everywhere, I am sure, is private industry is scooping up
anybody with a certain talent or skill set. How do we get at
that problem where we actually create the teachers who will be
able to sort of multiply the effort and help us either catch up
to some of the countries that seem to have taken a lead in this
or actually maintain our edge? Dr. Toregas?
Dr. Toregas. Thanks again for that question. I think
experimentation and boldness are the keys. In my own University
of George Washington, the National Science Foundation and NIST,
the National Institute of Science and Technology, has provided
a grant to develop a comprehensive way to look at law and
society. It is called Trustworthy AI in Law and Society. The
key there is the combination of different disciplines which can
inspire teachers to want to become involved in it.
A second simple example is the networking of professionals
together. National Science Foundation
--again, bravo to the National Science Foundation--has
funded something called the National Center for Training and
Education in Cybersecurity. They assemble more than 300
universities and community colleges, they develop common
curricula, and they help faculty careers. The same model can be
used in the AI field. There is no reason why we cannot begin to
develop multiple solutions as opposed to one good program here,
one good program there. We can develop a network of programs.
Mr. Lynch. That is great. Ms. Hadra, your thoughts?
Ms. Hadra. Thank you for that question, and we do agree
that there is an urgent need to train the student and faculty
capacity in AI, and that is why IBM continues to invest as an
industry partner in programs that are free, such as the IBM
SkillsBuild platform. And that is for teachers, that is for
students, that is for adult learners, so that we can provide
that fast start to help build that capacity that we need now.
Mr. Lynch. That is great. Dr. Scherlis?
Dr. Scherlis. So, I mentioned in my remarks the CS Academy,
which was in response to exactly this challenge that you just
identified, and it came to us from teachers in our public
school system. And it is basically an upskilling program
intended to help teachers develop skills in computer science,
and because it is free, it has been adopted extremely widely.
And it is not just 7,000 teachers, but it is more than 380,000
students that have benefited from this. But we also have an,
since you mentioned the NSF, an NSF-funded program focused
specifically on AI for K through 12, and that is a program
where we have been developing curriculum. We were a co-lead in
the creation of that curriculum, and it is now being piloted in
schools in Georgia. So, this is a really important topic.
I will just mention when computer science itself burst onto
the scene in the 1980's, we had similar programs to upskill
college professors to help them understand the basic features
of computer science. These are traditionally teachers of
mathematics and physics.
Mr. Lynch. Great. Madam Chair, I yield back.
Ms. Mace. Thank you. I will now turn to Mr. Burlison to be
recognized for 5 minutes.
Mr. Burlison. Thank you, Madam Chair. You know, having come
from the IT industry, there is a very broad array of jobs and
roles and responsibilities. I recall oftentimes, though, people
lumped them all in the same thing. They think that everybody in
IT can fix their computer. You get where I am going. So, with
that being said, it has already been alluded to that there is a
varying scope that is broad in the different fields that are
going to be within this industry, and I will give opportunity
for each of you to elaborate on what careers or jobs that are
going to be. What is the range that you see happening, and what
is the one that is most in demand?
Dr. Toregas. Thanks for the question. Again, this is why I
emphasize the importance of typology. How do we describe jobs
in the AI field? We have already heard that there are jobs,
like, how do you shape a question? That has become a job now.
How do you shape a question for ChatGPT? There is also the
computer science behind it. How do you make better AI
machinery?
My kind of North Star would be to make sure that we develop
our faculty because the key area is faculty. If we do not have
faculty at the high school level, on the community college and
university level, we will lose the battle 10 years from now
because they are being diverted in other fields. And we
desperately need educators, so that is where the focus has to
be.
Ms. Hadra. Thank you for the question. In addition to the
roles that he just mentioned, I think it is really important to
think about AI is going to change 90 percent of jobs, right?
So, we need to think about how, one of the Members mentioned,
around freeing people up from redundant tasks that you may not
want, that is not the most exciting part of our work. We have
done this at the VBA, the Veterans Benefits Administration. I
mentioned in my written testimony that we used AI and
automation to help them process a lot of the information that
comes in, increasing that process time and freeing those
overworked VBA employees up to do more higher value work for
the veteran.
So, I think it is really important that as we are
considering the era of AI adoption, how we help each employee,
each Federal worker, think about how their job can change,
maybe what they can free up to do, and then if their job is----
Mr. Burlison. Without feeling that their job is threatened.
Ms. Hadra. Yes. I absolutely acknowledge the uncertainty,
but if we have transparency about more jobs that are being
created, jobs are being elevated in our H.R. function. As an
example, in IBM, when we have done a lot of this reskilling
already, most of the people in our H.R. job function are now
one job band higher because we have used AI to----
Mr. Burlison. Yes.
Ms. Hadra [continuing]. Automate some of the work that----
Mr. Burlison. To magnify productivity.
Ms. Hadra. Exactly.
Dr. Scherlis. So, I will mention a few elements of this.
One is, of course, enhancing productivity. Another is providing
good over-the-shoulder advice. That is the sweet spot for both
machine learning and large language models, but also, of
course, developing new kinds of capabilities, but in addition,
let us think about the network of various roles in an
organization. Think of that as kind of a workforce supply
chain, so to speak, and we can take a holistic view of those
supply chains and think, OK, with the advent of AI, maybe we
can reconfigure roles that maybe have a traditional
configuration that could be improved because of the benefits of
AI.
And so, we have a program at the Block Center at Carnegie
Mellon that is doing exactly this. It is a supply chain-focused
initiative. So, instead of taking existing job roles within
existing structures, we are looking at the structures
themselves to see maybe there are ways that we can reconfigure
how we do the overall body of work within an organization,
rather than just trying to optimize the individual elements of
an existing organizational structure. Thank you.
Mr. Burlison. Quickly, Dr. Scherlis, regarding China, where
would you place America in its competitive stance when it comes
to AI with China?
Dr. Scherlis. I do not have a strong judgment on that point
because I do not have particular visibility. Everything I read
tells me that China is definitely investing very heavily. They
have close connections between universities and government and
industry that allows them to move very quickly, and I think we
need to learn how to move very quickly within the structures of
our democracy.
Mr. Burlison. Thank you. I yield back.
Ms. Mace. Thank you. I will now yield to Mr. Khanna for 5
minutes.
Mr. Khanna. Thank you, Madam Chair. Thank you for convening
us.
Paul Krugman writes about the power loom, that initially,
obviously, it had high compensation remuneration for those who
knew how to use it, and then eventually, the technology became
sufficiently deployed and that the skills required for it were
not as much, and so you saw a leveling of the income disparity
as the access to new technology became easier. One of our big
challenges in the digital revolution, combined with the AI
revolution, is how we avoid the growing disparity in income
that we have seen so far and disparity of opportunity. I often
say, you know, my district, I represent a $10 trillion market
cap with Apple, Google, Intel, Yahoo, and Tesla, and many other
parts of the country do not have that economic opportunity.
Dr. Scherlis, what are a few major initiatives we can
undertake to create more equality of opportunity when it comes
to digital wealth generation?
Dr. Scherlis. I think that starting early is essential, and
that is one of the reasons why at Carnegie Mellon, we focus so
closely on K through 12 programs, outreach programs. One, we
get to people early so they can become kind of acculturated
with the new technologies, first computer science and now AI,
and then that also puts them into a state of improved readiness
so that they can participate, for example, in our degree
programs with strong backgrounds. In the early days of computer
science undergraduate degrees, we reached out to K through 12
for exactly this purpose, to improve the applicant pool for our
programs so that we could operate those programs at a high
level. So, I think starting early is the most important lever
we can push.
Mr. Khanna. I agree with a far more need for technology
education starting very, very early on. That does not mean that
people have to go become computer scientists or programmers,
but they need to have the facility and understanding, as all of
you have testified, because these are going to be every job in
this country. Whether it is manufacturing, retail, or services,
it is going to require technology proficiency, and I think our
schools are woefully behind in creating the technology
proficiency that is going to be required of every individual.
When we go beyond that and think of the 60 percent who may
not have college degrees, how do we bridge that divide in what
I think will be a lot of digital trades jobs, jobs that may
require 9-month, 12-month, 18-month credentialing and be able
to pay a lot, but we are not preparing them. I mean, one of the
programs I have done with Google and HBCUs, community colleges,
and HSIs is, actually, Google, with other technology companies
have come, and Carnegie Mellon, I think, is actually involved
in it, in providing some of the curriculum and then creating a
pathway for these folks after 18 months to get a job. And
candidly, some of the community colleges I went to, they could
have all the education in the world. They would be woefully
unprepared to actually get a job and no pathway to a job.
So, how do we get private industry and the right folks who
understand the curriculum that is going to be needed for hiring
involved in this kind of partnership? Maybe I will ask everyone
on the panel. Dr. Scherlis, we can start with you, and then Ms.
Hadra and Dr. Toregas.
Dr. Scherlis. So, I will just mention one example. We have
a program at Carnegie Mellon called Social and Interactive
Learning--SAIL--which is a platform that is directly targeted
to community colleges. We have about 40 community college
systems across the country that make use of this platform to
help provide certificates for IT career growth, and so in a lot
of cases, we collaborate with industry so that these
certificate programs can be tailored to local needs of local
employers.
Ms. Hadra. A quality of opportunity was really the
foundation of our Skills First talent perspective. One of the
tools that we use to put this in action is our apprenticeship
program. You mentioned trades. The apprenticeship programs that
we have, there are 35 roles registered with the Department of
Labor, recognized by industry. They get a completion
certificate when they graduate. And for the workforce that you
mentioned that might not have the skills to even begin in an
apprenticeship program, we have also published free pre-
apprenticeship program course work that people can take to
prepare themselves to become an apprentice.
But it should be mentioned that our IBM apprenticeship
program is a full-time paid learn-while-you-earn full benefits
program, and we have successfully hired over a thousand
apprentices just in the U.S. since we deployed it, 50 in my
state of West Virginia.
Mr. Khanna. My time has expired.
Ms. Mace. Apologies. You had great questions, by the way.
They were good. All right. Mr. Langworthy, you are recognized
for 5 minutes.
Mr. Langworthy. Thank you, Madam Chair. I would like to
thank all of our witnesses for being here with us today to
discuss America's path to an AI-ready workforce, and it seems
like each passing day is bringing AI more incorporated into
everyday Americans' lives. Leading experts believe AI will
completely change the workplace and significantly increase all
of our productivity, but the bottom line is this: the American
people need to be prepared for a future where AI technology
plays an even greater role than it does today. Additionally, we
need to ensure that AI is harnessed as a job creator and that,
as this technology is more widely used, effective guardrails
and training are in place to ensure that it is ethical, and it
is responsible in its use.
Ms. Hadra, I would like to start with you and talk about
our population who is already in the workforce and may not have
the skill set necessary to be prepared for an AI-integrated
work product. Many Americans are still new to AI and may only
see its uses in mainstream models like ChatGPT. How does IBM
approach the process of retraining and upskilling existing
employees?
Ms. Hadra. Thank you for that question. Investing in
upskilling, reskilling, and lifelong learning is just in IBM's
DNA. And as we usher in new technologies like AI, we must
ensure that both our employees and our society more broadly
have opportunities to gain these skills, and we do this in real
time. We want our employees to remain valuable to our clients,
so we employ reskilling and upskilling programs annually. I
mentioned in my testimony that we require a minimum of 40 hours
of education learning annually for every employee.
An example of where we see employees' jobs that are being
affected by AI, we are reskilling and being transparent about
what their next path could be. All of our job roles in IBM have
an associated learning path that shows you what continuous
learning you need to do to stay on your current role and
progress in your current role or progress out of your role, so
we have a multipronged approach to making sure that people have
transparency. We use a co-creation strategy, so they are part
of the solution and really have them help buy in to wanting to
learn the skills because they see the opportunity to grow their
careers.
Mr. Langworthy. Thank you, and turning toward college-age
students and recent graduates, Dr. Scherlis, from a long-term
perspective, what are the benefits of a traditional 4-year AI
degree?
Dr. Scherlis. So, the 4-year AI degree is closely aligned
with our 4-year computer science degree. It is very intensely
focused on the mathematics of computer science, on the
development of software code, on algorithms, on all of the
technical foundations that are necessary for a full career in
computer science. Given that the field of computer science is
moving so quickly, we want to provide our students with a
foundation that will endure. And so similarly with AI, we are
focused on that foundation in statistics and data science, in
computer science, in the design of the computing hardware and
the computing software so they have a very full background in
all those capabilities, and then they will be ready to be
involved in the creation of new kinds of AI capabilities, as
well as in the application of AI capabilities in ways that are
safe, that avoid the pitfalls, and that also push the envelope
in terms of the kinds of things that we can do with AI.
Mr. Langworthy. OK. You know, my district is a rural
district, New York's 23d congressional District, where many of
our high school students, they would choose a career in the
trades or vocational skills over maybe a 4-year degree. Are
there opportunities for high school graduates and college-age
Americans who do not want to go down a traditional 4-year
degree route to learn the same AI skills?
Dr. Scherlis. So, that is one of the reasons why I
mentioned the work that we are doing with community colleges
but also directly with employers and employees and the program
that is looking at these so-called supply chains, the structure
of organizations, because we have to look at the influence of
AI on everybody, not just the leaders in the technology field.
Mr. Langworthy. Great. Want to open up this question to all
of our witnesses because I feel like there are several
approaches. In terms of school-age children, what programs are
in place to begin teaching our youngest generations how to be
proficient with AI, and is there anything that the educational
system should be doing better to prepare this and for
subsequent generations? Quickly. Oh, I did not realize. My time
has expired, so anything that you have to offer in writing, we
would certainly appreciate that, and I yield back.
Ms. Mace. Thank you so much. I will now recognize Mr.
Krishnamoorthi for 5 minutes, sir. Thank you for your presence
today.
Mr. Krishnamoorthi. Hey. Thank you, Madam Chair, thank you
to our Ranking Member for this excellent hearing. Thank you to
the witnesses.
[Chart.]
Mr. Krishnamoorthi. I have this visual here. I did not come
up with this. I thought it was a really neat visual created by
Opportunity at Work, and it talks about something called the
paper ceiling, and it is defined as the invisible barrier that
comes at every turn for the 70 million workers who are STARs.
Now, you might be wondering what is a STAR? They define a STAR
as a person skilled through alternative routes other than
college, 4-year college. And so, maybe I will start with Ms.
Hadra.
Ms. Hadra, I assume that the people that are in your earn-
and-learn program are precisely these types of people, these
STARs, correct?
Ms. Hadra. Thank you for that question, and I am so glad
you brought that up, because, actually, one of our apprentices
at my center, in Rocket Center, and I mentioned her in my
written testimony, Cindy, was featured on a STARs campaign ad,
so yes. Yes.
Mr. Krishnamoorthi. Oh, great. So, the answer is yes.
Ms. Hadra. Exactly. Yes, hundred percent.
Mr. Krishnamoorthi. And so, you know, one of the big
concerns that I have is that, are you familiar with these
automated hiring systems that a lot of employers use?
Ms. Hadra. Not as much, no.
Mr. Krishnamoorthi. OK. Dr. Scherlis, I think I saw you
nodding your head. These automated hiring systems basically
screen resumes to look for certain indicia of whether the
applicant can do the job that is being posted, but oftentimes,
what they do is they screen out people without a 4-year college
degree. Dr. Scherlis, have you seen this automated hiring
system at work?
Dr. Scherlis. I have not seen it up close, but when I talk
with young students who are entering the workforce, this is one
of the topics of their conversation is what are the magic
keywords to include----
Mr. Krishnamoorthi. Right.
Dr. Scherlis [continuing]. In my resume that will allow me
to get caught in the net. But I just want to mention that
outreach to nongraduates is an essential piece of the story. We
have an outreach program called CMU Computer Science Pathways,
that works with community organizations to engage with people
who are traditionally maybe not entering the high-tech
workforce because they are under resourced, or we could say
underestimated.
Mr. Krishnamoorthi. I am glad you have that. I want to
focus for a moment on these automated hiring systems because
they are ubiquitous. Most large employers use them, and
apparently, according to 90 percent of employers in a recent
study, they felt that because of those automated hiring
systems, they are screening out precisely those STARs who could
otherwise do the job. And so, because of that, we have
introduced legislation called the Opportunity to Compete Act,
H.R. 5960, to ``tear the paper ceiling and prevent automated
discrimination against applicants without bachelor's degrees so
that these STARs could flourish.'' What is your opinion of
that, Ms. Hadra?
Ms. Hadra. Well, as I mentioned in my written and oral
testimony, 50 percent of our job postings at IBM in the U.S. do
not require a college degree anymore. So, we are definitely
moving to a skills first perspective, and we encourage that
adoption in the Federal Government as well.
Mr. Krishnamoorthi. And what is the barrier to adopting
that skills first mindset among your peers?
Ms. Hadra. I think it is really getting the demands of the
learning institutions and industry together to help identify
which roles can be more apprenticed. Like, we are saying AI,
cybersecurity. There is a shortcoming in the workforce system
around these requirements, so I think a lot of it is, you know,
dialog, and I think we would be happy to have a follow-on
conversation with you because we are very passionate about the
success of our skills-based hiring program.
Mr. Krishnamoorthi. Well, we would love to see more people
talk about this legislation and the need to allow people who
can do the job to be able to prove they can without presenting
a diploma to prove they can. And so, what do you do, for
instance, to allow STARs to be able to prove that they have
those qualifications to meet the skills of the job?
Ms. Hadra. So, our apprenticeship program, which, again,
registered with the Department of Labor, 35 different job
roles, there is a clear curriculum and a clear completion
criteria. So, our Application Developer Apprenticeship Program,
as an example, has around 500 hours of specific learning that
we provide, and then there is also some specific performance
objectives. Have you been able to develop in this code? Do you
get a good performance review, you know, the whole learn-as-you
earn, and then they have a graduation because they have
completed all of the criteria.
And it is important to note that we registered our
apprenticeship programs with the Department of Labor, so they
were portable, so they were recognized, and they were not just
this is what we are saying is important. It is an industry
standard.
Mr. Krishnamoorthi. Great. Thank you so much.
Ms. Mace. Do you have any more questions that you want to
ask? We have, like, 30 seconds to a minute. We are waiting on
Mr. Beyer to get here.
Mr. Krishnamoorthi. Can I----
Ms. Mace. Yes.
Mr. Krishnamoorthi. OK. OK.
Ms. Mace. Please do, 30 seconds. He is running as fast as
he can, but you got it.
Mr. Krishnamoorthi. OK. Well, in that case, let me ask you
this. How do we in Congress use artificial intelligence to do
our jobs? Have any of you thought about that because, you know,
I do not see automatons taking our jobs because we are going to
make that illegal probably, but I could see AI someday being
used to perform certain functions in our office. And I would
just be curious if you have ever thought about that or what
your thoughts are on that.
Mr. Khanna. We have AI ask a question, and we can
filibuster it.
Mr. Krishnamoorthi. That is right. ChatGPT, please
filibuster here.
Dr. Toregas. If I can take that quick question, I am sure
that your staff uses AI today. Well, they use it to get
background information, they use it to shape arguments, and
then they perform a very important function. They use human
intelligence after AI to give you the advice that you need. So,
that would be my guess, that you are already in it.
Ms. Mace. Thank you, and we properly filibustered for you,
Mr. Beyer, this afternoon, and Mr. Krishnamoorthi, we use AI in
our office, just quick write-up stuff, so I think we should not
ban it. Helps our comms teams. Mr. Beyer, you are now
recognized, if you are ready, for 5 minutes.
Mr. Beyer. Madam Chairman, thank you so much for
filibustering for me. I do not want to make it a habit on this
side of the Capitol, though, but thank you.
Dr. Scherlis, in your testimony, you spoke about
explainability, transparency, bias, fairness, accuracy, and
reliability of the AI models. Just last month, we introduced a
bill called the AI Foundation Model Transparency Act to try to
address the issue and shed some light into the black box of AI
foundation models. My bill would call for the model deployer to
make certain information about the training data, how the model
is trained, publicly available. The hope is that users should
know why the model is giving certain results, so it is not used
in a discriminatory way.
So, the question, Dr. Scherlis, is, do you think this type
of transparency effort would support the Federal Government's
existing workforce with evaluating AI models?
Dr. Scherlis. Thank you for that question. I think there
are tremendous challenges with the AI foundation models, and,
in fact, some of the inventors of those models have stated
openly that they themselves do not fully understand how those
models come to certain conclusions and outputs. There is a
challenge because within a large language model, you can have
hundreds of billions of parameters that are all adjusted as it
goes through its learning process. So, to understand what goes
on inside of one of those models is like doing brain surgery on
a person as a way to undercover what their opinions might be on
various issues. It is a frighteningly difficult challenge. And
so, from a research perspective, explainable AI is a very
significant problem, and there are many possible solutions and
a lot of discussion and disagreements within the AI research
community.
There is another pitfall, which is because these models are
fundamentally statistical in nature--they are like predictive
statistics--even if I train it on a hundred percent correct
training cases, I can still get hallucinations coming out,
certainly with the current systems, and so that is because the
statistical nature kind of lumps similar things into buckets.
The distinctions go away in that learning process.
So, there are lots of actions that AI people are taking to
mitigate this problem, to make the outputs a little bit more
accurate, but to get to a hundred-percent pure accuracy with
full explanations, that is, I think, still in the future.
Mr. Beyer. Yes. We participated in a red teaming exercise
late last week that Congressman Jay Obernolti put together, and
they told me that they could get up to an 80 percent on AP
calculus test, and I found a couple of people there that had
got a hundred percent without using AI, so.
Ms. Hadra, you know, one of the things we pulled out of the
President's executive order is the notion of mandating the NIST
AI framework for Federal Government agencies using it, bill
last week, bipartisan--Zach Nunn, Marcus Molinaro, Ted Lieu,
and I--the Federal Artificial Intelligence Risk Management Act.
Can you comment on both the notion of can we begin to move to
the private sector by first working on the NIST framework
within government contracting?
Ms. Hadra. Thank you for that question, and IBM is urging
governments to focus on three priorities as we have been
advocating for a risk-based approach to regulating AI. We have
been advocating for that since 2019. Our three priorities are
regulation of the specific application of AI, not the
underlying algorithm; prioritizing liability over licensing;
and supporting open-source AI innovation. So, specifically to
your question, I think we could offer a follow-on AI focused
briefing, but it is important to mention our three priorities
as it relates to the risk that my fellow panelist is
mentioning.
Mr. Beyer. Thank you very much. Dr. Toregas, the biggest
thing that has come out of our AI Caucus so far is the Create
AI Act, you know, creating a massive data set, not the 6
trillion words that Sam Altman scrubbed off the internet, but,
rather, a curated data set for researchers and the like. Are we
on the right track? Will this help us overcome some of the
biases, the hallucinations, the incorrect information, or is it
hopelessly naive?
Dr. Toregas. Thank you for your question. I think anything
is good as long as it feeds the workforce question. We are here
discussing workforce development, and workforce development
cannot happen unless we understand the technology underlying
AI. So, your efforts and the efforts, by the way, of other
nations in this same sphere are helpful in that regard, yes.
Mr. Beyer. Thank you. Madam Chair, I yield back.
Ms. Mace. Great questions. Thank you. In closing today, I
want to thank our panelists once again for their testimony, and
do you have any closing remarks, Mr. Ranking Member?
Mr. Lynch. No.
Ms. Mace. OK. Mr. Lynch does not, and with that, without
objection, all Members will have 5 legislative days within
which to submit materials and to submit additional written
questions for the witnesses, which will be forwarded to the
witnesses for their response.
Ms. Mace. If there is no further business, without
objection, the Subcommittee stands adjourned.
[Whereupon, at 3:33 p.m., the Subcommittee was adjourned.]

[all]
